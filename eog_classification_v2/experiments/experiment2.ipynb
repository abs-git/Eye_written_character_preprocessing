{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "<br>\n",
    "-- ViT Base model hyperparameters explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 13:32:50.184763: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-05 13:32:50.742765: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-01-05 13:32:50.742808: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-01-05 13:32:50.742813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/donghyun/eye_writing_classification/v2_dataset/200_points_dataset/'\n",
    "\n",
    "with open(data_path + 'eog_raw_numbers_200.json') as f:\n",
    "  eog_raw_numbers = json.load(f)\n",
    "\n",
    "with open(data_path + 'reference_data_200.json') as f:\n",
    "  reference_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "vit_hidden_size = [128,256,512]\n",
    "vit_patch_size = [5,10]\n",
    "vit_heads = [4,8]\n",
    "vit_n_layers = [8,12]\n",
    "vit_mlp_units = [[128,64],\n",
    "                 [64,32]]\n",
    "vit_dropout = [0]\n",
    "vit_mlp_dropout = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config class\n",
    "\n",
    "class Config:\n",
    "    split_ratio = 0.3\n",
    "    ref_key = 'numbers'\n",
    "    batch_size = 10            # fix : must be equaled with number of test pairs\n",
    "    n_batch = 180\n",
    "    lr = 0.0005\n",
    "    model_type = 'ViTBaseModel'\n",
    "    ViT_params = {}\n",
    "    epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for hyperparameters\n",
    "\n",
    "cols = ['hidden_size', 'batch_size', 'patch_size', 'heads', 'n_layers', 'mlp_units', 'dropout', 'mlp_dropout', 'score']\n",
    "best_perform_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "raw_numbers_dict = defaultdict(list)\n",
    "\n",
    "i = 0\n",
    "for hs, ps, heads, n_layers, mlp_units, dropout, mlp_dropout in product(vit_hidden_size,\n",
    "                                                                            vit_patch_size,\n",
    "                                                                            vit_heads,\n",
    "                                                                            vit_n_layers,\n",
    "                                                                            vit_mlp_units,\n",
    "                                                                            vit_dropout,\n",
    "                                                                            vit_mlp_dropout\n",
    "                                                                            ):\n",
    "    i+=1\n",
    "    print('index : ', i)\n",
    "\n",
    "    cfg = Config\n",
    "    cfg.ViT_params['hidden_size'] = hs\n",
    "    cfg.ViT_params['batch_size'] = cfg.batch_size\n",
    "    cfg.ViT_params['patch_size'] = ps\n",
    "    cfg.ViT_params['heads'] = heads\n",
    "    cfg.ViT_params['n_layers'] = n_layers\n",
    "    cfg.ViT_params['mlp_units'] = mlp_units\n",
    "    cfg.ViT_params['dropout'] = dropout\n",
    "    cfg.ViT_params['mlp_dropout'] = mlp_dropout\n",
    "\n",
    "    _, _, _, test_acc_list = utils.experiment(cfg, eog_raw_numbers, reference_data)\n",
    "    score = np.mean(test_acc_list[-3:])\n",
    "\n",
    "    best_perform_df.loc[i] = [hs, cfg.batch_size, ps, heads, n_layers, str(mlp_units), dropout, mlp_dropout, score]\n",
    "\n",
    "best_perform_df = best_perform_df.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters save\n",
    "\n",
    "save_path = '/home/donghyun/eye_writing_classification/experiments/save/'\n",
    "best_perform_df.to_csv(save_path+'experiment2_vit_hyperparams.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>heads</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>mlp_units</th>\n",
       "      <th>dropout</th>\n",
       "      <th>mlp_dropout</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[64, 32]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>41</td>\n",
       "      <td>512</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  hidden_size  batch_size  patch_size  heads  \\\n",
       "0             0          14          128          10          10      8   \n",
       "1             1           8          128          10           5      8   \n",
       "2             2          15          128          10          10      8   \n",
       "3             3           1          128          10           5      4   \n",
       "4             4           6          128          10           5      8   \n",
       "5             5          13          128          10          10      8   \n",
       "6             6          16          128          10          10      8   \n",
       "7             7           2          128          10           5      4   \n",
       "8             8          41          512          10          10      4   \n",
       "9             9           9          128          10          10      4   \n",
       "\n",
       "   n_layers  mlp_units  dropout  mlp_dropout      score  \n",
       "0         8   [64, 32]        0            0  90.416667  \n",
       "1        12   [64, 32]        0            0  90.000000  \n",
       "2        12  [128, 64]        0            0  89.583333  \n",
       "3         8  [128, 64]        0            0  88.958333  \n",
       "4         8   [64, 32]        0            0  87.500000  \n",
       "5         8  [128, 64]        0            0  87.083333  \n",
       "6        12   [64, 32]        0            0  87.083333  \n",
       "7         8   [64, 32]        0            0  86.875000  \n",
       "8         8  [128, 64]        0            0  86.875000  \n",
       "9         8  [128, 64]        0            0  86.875000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the hyperparameters\n",
    "\n",
    "save_path = '/home/donghyun/eye_writing_classification/experiments/save/'\n",
    "best_perform_df = pd.read_csv(save_path+'experiment2_vit_hyperparams.csv')\n",
    "\n",
    "best_perform_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-31 02:03:21.786910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 02:03:21.869253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 02:03:21.869372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 02:03:21.871699: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-31 02:03:21.872766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 02:03:21.872857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 02:03:21.872935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 02:03:22.514007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 02:03:22.514342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 02:03:22.514428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-31 02:03:22.514681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21802 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-12-31 02:03:33.461741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8600\n",
      "2022-12-31 02:03:34.412593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-12-31 02:03:34.441378: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f48434a0ac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-31 02:03:34.441403: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2022-12-31 02:03:34.458723: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-12-31 02:03:34.614265: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, train acc : 70.7222 %, train loss : 0.53205646, test acc : 50.6250 %, \n",
      "epoch : 2, train acc : 80.3333 %, train loss : 0.45883607, test acc : 66.2500 %, \n",
      "epoch : 3, train acc : 85.0556 %, train loss : 0.37104514, test acc : 48.1250 %, \n",
      "epoch : 4, train acc : 88.3889 %, train loss : 0.33690923, test acc : 64.3750 %, \n",
      "epoch : 5, train acc : 92.1667 %, train loss : 0.24560889, test acc : 75.6250 %, \n",
      "epoch : 6, train acc : 93.2778 %, train loss : 0.21391425, test acc : 81.8750 %, \n",
      "epoch : 7, train acc : 94.1667 %, train loss : 0.17655182, test acc : 75.0000 %, \n",
      "epoch : 8, train acc : 95.8889 %, train loss : 0.14406878, test acc : 76.8750 %, \n",
      "epoch : 9, train acc : 96.5000 %, train loss : 0.11755408, test acc : 78.7500 %, \n",
      "epoch : 10, train acc : 96.3333 %, train loss : 0.12045867, test acc : 91.8750 %, \n",
      "epoch : 11, train acc : 97.5556 %, train loss : 0.10206217, test acc : 85.6250 %, \n",
      "epoch : 12, train acc : 98.6111 %, train loss : 0.06193276, test acc : 90.6250 %, \n",
      "epoch : 13, train acc : 98.4444 %, train loss : 0.06349898, test acc : 91.2500 %, \n",
      "epoch : 14, train acc : 98.8333 %, train loss : 0.05994807, test acc : 81.8750 %, \n",
      "epoch : 15, train acc : 96.5000 %, train loss : 0.11930555, test acc : 91.8750 %, \n",
      "epoch : 16, train acc : 97.7222 %, train loss : 0.08145179, test acc : 88.7500 %, \n",
      "epoch : 17, train acc : 98.0556 %, train loss : 0.05953445, test acc : 77.5000 %, \n",
      "epoch : 18, train acc : 96.6111 %, train loss : 0.12309746, test acc : 90.0000 %, \n",
      "epoch : 19, train acc : 98.0556 %, train loss : 0.07664845, test acc : 86.8750 %, \n",
      "epoch : 20, train acc : 98.3333 %, train loss : 0.06676669, test acc : 91.2500 %, \n",
      "epoch : 21, train acc : 98.6111 %, train loss : 0.04684388, test acc : 91.8750 %, \n",
      "epoch : 22, train acc : 98.9444 %, train loss : 0.05182631, test acc : 91.2500 %, \n",
      "epoch : 23, train acc : 99.8333 %, train loss : 0.01852622, test acc : 90.6250 %, \n",
      "epoch : 24, train acc : 99.4444 %, train loss : 0.02709140, test acc : 91.8750 %, \n",
      "epoch : 25, train acc : 99.5000 %, train loss : 0.03069570, test acc : 90.0000 %, \n",
      "epoch : 26, train acc : 99.4444 %, train loss : 0.02918327, test acc : 88.7500 %, \n",
      "epoch : 27, train acc : 98.3333 %, train loss : 0.05306794, test acc : 90.0000 %, \n",
      "epoch : 28, train acc : 95.7222 %, train loss : 0.12963068, test acc : 79.3750 %, \n",
      "epoch : 29, train acc : 97.3333 %, train loss : 0.09318593, test acc : 91.8750 %, \n",
      "epoch : 30, train acc : 97.8889 %, train loss : 0.07040408, test acc : 90.6250 %, \n",
      "epoch : 31, train acc : 99.1667 %, train loss : 0.03609032, test acc : 90.0000 %, \n",
      "epoch : 32, train acc : 99.9444 %, train loss : 0.01087943, test acc : 88.1250 %, \n",
      "epoch : 33, train acc : 99.8889 %, train loss : 0.01211163, test acc : 90.6250 %, \n",
      "epoch : 34, train acc : 100.0000 %, train loss : 0.00682252, test acc : 91.8750 %, \n",
      "epoch : 35, train acc : 99.8333 %, train loss : 0.01009488, test acc : 90.0000 %, \n",
      "epoch : 36, train acc : 99.1111 %, train loss : 0.03590123, test acc : 90.6250 %, \n",
      "epoch : 37, train acc : 99.8333 %, train loss : 0.01460133, test acc : 89.3750 %, \n",
      "epoch : 38, train acc : 99.9444 %, train loss : 0.00992745, test acc : 91.2500 %, \n",
      "epoch : 39, train acc : 99.8889 %, train loss : 0.00907167, test acc : 89.3750 %, \n",
      "epoch : 40, train acc : 100.0000 %, train loss : 0.00696909, test acc : 90.0000 %, \n",
      "epoch : 41, train acc : 99.9444 %, train loss : 0.00604323, test acc : 88.7500 %, \n",
      "epoch : 42, train acc : 98.3889 %, train loss : 0.06780799, test acc : 89.3750 %, \n",
      "epoch : 43, train acc : 96.2778 %, train loss : 0.12095076, test acc : 90.6250 %, \n",
      "epoch : 44, train acc : 97.1111 %, train loss : 0.09143435, test acc : 87.5000 %, \n",
      "epoch : 45, train acc : 98.3889 %, train loss : 0.05410729, test acc : 88.7500 %, \n",
      "epoch : 46, train acc : 98.0000 %, train loss : 0.06513032, test acc : 87.5000 %, \n",
      "epoch : 47, train acc : 99.5000 %, train loss : 0.02401486, test acc : 88.1250 %, \n",
      "epoch : 48, train acc : 99.6111 %, train loss : 0.01706017, test acc : 88.1250 %, \n",
      "epoch : 49, train acc : 99.9444 %, train loss : 0.00630585, test acc : 89.3750 %, \n",
      "epoch : 50, train acc : 99.9444 %, train loss : 0.00517372, test acc : 89.3750 %, \n",
      "epoch : 51, train acc : 100.0000 %, train loss : 0.00451076, test acc : 89.3750 %, \n",
      "epoch : 52, train acc : 99.9444 %, train loss : 0.00606095, test acc : 90.6250 %, \n",
      "epoch : 53, train acc : 100.0000 %, train loss : 0.00339339, test acc : 90.0000 %, \n",
      "epoch : 54, train acc : 100.0000 %, train loss : 0.00210988, test acc : 90.0000 %, \n",
      "epoch : 55, train acc : 100.0000 %, train loss : 0.00383601, test acc : 89.3750 %, \n",
      "epoch : 56, train acc : 100.0000 %, train loss : 0.00202753, test acc : 89.3750 %, \n",
      "epoch : 57, train acc : 99.9444 %, train loss : 0.00247503, test acc : 91.2500 %, \n",
      "epoch : 58, train acc : 100.0000 %, train loss : 0.00256023, test acc : 91.2500 %, \n",
      "epoch : 59, train acc : 100.0000 %, train loss : 0.00276108, test acc : 90.6250 %, \n",
      "epoch : 60, train acc : 100.0000 %, train loss : 0.00295071, test acc : 90.0000 %, \n",
      "epoch : 61, train acc : 100.0000 %, train loss : 0.00165600, test acc : 90.6250 %, \n",
      "epoch : 62, train acc : 100.0000 %, train loss : 0.00171767, test acc : 90.6250 %, \n",
      "epoch : 63, train acc : 100.0000 %, train loss : 0.00171901, test acc : 89.3750 %, \n",
      "epoch : 64, train acc : 100.0000 %, train loss : 0.00174595, test acc : 89.3750 %, \n",
      "epoch : 65, train acc : 100.0000 %, train loss : 0.00198868, test acc : 89.3750 %, \n",
      "epoch : 66, train acc : 100.0000 %, train loss : 0.00112773, test acc : 90.0000 %, \n",
      "epoch : 67, train acc : 100.0000 %, train loss : 0.00082851, test acc : 90.6250 %, \n",
      "epoch : 68, train acc : 100.0000 %, train loss : 0.00179179, test acc : 90.6250 %, \n",
      "epoch : 69, train acc : 100.0000 %, train loss : 0.00209541, test acc : 90.0000 %, \n",
      "epoch : 70, train acc : 100.0000 %, train loss : 0.00249097, test acc : 90.6250 %, \n",
      "epoch : 71, train acc : 100.0000 %, train loss : 0.00129980, test acc : 90.6250 %, \n",
      "epoch : 72, train acc : 100.0000 %, train loss : 0.00184909, test acc : 90.0000 %, \n",
      "epoch : 73, train acc : 100.0000 %, train loss : 0.00168382, test acc : 90.0000 %, \n",
      "epoch : 74, train acc : 100.0000 %, train loss : 0.00162137, test acc : 90.0000 %, \n",
      "epoch : 75, train acc : 100.0000 %, train loss : 0.00173722, test acc : 89.3750 %, \n",
      "epoch : 76, train acc : 100.0000 %, train loss : 0.00220162, test acc : 89.3750 %, \n",
      "epoch : 77, train acc : 100.0000 %, train loss : 0.00152231, test acc : 89.3750 %, \n",
      "epoch : 78, train acc : 99.5556 %, train loss : 0.01863730, test acc : 79.3750 %, \n",
      "epoch : 79, train acc : 91.4444 %, train loss : 0.25069237, test acc : 80.0000 %, \n",
      "epoch : 80, train acc : 95.7222 %, train loss : 0.13122168, test acc : 88.7500 %, \n",
      "epoch : 81, train acc : 97.9444 %, train loss : 0.06794217, test acc : 88.7500 %, \n",
      "epoch : 82, train acc : 99.0556 %, train loss : 0.03416623, test acc : 88.7500 %, \n",
      "epoch : 83, train acc : 99.6111 %, train loss : 0.01852858, test acc : 88.1250 %, \n",
      "epoch : 84, train acc : 99.8333 %, train loss : 0.01262940, test acc : 91.2500 %, \n",
      "epoch : 85, train acc : 99.5000 %, train loss : 0.02131810, test acc : 88.1250 %, \n",
      "epoch : 86, train acc : 97.1111 %, train loss : 0.10434437, test acc : 88.1250 %, \n",
      "epoch : 87, train acc : 96.7778 %, train loss : 0.10288462, test acc : 89.3750 %, \n",
      "epoch : 88, train acc : 99.0000 %, train loss : 0.03390812, test acc : 90.6250 %, \n",
      "epoch : 89, train acc : 99.6667 %, train loss : 0.01819460, test acc : 88.7500 %, \n",
      "epoch : 90, train acc : 99.6111 %, train loss : 0.01657604, test acc : 88.7500 %, \n",
      "epoch : 91, train acc : 99.6111 %, train loss : 0.01298557, test acc : 88.7500 %, \n",
      "epoch : 92, train acc : 99.9444 %, train loss : 0.00655685, test acc : 89.3750 %, \n",
      "epoch : 93, train acc : 99.9444 %, train loss : 0.00502956, test acc : 90.6250 %, \n",
      "epoch : 94, train acc : 99.6111 %, train loss : 0.02005130, test acc : 88.7500 %, \n",
      "epoch : 95, train acc : 99.3889 %, train loss : 0.02782333, test acc : 89.3750 %, \n",
      "epoch : 96, train acc : 98.5556 %, train loss : 0.04094031, test acc : 88.1250 %, \n",
      "epoch : 97, train acc : 98.3333 %, train loss : 0.05166239, test acc : 88.7500 %, \n",
      "epoch : 98, train acc : 98.5556 %, train loss : 0.05352321, test acc : 88.7500 %, \n",
      "epoch : 99, train acc : 98.2778 %, train loss : 0.05291813, test acc : 90.0000 %, \n",
      "epoch : 100, train acc : 98.0000 %, train loss : 0.06637372, test acc : 90.6250 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 75.1111 %, train loss : 0.50351844, test acc : 50.6250 %, \n",
      "epoch : 2, train acc : 83.1111 %, train loss : 0.41718975, test acc : 62.5000 %, \n",
      "epoch : 3, train acc : 89.3889 %, train loss : 0.30868976, test acc : 57.5000 %, \n",
      "epoch : 4, train acc : 91.1111 %, train loss : 0.25714281, test acc : 66.8750 %, \n",
      "epoch : 5, train acc : 94.2778 %, train loss : 0.18446375, test acc : 63.7500 %, \n",
      "epoch : 6, train acc : 96.0556 %, train loss : 0.14085011, test acc : 73.1250 %, \n",
      "epoch : 7, train acc : 96.7778 %, train loss : 0.11795966, test acc : 78.7500 %, \n",
      "epoch : 8, train acc : 97.1667 %, train loss : 0.10948947, test acc : 85.6250 %, \n",
      "epoch : 9, train acc : 97.7222 %, train loss : 0.10414468, test acc : 80.6250 %, \n",
      "epoch : 10, train acc : 98.0556 %, train loss : 0.07475919, test acc : 86.8750 %, \n",
      "epoch : 11, train acc : 98.8889 %, train loss : 0.05554397, test acc : 85.0000 %, \n",
      "epoch : 12, train acc : 99.0556 %, train loss : 0.05241610, test acc : 85.6250 %, \n",
      "epoch : 13, train acc : 95.6667 %, train loss : 0.13658272, test acc : 83.7500 %, \n",
      "epoch : 14, train acc : 97.3889 %, train loss : 0.08748639, test acc : 85.0000 %, \n",
      "epoch : 15, train acc : 97.7778 %, train loss : 0.08014168, test acc : 86.8750 %, \n",
      "epoch : 16, train acc : 98.6111 %, train loss : 0.05514354, test acc : 80.0000 %, \n",
      "epoch : 17, train acc : 98.6667 %, train loss : 0.06343902, test acc : 85.6250 %, \n",
      "epoch : 18, train acc : 99.1667 %, train loss : 0.04003896, test acc : 88.7500 %, \n",
      "epoch : 19, train acc : 99.1111 %, train loss : 0.04133680, test acc : 85.0000 %, \n",
      "epoch : 20, train acc : 97.5556 %, train loss : 0.08458888, test acc : 88.1250 %, \n",
      "epoch : 21, train acc : 99.1111 %, train loss : 0.03651167, test acc : 84.3750 %, \n",
      "epoch : 22, train acc : 99.3333 %, train loss : 0.02961776, test acc : 88.7500 %, \n",
      "epoch : 23, train acc : 98.7222 %, train loss : 0.04500381, test acc : 89.3750 %, \n",
      "epoch : 24, train acc : 98.9444 %, train loss : 0.03868195, test acc : 86.8750 %, \n",
      "epoch : 25, train acc : 99.7222 %, train loss : 0.02310164, test acc : 85.6250 %, \n",
      "epoch : 26, train acc : 98.8333 %, train loss : 0.04741307, test acc : 85.0000 %, \n",
      "epoch : 27, train acc : 98.7222 %, train loss : 0.04748946, test acc : 86.8750 %, \n",
      "epoch : 28, train acc : 98.1111 %, train loss : 0.06644066, test acc : 81.8750 %, \n",
      "epoch : 29, train acc : 98.8333 %, train loss : 0.05236277, test acc : 86.2500 %, \n",
      "epoch : 30, train acc : 99.8889 %, train loss : 0.01462467, test acc : 86.8750 %, \n",
      "epoch : 31, train acc : 99.9444 %, train loss : 0.00616220, test acc : 87.5000 %, \n",
      "epoch : 32, train acc : 100.0000 %, train loss : 0.00572947, test acc : 88.1250 %, \n",
      "epoch : 33, train acc : 100.0000 %, train loss : 0.00506243, test acc : 86.8750 %, \n",
      "epoch : 34, train acc : 100.0000 %, train loss : 0.00422212, test acc : 86.8750 %, \n",
      "epoch : 35, train acc : 99.8889 %, train loss : 0.00883160, test acc : 86.8750 %, \n",
      "epoch : 36, train acc : 99.7222 %, train loss : 0.01718920, test acc : 87.5000 %, \n",
      "epoch : 37, train acc : 99.9444 %, train loss : 0.00970678, test acc : 88.1250 %, \n",
      "epoch : 38, train acc : 95.5000 %, train loss : 0.14459545, test acc : 80.0000 %, \n",
      "epoch : 39, train acc : 94.7778 %, train loss : 0.15262859, test acc : 85.6250 %, \n",
      "epoch : 40, train acc : 97.8333 %, train loss : 0.07955601, test acc : 83.7500 %, \n",
      "epoch : 41, train acc : 99.5556 %, train loss : 0.02411683, test acc : 86.8750 %, \n",
      "epoch : 42, train acc : 99.7222 %, train loss : 0.01497738, test acc : 86.2500 %, \n",
      "epoch : 43, train acc : 99.9444 %, train loss : 0.00727908, test acc : 89.3750 %, \n",
      "epoch : 44, train acc : 99.2222 %, train loss : 0.02459019, test acc : 90.0000 %, \n",
      "epoch : 45, train acc : 99.7222 %, train loss : 0.01303846, test acc : 85.0000 %, \n",
      "epoch : 46, train acc : 98.1667 %, train loss : 0.06801297, test acc : 89.3750 %, \n",
      "epoch : 47, train acc : 97.8889 %, train loss : 0.06353333, test acc : 86.8750 %, \n",
      "epoch : 48, train acc : 98.5556 %, train loss : 0.05006989, test acc : 88.7500 %, \n",
      "epoch : 49, train acc : 99.5000 %, train loss : 0.02904280, test acc : 86.2500 %, \n",
      "epoch : 50, train acc : 99.8333 %, train loss : 0.01191341, test acc : 88.1250 %, \n",
      "epoch : 51, train acc : 99.7778 %, train loss : 0.00900033, test acc : 88.1250 %, \n",
      "epoch : 52, train acc : 99.7778 %, train loss : 0.01408764, test acc : 88.1250 %, \n",
      "epoch : 53, train acc : 99.8889 %, train loss : 0.00859037, test acc : 87.5000 %, \n",
      "epoch : 54, train acc : 99.3889 %, train loss : 0.02656564, test acc : 90.0000 %, \n",
      "epoch : 55, train acc : 99.5000 %, train loss : 0.02346587, test acc : 86.8750 %, \n",
      "epoch : 56, train acc : 99.3333 %, train loss : 0.02338986, test acc : 85.0000 %, \n",
      "epoch : 57, train acc : 99.5000 %, train loss : 0.02209041, test acc : 85.6250 %, \n",
      "epoch : 58, train acc : 97.8333 %, train loss : 0.06705605, test acc : 85.6250 %, \n",
      "epoch : 59, train acc : 98.7222 %, train loss : 0.04554831, test acc : 84.3750 %, \n",
      "epoch : 60, train acc : 99.5000 %, train loss : 0.02151110, test acc : 88.1250 %, \n",
      "epoch : 61, train acc : 99.5000 %, train loss : 0.02043508, test acc : 87.5000 %, \n",
      "epoch : 62, train acc : 99.7222 %, train loss : 0.01544120, test acc : 86.2500 %, \n",
      "epoch : 63, train acc : 99.0000 %, train loss : 0.03319548, test acc : 88.7500 %, \n",
      "epoch : 64, train acc : 99.4444 %, train loss : 0.02536111, test acc : 88.1250 %, \n",
      "epoch : 65, train acc : 99.5556 %, train loss : 0.02512064, test acc : 86.2500 %, \n",
      "epoch : 66, train acc : 100.0000 %, train loss : 0.00674288, test acc : 88.1250 %, \n",
      "epoch : 67, train acc : 100.0000 %, train loss : 0.00423111, test acc : 88.1250 %, \n",
      "epoch : 68, train acc : 100.0000 %, train loss : 0.00231737, test acc : 87.5000 %, \n",
      "epoch : 69, train acc : 100.0000 %, train loss : 0.00274041, test acc : 87.5000 %, \n",
      "epoch : 70, train acc : 98.9444 %, train loss : 0.03557227, test acc : 85.6250 %, \n",
      "epoch : 71, train acc : 98.8333 %, train loss : 0.05316242, test acc : 81.8750 %, \n",
      "epoch : 72, train acc : 98.2222 %, train loss : 0.06002318, test acc : 85.6250 %, \n",
      "epoch : 73, train acc : 99.1111 %, train loss : 0.02861530, test acc : 85.0000 %, \n",
      "epoch : 74, train acc : 99.6111 %, train loss : 0.01816346, test acc : 88.1250 %, \n",
      "epoch : 75, train acc : 99.2222 %, train loss : 0.02175095, test acc : 85.6250 %, \n",
      "epoch : 76, train acc : 99.7778 %, train loss : 0.01074881, test acc : 86.2500 %, \n",
      "epoch : 77, train acc : 99.6667 %, train loss : 0.01447275, test acc : 85.6250 %, \n",
      "epoch : 78, train acc : 99.4444 %, train loss : 0.02021110, test acc : 86.8750 %, \n",
      "epoch : 79, train acc : 99.5000 %, train loss : 0.01305283, test acc : 87.5000 %, \n",
      "epoch : 80, train acc : 99.7778 %, train loss : 0.01432607, test acc : 86.2500 %, \n",
      "epoch : 81, train acc : 99.7222 %, train loss : 0.01732480, test acc : 86.8750 %, \n",
      "epoch : 82, train acc : 97.5000 %, train loss : 0.07859335, test acc : 85.0000 %, \n",
      "epoch : 83, train acc : 99.2778 %, train loss : 0.02515393, test acc : 85.6250 %, \n",
      "epoch : 84, train acc : 99.3889 %, train loss : 0.02172090, test acc : 86.8750 %, \n",
      "epoch : 85, train acc : 99.7778 %, train loss : 0.01120046, test acc : 86.8750 %, \n",
      "epoch : 86, train acc : 99.9444 %, train loss : 0.00621777, test acc : 87.5000 %, \n",
      "epoch : 87, train acc : 99.9444 %, train loss : 0.00500972, test acc : 86.8750 %, \n",
      "epoch : 88, train acc : 99.9444 %, train loss : 0.00370669, test acc : 88.1250 %, \n",
      "epoch : 89, train acc : 100.0000 %, train loss : 0.00281985, test acc : 88.1250 %, \n",
      "epoch : 90, train acc : 99.8333 %, train loss : 0.00727177, test acc : 83.1250 %, \n",
      "epoch : 91, train acc : 99.3333 %, train loss : 0.02482467, test acc : 85.6250 %, \n",
      "epoch : 92, train acc : 99.0556 %, train loss : 0.04108969, test acc : 86.8750 %, \n",
      "epoch : 93, train acc : 99.3333 %, train loss : 0.02096181, test acc : 84.3750 %, \n",
      "epoch : 94, train acc : 99.3889 %, train loss : 0.02456748, test acc : 85.0000 %, \n",
      "epoch : 95, train acc : 99.2778 %, train loss : 0.02410849, test acc : 81.2500 %, \n",
      "epoch : 96, train acc : 99.0000 %, train loss : 0.02939337, test acc : 86.2500 %, \n",
      "epoch : 97, train acc : 99.3333 %, train loss : 0.02493782, test acc : 83.1250 %, \n",
      "epoch : 98, train acc : 99.1667 %, train loss : 0.03311366, test acc : 87.5000 %, \n",
      "epoch : 99, train acc : 99.7778 %, train loss : 0.01173631, test acc : 87.5000 %, \n",
      "epoch : 100, train acc : 99.2778 %, train loss : 0.01994043, test acc : 85.0000 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 73.4444 %, train loss : 0.51818954, test acc : 55.6250 %, \n",
      "epoch : 2, train acc : 85.4444 %, train loss : 0.38414692, test acc : 46.8750 %, \n",
      "epoch : 3, train acc : 89.9444 %, train loss : 0.32192545, test acc : 62.5000 %, \n",
      "epoch : 4, train acc : 91.8889 %, train loss : 0.24189466, test acc : 71.8750 %, \n",
      "epoch : 5, train acc : 94.0000 %, train loss : 0.19254396, test acc : 81.2500 %, \n",
      "epoch : 6, train acc : 93.5556 %, train loss : 0.19597359, test acc : 79.3750 %, \n",
      "epoch : 7, train acc : 96.7778 %, train loss : 0.12243668, test acc : 84.3750 %, \n",
      "epoch : 8, train acc : 97.4444 %, train loss : 0.09796777, test acc : 85.0000 %, \n",
      "epoch : 9, train acc : 96.0000 %, train loss : 0.14832715, test acc : 88.7500 %, \n",
      "epoch : 10, train acc : 96.2222 %, train loss : 0.13342558, test acc : 86.2500 %, \n",
      "epoch : 11, train acc : 98.1111 %, train loss : 0.07604637, test acc : 87.5000 %, \n",
      "epoch : 12, train acc : 97.2222 %, train loss : 0.09879233, test acc : 88.1250 %, \n",
      "epoch : 13, train acc : 98.0556 %, train loss : 0.07362004, test acc : 89.3750 %, \n",
      "epoch : 14, train acc : 98.8889 %, train loss : 0.05053844, test acc : 85.0000 %, \n",
      "epoch : 15, train acc : 99.0556 %, train loss : 0.04857450, test acc : 90.0000 %, \n",
      "epoch : 16, train acc : 99.5000 %, train loss : 0.03252106, test acc : 85.6250 %, \n",
      "epoch : 17, train acc : 99.0000 %, train loss : 0.04443675, test acc : 90.6250 %, \n",
      "epoch : 18, train acc : 99.5000 %, train loss : 0.02656493, test acc : 90.0000 %, \n",
      "epoch : 19, train acc : 99.0556 %, train loss : 0.04365695, test acc : 87.5000 %, \n",
      "epoch : 20, train acc : 98.8889 %, train loss : 0.04726917, test acc : 83.7500 %, \n",
      "epoch : 21, train acc : 98.5556 %, train loss : 0.05712069, test acc : 88.1250 %, \n",
      "epoch : 22, train acc : 97.3333 %, train loss : 0.09799693, test acc : 86.2500 %, \n",
      "epoch : 23, train acc : 96.2778 %, train loss : 0.11233206, test acc : 85.6250 %, \n",
      "epoch : 24, train acc : 98.0556 %, train loss : 0.06598953, test acc : 89.3750 %, \n",
      "epoch : 25, train acc : 97.5556 %, train loss : 0.07771162, test acc : 90.6250 %, \n",
      "epoch : 26, train acc : 98.8889 %, train loss : 0.04632677, test acc : 88.1250 %, \n",
      "epoch : 27, train acc : 99.0556 %, train loss : 0.04336851, test acc : 91.8750 %, \n",
      "epoch : 28, train acc : 99.3889 %, train loss : 0.02462177, test acc : 90.6250 %, \n",
      "epoch : 29, train acc : 99.7222 %, train loss : 0.01353404, test acc : 89.3750 %, \n",
      "epoch : 30, train acc : 99.4444 %, train loss : 0.02653604, test acc : 91.2500 %, \n",
      "epoch : 31, train acc : 99.8889 %, train loss : 0.01179712, test acc : 91.2500 %, \n",
      "epoch : 32, train acc : 99.8889 %, train loss : 0.00923760, test acc : 90.6250 %, \n",
      "epoch : 33, train acc : 99.8333 %, train loss : 0.01059447, test acc : 90.0000 %, \n",
      "epoch : 34, train acc : 99.9444 %, train loss : 0.00865902, test acc : 90.0000 %, \n",
      "epoch : 35, train acc : 99.9444 %, train loss : 0.00513437, test acc : 90.0000 %, \n",
      "epoch : 36, train acc : 99.8889 %, train loss : 0.00916031, test acc : 89.3750 %, \n",
      "epoch : 37, train acc : 99.8889 %, train loss : 0.00975943, test acc : 89.3750 %, \n",
      "epoch : 38, train acc : 100.0000 %, train loss : 0.00500102, test acc : 90.6250 %, \n",
      "epoch : 39, train acc : 100.0000 %, train loss : 0.00353636, test acc : 90.6250 %, \n",
      "epoch : 40, train acc : 99.9444 %, train loss : 0.00658691, test acc : 90.0000 %, \n",
      "epoch : 41, train acc : 100.0000 %, train loss : 0.00412813, test acc : 90.0000 %, \n",
      "epoch : 42, train acc : 100.0000 %, train loss : 0.00236458, test acc : 89.3750 %, \n",
      "epoch : 43, train acc : 95.9444 %, train loss : 0.13611465, test acc : 65.6250 %, \n",
      "epoch : 44, train acc : 93.7778 %, train loss : 0.18618924, test acc : 86.2500 %, \n",
      "epoch : 45, train acc : 97.8333 %, train loss : 0.07988577, test acc : 88.7500 %, \n",
      "epoch : 46, train acc : 99.0556 %, train loss : 0.03389916, test acc : 91.8750 %, \n",
      "epoch : 47, train acc : 99.1111 %, train loss : 0.03525086, test acc : 90.0000 %, \n",
      "epoch : 48, train acc : 99.7778 %, train loss : 0.01537627, test acc : 90.0000 %, \n",
      "epoch : 49, train acc : 99.9444 %, train loss : 0.00961417, test acc : 88.7500 %, \n",
      "epoch : 50, train acc : 99.2778 %, train loss : 0.02921599, test acc : 88.7500 %, \n",
      "epoch : 51, train acc : 98.7222 %, train loss : 0.05216457, test acc : 87.5000 %, \n",
      "epoch : 52, train acc : 99.3333 %, train loss : 0.02732625, test acc : 89.3750 %, \n",
      "epoch : 53, train acc : 99.6667 %, train loss : 0.01680958, test acc : 90.0000 %, \n",
      "epoch : 54, train acc : 100.0000 %, train loss : 0.00812738, test acc : 90.0000 %, \n",
      "epoch : 55, train acc : 99.3333 %, train loss : 0.02357773, test acc : 90.6250 %, \n",
      "epoch : 56, train acc : 97.8889 %, train loss : 0.06783549, test acc : 86.2500 %, \n",
      "epoch : 57, train acc : 97.2222 %, train loss : 0.08807763, test acc : 84.3750 %, \n",
      "epoch : 58, train acc : 98.6111 %, train loss : 0.05094220, test acc : 91.8750 %, \n",
      "epoch : 59, train acc : 99.3889 %, train loss : 0.02695851, test acc : 86.8750 %, \n",
      "epoch : 60, train acc : 99.3889 %, train loss : 0.01927571, test acc : 85.0000 %, \n",
      "epoch : 61, train acc : 99.2778 %, train loss : 0.03042489, test acc : 90.0000 %, \n",
      "epoch : 62, train acc : 99.7222 %, train loss : 0.00949560, test acc : 90.0000 %, \n",
      "epoch : 63, train acc : 99.8333 %, train loss : 0.01254144, test acc : 88.1250 %, \n",
      "epoch : 64, train acc : 99.9444 %, train loss : 0.00642162, test acc : 85.6250 %, \n",
      "epoch : 65, train acc : 99.9444 %, train loss : 0.00554803, test acc : 88.1250 %, \n",
      "epoch : 66, train acc : 99.9444 %, train loss : 0.00477193, test acc : 88.1250 %, \n",
      "epoch : 67, train acc : 100.0000 %, train loss : 0.00255844, test acc : 88.1250 %, \n",
      "epoch : 68, train acc : 99.9444 %, train loss : 0.00391023, test acc : 90.0000 %, \n",
      "epoch : 69, train acc : 100.0000 %, train loss : 0.00306081, test acc : 87.5000 %, \n",
      "epoch : 70, train acc : 98.5000 %, train loss : 0.06383205, test acc : 74.3750 %, \n",
      "epoch : 71, train acc : 96.0556 %, train loss : 0.12420402, test acc : 87.5000 %, \n",
      "epoch : 72, train acc : 97.3889 %, train loss : 0.09024027, test acc : 90.0000 %, \n",
      "epoch : 73, train acc : 98.8889 %, train loss : 0.04163540, test acc : 90.0000 %, \n",
      "epoch : 74, train acc : 99.5000 %, train loss : 0.01904986, test acc : 90.0000 %, \n",
      "epoch : 75, train acc : 99.6111 %, train loss : 0.01230961, test acc : 90.6250 %, \n",
      "epoch : 76, train acc : 99.9444 %, train loss : 0.00827432, test acc : 91.8750 %, \n",
      "epoch : 77, train acc : 99.7778 %, train loss : 0.00862011, test acc : 90.0000 %, \n",
      "epoch : 78, train acc : 99.7222 %, train loss : 0.01384957, test acc : 90.0000 %, \n",
      "epoch : 79, train acc : 99.9444 %, train loss : 0.00850147, test acc : 88.7500 %, \n",
      "epoch : 80, train acc : 99.6111 %, train loss : 0.01666057, test acc : 85.6250 %, \n",
      "epoch : 81, train acc : 99.2222 %, train loss : 0.02455007, test acc : 86.8750 %, \n",
      "epoch : 82, train acc : 98.2778 %, train loss : 0.04381364, test acc : 89.3750 %, \n",
      "epoch : 83, train acc : 99.3889 %, train loss : 0.03394464, test acc : 86.8750 %, \n",
      "epoch : 84, train acc : 98.5000 %, train loss : 0.04565550, test acc : 88.1250 %, \n",
      "epoch : 85, train acc : 99.0000 %, train loss : 0.03458578, test acc : 88.1250 %, \n",
      "epoch : 86, train acc : 99.3333 %, train loss : 0.03022049, test acc : 86.8750 %, \n",
      "epoch : 87, train acc : 99.0556 %, train loss : 0.03048665, test acc : 89.3750 %, \n",
      "epoch : 88, train acc : 99.6667 %, train loss : 0.01295306, test acc : 88.1250 %, \n",
      "epoch : 89, train acc : 98.8889 %, train loss : 0.03485924, test acc : 86.2500 %, \n",
      "epoch : 90, train acc : 99.5000 %, train loss : 0.01948076, test acc : 90.0000 %, \n",
      "epoch : 91, train acc : 99.7778 %, train loss : 0.01152915, test acc : 88.7500 %, \n",
      "epoch : 92, train acc : 100.0000 %, train loss : 0.00338753, test acc : 88.7500 %, \n",
      "epoch : 93, train acc : 100.0000 %, train loss : 0.00339499, test acc : 87.5000 %, \n",
      "epoch : 94, train acc : 99.6667 %, train loss : 0.01653595, test acc : 90.6250 %, \n",
      "epoch : 95, train acc : 99.7222 %, train loss : 0.01676409, test acc : 88.1250 %, \n",
      "epoch : 96, train acc : 98.8889 %, train loss : 0.04108379, test acc : 90.6250 %, \n",
      "epoch : 97, train acc : 96.8889 %, train loss : 0.10971627, test acc : 85.6250 %, \n",
      "epoch : 98, train acc : 98.6667 %, train loss : 0.05040382, test acc : 90.6250 %, \n",
      "epoch : 99, train acc : 99.3333 %, train loss : 0.02065145, test acc : 90.0000 %, \n",
      "epoch : 100, train acc : 100.0000 %, train loss : 0.00563707, test acc : 91.8750 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 72.8333 %, train loss : 0.51458784, test acc : 53.1250 %, \n",
      "epoch : 2, train acc : 80.7222 %, train loss : 0.44691687, test acc : 52.5000 %, \n",
      "epoch : 3, train acc : 85.3889 %, train loss : 0.38766452, test acc : 79.3750 %, \n",
      "epoch : 4, train acc : 91.8889 %, train loss : 0.24941553, test acc : 70.6250 %, \n",
      "epoch : 5, train acc : 93.4444 %, train loss : 0.21016357, test acc : 78.7500 %, \n",
      "epoch : 6, train acc : 92.5556 %, train loss : 0.22572841, test acc : 79.3750 %, \n",
      "epoch : 7, train acc : 95.6667 %, train loss : 0.14866366, test acc : 82.5000 %, \n",
      "epoch : 8, train acc : 96.6667 %, train loss : 0.11534254, test acc : 76.2500 %, \n",
      "epoch : 9, train acc : 95.7778 %, train loss : 0.13919127, test acc : 80.6250 %, \n",
      "epoch : 10, train acc : 97.3889 %, train loss : 0.10110742, test acc : 80.6250 %, \n",
      "epoch : 11, train acc : 98.5000 %, train loss : 0.06485194, test acc : 77.5000 %, \n",
      "epoch : 12, train acc : 98.8889 %, train loss : 0.04679253, test acc : 82.5000 %, \n",
      "epoch : 13, train acc : 98.5556 %, train loss : 0.05494885, test acc : 83.7500 %, \n",
      "epoch : 14, train acc : 97.4444 %, train loss : 0.09169058, test acc : 82.5000 %, \n",
      "epoch : 15, train acc : 98.2778 %, train loss : 0.06496324, test acc : 80.0000 %, \n",
      "epoch : 16, train acc : 98.7222 %, train loss : 0.05808780, test acc : 86.2500 %, \n",
      "epoch : 17, train acc : 98.0556 %, train loss : 0.07424926, test acc : 87.5000 %, \n",
      "epoch : 18, train acc : 97.5000 %, train loss : 0.07640012, test acc : 81.2500 %, \n",
      "epoch : 19, train acc : 98.4444 %, train loss : 0.05774798, test acc : 86.2500 %, \n",
      "epoch : 20, train acc : 99.3333 %, train loss : 0.03532639, test acc : 86.8750 %, \n",
      "epoch : 21, train acc : 99.9444 %, train loss : 0.01213655, test acc : 85.0000 %, \n",
      "epoch : 22, train acc : 98.5556 %, train loss : 0.05085588, test acc : 84.3750 %, \n",
      "epoch : 23, train acc : 99.3333 %, train loss : 0.03598828, test acc : 88.1250 %, \n",
      "epoch : 24, train acc : 98.9444 %, train loss : 0.04495378, test acc : 87.5000 %, \n",
      "epoch : 25, train acc : 98.8333 %, train loss : 0.04172661, test acc : 88.7500 %, \n",
      "epoch : 26, train acc : 99.2778 %, train loss : 0.03789102, test acc : 83.7500 %, \n",
      "epoch : 27, train acc : 99.2778 %, train loss : 0.03562489, test acc : 87.5000 %, \n",
      "epoch : 28, train acc : 99.6667 %, train loss : 0.02600377, test acc : 86.2500 %, \n",
      "epoch : 29, train acc : 99.1667 %, train loss : 0.03381341, test acc : 86.8750 %, \n",
      "epoch : 30, train acc : 99.3333 %, train loss : 0.02942327, test acc : 85.6250 %, \n",
      "epoch : 31, train acc : 97.2778 %, train loss : 0.10613087, test acc : 78.7500 %, \n",
      "epoch : 32, train acc : 98.4444 %, train loss : 0.06596079, test acc : 88.1250 %, \n",
      "epoch : 33, train acc : 99.3889 %, train loss : 0.03345701, test acc : 87.5000 %, \n",
      "epoch : 34, train acc : 98.5000 %, train loss : 0.05587063, test acc : 85.0000 %, \n",
      "epoch : 35, train acc : 99.3333 %, train loss : 0.02590958, test acc : 89.3750 %, \n",
      "epoch : 36, train acc : 99.4444 %, train loss : 0.02598294, test acc : 90.0000 %, \n",
      "epoch : 37, train acc : 99.0000 %, train loss : 0.03123375, test acc : 89.3750 %, \n",
      "epoch : 38, train acc : 99.7222 %, train loss : 0.01585001, test acc : 88.7500 %, \n",
      "epoch : 39, train acc : 99.8333 %, train loss : 0.01527573, test acc : 89.3750 %, \n",
      "epoch : 40, train acc : 99.9444 %, train loss : 0.00834832, test acc : 89.3750 %, \n",
      "epoch : 41, train acc : 97.6111 %, train loss : 0.07384003, test acc : 85.6250 %, \n",
      "epoch : 42, train acc : 99.3333 %, train loss : 0.02848162, test acc : 86.8750 %, \n",
      "epoch : 43, train acc : 99.6667 %, train loss : 0.01919495, test acc : 88.1250 %, \n",
      "epoch : 44, train acc : 99.6111 %, train loss : 0.01594100, test acc : 86.8750 %, \n",
      "epoch : 45, train acc : 99.3333 %, train loss : 0.02938106, test acc : 84.3750 %, \n",
      "epoch : 46, train acc : 98.3333 %, train loss : 0.05674685, test acc : 85.0000 %, \n",
      "epoch : 47, train acc : 98.9444 %, train loss : 0.04006272, test acc : 85.6250 %, \n",
      "epoch : 48, train acc : 99.5556 %, train loss : 0.02003211, test acc : 88.1250 %, \n",
      "epoch : 49, train acc : 99.6111 %, train loss : 0.01672493, test acc : 88.7500 %, \n",
      "epoch : 50, train acc : 99.9444 %, train loss : 0.00771974, test acc : 88.7500 %, \n",
      "epoch : 51, train acc : 99.7222 %, train loss : 0.01053703, test acc : 86.8750 %, \n",
      "epoch : 52, train acc : 99.9444 %, train loss : 0.00782058, test acc : 88.1250 %, \n",
      "epoch : 53, train acc : 99.8889 %, train loss : 0.00737778, test acc : 86.8750 %, \n",
      "epoch : 54, train acc : 99.8889 %, train loss : 0.00862301, test acc : 86.8750 %, \n",
      "epoch : 55, train acc : 99.7778 %, train loss : 0.00990379, test acc : 86.8750 %, \n",
      "epoch : 56, train acc : 99.7778 %, train loss : 0.00885738, test acc : 87.5000 %, \n",
      "epoch : 57, train acc : 99.6667 %, train loss : 0.02029650, test acc : 88.1250 %, \n",
      "epoch : 58, train acc : 96.8333 %, train loss : 0.08717091, test acc : 60.6250 %, \n",
      "epoch : 59, train acc : 98.1111 %, train loss : 0.07026884, test acc : 84.3750 %, \n",
      "epoch : 60, train acc : 99.1667 %, train loss : 0.03980620, test acc : 85.6250 %, \n",
      "epoch : 61, train acc : 99.6111 %, train loss : 0.02359229, test acc : 85.0000 %, \n",
      "epoch : 62, train acc : 99.7778 %, train loss : 0.01272585, test acc : 88.1250 %, \n",
      "epoch : 63, train acc : 100.0000 %, train loss : 0.00550688, test acc : 86.8750 %, \n",
      "epoch : 64, train acc : 99.8333 %, train loss : 0.00951988, test acc : 88.7500 %, \n",
      "epoch : 65, train acc : 99.9444 %, train loss : 0.00502629, test acc : 88.1250 %, \n",
      "epoch : 66, train acc : 99.8889 %, train loss : 0.00412838, test acc : 89.3750 %, \n",
      "epoch : 67, train acc : 100.0000 %, train loss : 0.00256420, test acc : 90.0000 %, \n",
      "epoch : 68, train acc : 100.0000 %, train loss : 0.00284648, test acc : 89.3750 %, \n",
      "epoch : 69, train acc : 100.0000 %, train loss : 0.00236854, test acc : 88.7500 %, \n",
      "epoch : 70, train acc : 100.0000 %, train loss : 0.00200649, test acc : 88.7500 %, \n",
      "epoch : 71, train acc : 99.1111 %, train loss : 0.02743102, test acc : 85.6250 %, \n",
      "epoch : 72, train acc : 97.7222 %, train loss : 0.07429835, test acc : 83.1250 %, \n",
      "epoch : 73, train acc : 97.8333 %, train loss : 0.07077673, test acc : 81.8750 %, \n",
      "epoch : 74, train acc : 99.2778 %, train loss : 0.02843454, test acc : 86.2500 %, \n",
      "epoch : 75, train acc : 99.2778 %, train loss : 0.02173242, test acc : 86.2500 %, \n",
      "epoch : 76, train acc : 99.9444 %, train loss : 0.00718517, test acc : 85.0000 %, \n",
      "epoch : 77, train acc : 99.7222 %, train loss : 0.01085681, test acc : 86.8750 %, \n",
      "epoch : 78, train acc : 99.9444 %, train loss : 0.00519714, test acc : 84.3750 %, \n",
      "epoch : 79, train acc : 100.0000 %, train loss : 0.00318750, test acc : 86.2500 %, \n",
      "epoch : 80, train acc : 99.5556 %, train loss : 0.01857965, test acc : 85.6250 %, \n",
      "epoch : 81, train acc : 98.5556 %, train loss : 0.04474518, test acc : 83.7500 %, \n",
      "epoch : 82, train acc : 98.7778 %, train loss : 0.03911659, test acc : 84.3750 %, \n",
      "epoch : 83, train acc : 99.2778 %, train loss : 0.02542140, test acc : 83.1250 %, \n",
      "epoch : 84, train acc : 99.7778 %, train loss : 0.00988294, test acc : 86.8750 %, \n",
      "epoch : 85, train acc : 99.9444 %, train loss : 0.00642719, test acc : 86.8750 %, \n",
      "epoch : 86, train acc : 100.0000 %, train loss : 0.00245698, test acc : 86.2500 %, \n",
      "epoch : 87, train acc : 100.0000 %, train loss : 0.00256570, test acc : 86.8750 %, \n",
      "epoch : 88, train acc : 100.0000 %, train loss : 0.00197842, test acc : 87.5000 %, \n",
      "epoch : 89, train acc : 100.0000 %, train loss : 0.00171411, test acc : 86.2500 %, \n",
      "epoch : 90, train acc : 100.0000 %, train loss : 0.00190851, test acc : 85.6250 %, \n",
      "epoch : 91, train acc : 100.0000 %, train loss : 0.00167970, test acc : 87.5000 %, \n",
      "epoch : 92, train acc : 99.9444 %, train loss : 0.00343573, test acc : 85.0000 %, \n",
      "epoch : 93, train acc : 97.6111 %, train loss : 0.08607076, test acc : 85.0000 %, \n",
      "epoch : 94, train acc : 98.1667 %, train loss : 0.07426072, test acc : 84.3750 %, \n",
      "epoch : 95, train acc : 98.1111 %, train loss : 0.06243028, test acc : 84.3750 %, \n",
      "epoch : 96, train acc : 99.3889 %, train loss : 0.02221631, test acc : 85.0000 %, \n",
      "epoch : 97, train acc : 99.8889 %, train loss : 0.01012234, test acc : 87.5000 %, \n",
      "epoch : 98, train acc : 99.8333 %, train loss : 0.00693820, test acc : 88.1250 %, \n",
      "epoch : 99, train acc : 99.8889 %, train loss : 0.00682921, test acc : 87.5000 %, \n",
      "epoch : 100, train acc : 100.0000 %, train loss : 0.00329663, test acc : 86.8750 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 74.5000 %, train loss : 0.52824847, test acc : 51.2500 %, \n",
      "epoch : 2, train acc : 83.3333 %, train loss : 0.43227352, test acc : 47.5000 %, \n",
      "epoch : 3, train acc : 86.3889 %, train loss : 0.37085471, test acc : 65.0000 %, \n",
      "epoch : 4, train acc : 92.0556 %, train loss : 0.24121390, test acc : 63.7500 %, \n",
      "epoch : 5, train acc : 93.5000 %, train loss : 0.22161936, test acc : 78.7500 %, \n",
      "epoch : 6, train acc : 95.3889 %, train loss : 0.15537118, test acc : 69.3750 %, \n",
      "epoch : 7, train acc : 96.2222 %, train loss : 0.13122597, test acc : 80.0000 %, \n",
      "epoch : 8, train acc : 97.3889 %, train loss : 0.10603241, test acc : 85.0000 %, \n",
      "epoch : 9, train acc : 96.6667 %, train loss : 0.12228921, test acc : 80.6250 %, \n",
      "epoch : 10, train acc : 97.1111 %, train loss : 0.10076024, test acc : 85.6250 %, \n",
      "epoch : 11, train acc : 97.9444 %, train loss : 0.08152966, test acc : 83.7500 %, \n",
      "epoch : 12, train acc : 98.3333 %, train loss : 0.06354910, test acc : 85.6250 %, \n",
      "epoch : 13, train acc : 98.0556 %, train loss : 0.06836497, test acc : 88.1250 %, \n",
      "epoch : 14, train acc : 98.5000 %, train loss : 0.05310310, test acc : 84.3750 %, \n",
      "epoch : 15, train acc : 99.1111 %, train loss : 0.03855304, test acc : 83.1250 %, \n",
      "epoch : 16, train acc : 98.7222 %, train loss : 0.04986309, test acc : 85.0000 %, \n",
      "epoch : 17, train acc : 98.6111 %, train loss : 0.05583058, test acc : 78.1250 %, \n",
      "epoch : 18, train acc : 98.2778 %, train loss : 0.06831242, test acc : 85.0000 %, \n",
      "epoch : 19, train acc : 98.9444 %, train loss : 0.04512259, test acc : 83.7500 %, \n",
      "epoch : 20, train acc : 97.2222 %, train loss : 0.09549086, test acc : 85.0000 %, \n",
      "epoch : 21, train acc : 98.5000 %, train loss : 0.05992930, test acc : 85.6250 %, \n",
      "epoch : 22, train acc : 98.0556 %, train loss : 0.06752610, test acc : 86.2500 %, \n",
      "epoch : 23, train acc : 99.2778 %, train loss : 0.03306912, test acc : 86.8750 %, \n",
      "epoch : 24, train acc : 99.0000 %, train loss : 0.04616182, test acc : 83.7500 %, \n",
      "epoch : 25, train acc : 99.1667 %, train loss : 0.03239862, test acc : 86.2500 %, \n",
      "epoch : 26, train acc : 99.0000 %, train loss : 0.03387835, test acc : 85.6250 %, \n",
      "epoch : 27, train acc : 98.2778 %, train loss : 0.05561304, test acc : 87.5000 %, \n",
      "epoch : 28, train acc : 99.4444 %, train loss : 0.03525783, test acc : 86.8750 %, \n",
      "epoch : 29, train acc : 99.6667 %, train loss : 0.02225605, test acc : 86.8750 %, \n",
      "epoch : 30, train acc : 99.5556 %, train loss : 0.02231656, test acc : 81.8750 %, \n",
      "epoch : 31, train acc : 98.2778 %, train loss : 0.07250235, test acc : 85.0000 %, \n",
      "epoch : 32, train acc : 98.5556 %, train loss : 0.04607228, test acc : 85.0000 %, \n",
      "epoch : 33, train acc : 98.7778 %, train loss : 0.03896247, test acc : 84.3750 %, \n",
      "epoch : 34, train acc : 99.2778 %, train loss : 0.02811803, test acc : 84.3750 %, \n",
      "epoch : 35, train acc : 99.2778 %, train loss : 0.02802807, test acc : 87.5000 %, \n",
      "epoch : 36, train acc : 99.9444 %, train loss : 0.01022007, test acc : 86.2500 %, \n",
      "epoch : 37, train acc : 99.1667 %, train loss : 0.02948312, test acc : 83.7500 %, \n",
      "epoch : 38, train acc : 98.1667 %, train loss : 0.07229594, test acc : 83.7500 %, \n",
      "epoch : 39, train acc : 97.6667 %, train loss : 0.08190419, test acc : 83.7500 %, \n",
      "epoch : 40, train acc : 99.0000 %, train loss : 0.03306512, test acc : 83.1250 %, \n",
      "epoch : 41, train acc : 98.8889 %, train loss : 0.03932388, test acc : 84.3750 %, \n",
      "epoch : 42, train acc : 99.5000 %, train loss : 0.02099864, test acc : 85.0000 %, \n",
      "epoch : 43, train acc : 99.3889 %, train loss : 0.02968664, test acc : 83.7500 %, \n",
      "epoch : 44, train acc : 99.9444 %, train loss : 0.00754354, test acc : 85.6250 %, \n",
      "epoch : 45, train acc : 99.8333 %, train loss : 0.01324150, test acc : 82.5000 %, \n",
      "epoch : 46, train acc : 99.9444 %, train loss : 0.00643002, test acc : 84.3750 %, \n",
      "epoch : 47, train acc : 99.9444 %, train loss : 0.00528325, test acc : 85.0000 %, \n",
      "epoch : 48, train acc : 99.9444 %, train loss : 0.00568362, test acc : 84.3750 %, \n",
      "epoch : 49, train acc : 100.0000 %, train loss : 0.00341590, test acc : 84.3750 %, \n",
      "epoch : 50, train acc : 99.8889 %, train loss : 0.00570083, test acc : 85.0000 %, \n",
      "epoch : 51, train acc : 100.0000 %, train loss : 0.00402302, test acc : 85.0000 %, \n",
      "epoch : 52, train acc : 100.0000 %, train loss : 0.00193072, test acc : 85.0000 %, \n",
      "epoch : 53, train acc : 97.6667 %, train loss : 0.08635440, test acc : 74.3750 %, \n",
      "epoch : 54, train acc : 94.8889 %, train loss : 0.15921633, test acc : 81.8750 %, \n",
      "epoch : 55, train acc : 98.2778 %, train loss : 0.05051440, test acc : 86.8750 %, \n",
      "epoch : 56, train acc : 99.0556 %, train loss : 0.03194225, test acc : 83.7500 %, \n",
      "epoch : 57, train acc : 99.5000 %, train loss : 0.02385979, test acc : 85.6250 %, \n",
      "epoch : 58, train acc : 99.7778 %, train loss : 0.01268368, test acc : 84.3750 %, \n",
      "epoch : 59, train acc : 99.6667 %, train loss : 0.01396788, test acc : 84.3750 %, \n",
      "epoch : 60, train acc : 99.7222 %, train loss : 0.00829210, test acc : 85.6250 %, \n",
      "epoch : 61, train acc : 99.9444 %, train loss : 0.00717029, test acc : 85.0000 %, \n",
      "epoch : 62, train acc : 100.0000 %, train loss : 0.00330757, test acc : 85.0000 %, \n",
      "epoch : 63, train acc : 100.0000 %, train loss : 0.00240827, test acc : 84.3750 %, \n",
      "epoch : 64, train acc : 100.0000 %, train loss : 0.00303710, test acc : 85.0000 %, \n",
      "epoch : 65, train acc : 100.0000 %, train loss : 0.00247584, test acc : 84.3750 %, \n",
      "epoch : 66, train acc : 100.0000 %, train loss : 0.00194166, test acc : 85.0000 %, \n",
      "epoch : 67, train acc : 99.8889 %, train loss : 0.00605020, test acc : 83.1250 %, \n",
      "epoch : 68, train acc : 98.1111 %, train loss : 0.06597524, test acc : 84.3750 %, \n",
      "epoch : 69, train acc : 96.7222 %, train loss : 0.10047934, test acc : 84.3750 %, \n",
      "epoch : 70, train acc : 98.3889 %, train loss : 0.05259281, test acc : 87.5000 %, \n",
      "epoch : 71, train acc : 99.2222 %, train loss : 0.03105700, test acc : 88.1250 %, \n",
      "epoch : 72, train acc : 99.7778 %, train loss : 0.01506899, test acc : 87.5000 %, \n",
      "epoch : 73, train acc : 99.8889 %, train loss : 0.01001709, test acc : 86.2500 %, \n",
      "epoch : 74, train acc : 100.0000 %, train loss : 0.00462200, test acc : 86.8750 %, \n",
      "epoch : 75, train acc : 99.8889 %, train loss : 0.00570759, test acc : 86.2500 %, \n",
      "epoch : 76, train acc : 99.6111 %, train loss : 0.02194841, test acc : 85.6250 %, \n",
      "epoch : 77, train acc : 99.6667 %, train loss : 0.01262858, test acc : 86.2500 %, \n",
      "epoch : 78, train acc : 98.8333 %, train loss : 0.04243972, test acc : 85.0000 %, \n",
      "epoch : 79, train acc : 97.7778 %, train loss : 0.06270774, test acc : 84.3750 %, \n",
      "epoch : 80, train acc : 99.6111 %, train loss : 0.01933682, test acc : 84.3750 %, \n",
      "epoch : 81, train acc : 99.4444 %, train loss : 0.01969515, test acc : 85.0000 %, \n",
      "epoch : 82, train acc : 99.2778 %, train loss : 0.03146175, test acc : 82.5000 %, \n",
      "epoch : 83, train acc : 98.3889 %, train loss : 0.04473299, test acc : 83.1250 %, \n",
      "epoch : 84, train acc : 98.5556 %, train loss : 0.04899526, test acc : 86.2500 %, \n",
      "epoch : 85, train acc : 99.0000 %, train loss : 0.03115264, test acc : 81.8750 %, \n",
      "epoch : 86, train acc : 99.6111 %, train loss : 0.01649465, test acc : 86.8750 %, \n",
      "epoch : 87, train acc : 99.8333 %, train loss : 0.00779452, test acc : 86.8750 %, \n",
      "epoch : 88, train acc : 100.0000 %, train loss : 0.00379501, test acc : 87.5000 %, \n",
      "epoch : 89, train acc : 99.7778 %, train loss : 0.00860030, test acc : 85.6250 %, \n",
      "epoch : 90, train acc : 100.0000 %, train loss : 0.00262542, test acc : 85.6250 %, \n",
      "epoch : 91, train acc : 99.8333 %, train loss : 0.00648666, test acc : 86.2500 %, \n",
      "epoch : 92, train acc : 100.0000 %, train loss : 0.00304507, test acc : 85.0000 %, \n",
      "epoch : 93, train acc : 100.0000 %, train loss : 0.00235808, test acc : 85.0000 %, \n",
      "epoch : 94, train acc : 100.0000 %, train loss : 0.00207254, test acc : 85.6250 %, \n",
      "epoch : 95, train acc : 99.9444 %, train loss : 0.00545506, test acc : 84.3750 %, \n",
      "epoch : 96, train acc : 98.3889 %, train loss : 0.04158808, test acc : 81.8750 %, \n",
      "epoch : 97, train acc : 95.1111 %, train loss : 0.15901793, test acc : 79.3750 %, \n",
      "epoch : 98, train acc : 98.3333 %, train loss : 0.06252248, test acc : 83.1250 %, \n",
      "epoch : 99, train acc : 99.3889 %, train loss : 0.02835685, test acc : 83.1250 %, \n",
      "epoch : 100, train acc : 99.6111 %, train loss : 0.01248801, test acc : 85.0000 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 71.5556 %, train loss : 0.52881751, test acc : 51.8750 %, \n",
      "epoch : 2, train acc : 79.8333 %, train loss : 0.45990356, test acc : 53.1250 %, \n",
      "epoch : 3, train acc : 86.1667 %, train loss : 0.37532765, test acc : 65.6250 %, \n",
      "epoch : 4, train acc : 90.9444 %, train loss : 0.28921101, test acc : 68.7500 %, \n",
      "epoch : 5, train acc : 92.4444 %, train loss : 0.23083228, test acc : 68.1250 %, \n",
      "epoch : 6, train acc : 95.1111 %, train loss : 0.17117848, test acc : 77.5000 %, \n",
      "epoch : 7, train acc : 96.1667 %, train loss : 0.13816030, test acc : 80.6250 %, \n",
      "epoch : 8, train acc : 97.1111 %, train loss : 0.10578626, test acc : 83.7500 %, \n",
      "epoch : 9, train acc : 98.1667 %, train loss : 0.07112333, test acc : 83.7500 %, \n",
      "epoch : 10, train acc : 98.1111 %, train loss : 0.07794765, test acc : 85.0000 %, \n",
      "epoch : 11, train acc : 97.0000 %, train loss : 0.11317338, test acc : 87.5000 %, \n",
      "epoch : 12, train acc : 98.5000 %, train loss : 0.06842759, test acc : 88.7500 %, \n",
      "epoch : 13, train acc : 98.5000 %, train loss : 0.06528323, test acc : 84.3750 %, \n",
      "epoch : 14, train acc : 98.5556 %, train loss : 0.05960644, test acc : 87.5000 %, \n",
      "epoch : 15, train acc : 98.7222 %, train loss : 0.05413516, test acc : 85.6250 %, \n",
      "epoch : 16, train acc : 99.0000 %, train loss : 0.04694566, test acc : 86.8750 %, \n",
      "epoch : 17, train acc : 98.7778 %, train loss : 0.05096991, test acc : 88.1250 %, \n",
      "epoch : 18, train acc : 98.2778 %, train loss : 0.07089400, test acc : 83.1250 %, \n",
      "epoch : 19, train acc : 97.7778 %, train loss : 0.08047682, test acc : 83.7500 %, \n",
      "epoch : 20, train acc : 97.9444 %, train loss : 0.07865705, test acc : 86.2500 %, \n",
      "epoch : 21, train acc : 98.6111 %, train loss : 0.07090337, test acc : 86.8750 %, \n",
      "epoch : 22, train acc : 98.9444 %, train loss : 0.04843081, test acc : 88.1250 %, \n",
      "epoch : 23, train acc : 99.3889 %, train loss : 0.02759461, test acc : 89.3750 %, \n",
      "epoch : 24, train acc : 99.1667 %, train loss : 0.03328369, test acc : 90.6250 %, \n",
      "epoch : 25, train acc : 98.7222 %, train loss : 0.05381091, test acc : 90.6250 %, \n",
      "epoch : 26, train acc : 99.3889 %, train loss : 0.02590043, test acc : 88.1250 %, \n",
      "epoch : 27, train acc : 99.7222 %, train loss : 0.02122318, test acc : 90.6250 %, \n",
      "epoch : 28, train acc : 99.1667 %, train loss : 0.03304130, test acc : 87.5000 %, \n",
      "epoch : 29, train acc : 100.0000 %, train loss : 0.00974081, test acc : 86.8750 %, \n",
      "epoch : 30, train acc : 99.6667 %, train loss : 0.01495659, test acc : 85.6250 %, \n",
      "epoch : 31, train acc : 98.8333 %, train loss : 0.04629023, test acc : 85.0000 %, \n",
      "epoch : 32, train acc : 97.7222 %, train loss : 0.08520208, test acc : 86.8750 %, \n",
      "epoch : 33, train acc : 97.8889 %, train loss : 0.06827337, test acc : 82.5000 %, \n",
      "epoch : 34, train acc : 99.3889 %, train loss : 0.03328248, test acc : 88.1250 %, \n",
      "epoch : 35, train acc : 99.8333 %, train loss : 0.01380567, test acc : 88.1250 %, \n",
      "epoch : 36, train acc : 99.7222 %, train loss : 0.01824492, test acc : 89.3750 %, \n",
      "epoch : 37, train acc : 99.3889 %, train loss : 0.02342144, test acc : 89.3750 %, \n",
      "epoch : 38, train acc : 99.7222 %, train loss : 0.01525282, test acc : 89.3750 %, \n",
      "epoch : 39, train acc : 99.9444 %, train loss : 0.00814565, test acc : 88.7500 %, \n",
      "epoch : 40, train acc : 100.0000 %, train loss : 0.00471265, test acc : 88.7500 %, \n",
      "epoch : 41, train acc : 99.4444 %, train loss : 0.02819591, test acc : 86.8750 %, \n",
      "epoch : 42, train acc : 96.7222 %, train loss : 0.11041635, test acc : 87.5000 %, \n",
      "epoch : 43, train acc : 98.2778 %, train loss : 0.05572169, test acc : 85.0000 %, \n",
      "epoch : 44, train acc : 99.1111 %, train loss : 0.03507762, test acc : 86.8750 %, \n",
      "epoch : 45, train acc : 99.6667 %, train loss : 0.01817839, test acc : 90.0000 %, \n",
      "epoch : 46, train acc : 99.6111 %, train loss : 0.01771527, test acc : 88.7500 %, \n",
      "epoch : 47, train acc : 99.2778 %, train loss : 0.02686683, test acc : 88.7500 %, \n",
      "epoch : 48, train acc : 98.8333 %, train loss : 0.03880826, test acc : 89.3750 %, \n",
      "epoch : 49, train acc : 98.8889 %, train loss : 0.03805306, test acc : 87.5000 %, \n",
      "epoch : 50, train acc : 98.6111 %, train loss : 0.04842724, test acc : 88.1250 %, \n",
      "epoch : 51, train acc : 99.4444 %, train loss : 0.02463832, test acc : 88.7500 %, \n",
      "epoch : 52, train acc : 99.7778 %, train loss : 0.01323226, test acc : 89.3750 %, \n",
      "epoch : 53, train acc : 99.9444 %, train loss : 0.00516284, test acc : 90.6250 %, \n",
      "epoch : 54, train acc : 99.9444 %, train loss : 0.00559139, test acc : 88.1250 %, \n",
      "epoch : 55, train acc : 100.0000 %, train loss : 0.00512536, test acc : 90.6250 %, \n",
      "epoch : 56, train acc : 99.7778 %, train loss : 0.01227650, test acc : 91.2500 %, \n",
      "epoch : 57, train acc : 99.8889 %, train loss : 0.00890697, test acc : 90.6250 %, \n",
      "epoch : 58, train acc : 99.8333 %, train loss : 0.00889778, test acc : 91.2500 %, \n",
      "epoch : 59, train acc : 99.9444 %, train loss : 0.00386994, test acc : 90.0000 %, \n",
      "epoch : 60, train acc : 100.0000 %, train loss : 0.00281744, test acc : 90.0000 %, \n",
      "epoch : 61, train acc : 99.3333 %, train loss : 0.02092649, test acc : 90.6250 %, \n",
      "epoch : 62, train acc : 94.0000 %, train loss : 0.16872028, test acc : 86.8750 %, \n",
      "epoch : 63, train acc : 96.4444 %, train loss : 0.10039680, test acc : 86.8750 %, \n",
      "epoch : 64, train acc : 98.6111 %, train loss : 0.05589746, test acc : 88.1250 %, \n",
      "epoch : 65, train acc : 99.8333 %, train loss : 0.01255198, test acc : 87.5000 %, \n",
      "epoch : 66, train acc : 99.7778 %, train loss : 0.00978471, test acc : 88.7500 %, \n",
      "epoch : 67, train acc : 99.6111 %, train loss : 0.01531725, test acc : 89.3750 %, \n",
      "epoch : 68, train acc : 99.3889 %, train loss : 0.02730298, test acc : 86.2500 %, \n",
      "epoch : 69, train acc : 98.3889 %, train loss : 0.05303291, test acc : 88.1250 %, \n",
      "epoch : 70, train acc : 99.1667 %, train loss : 0.03218062, test acc : 90.0000 %, \n",
      "epoch : 71, train acc : 99.6667 %, train loss : 0.01756301, test acc : 90.6250 %, \n",
      "epoch : 72, train acc : 99.8889 %, train loss : 0.00765363, test acc : 89.3750 %, \n",
      "epoch : 73, train acc : 99.8889 %, train loss : 0.00775396, test acc : 90.6250 %, \n",
      "epoch : 74, train acc : 100.0000 %, train loss : 0.00358377, test acc : 91.2500 %, \n",
      "epoch : 75, train acc : 100.0000 %, train loss : 0.00236064, test acc : 91.2500 %, \n",
      "epoch : 76, train acc : 100.0000 %, train loss : 0.00211610, test acc : 89.3750 %, \n",
      "epoch : 77, train acc : 100.0000 %, train loss : 0.00201718, test acc : 88.7500 %, \n",
      "epoch : 78, train acc : 100.0000 %, train loss : 0.00266699, test acc : 90.0000 %, \n",
      "epoch : 79, train acc : 100.0000 %, train loss : 0.00250045, test acc : 90.0000 %, \n",
      "epoch : 80, train acc : 100.0000 %, train loss : 0.00156918, test acc : 90.0000 %, \n",
      "epoch : 81, train acc : 100.0000 %, train loss : 0.00216681, test acc : 91.2500 %, \n",
      "epoch : 82, train acc : 100.0000 %, train loss : 0.00210486, test acc : 91.2500 %, \n",
      "epoch : 83, train acc : 99.0556 %, train loss : 0.03318993, test acc : 75.6250 %, \n",
      "epoch : 84, train acc : 94.4444 %, train loss : 0.19339002, test acc : 85.0000 %, \n",
      "epoch : 85, train acc : 98.1111 %, train loss : 0.06327308, test acc : 86.8750 %, \n",
      "epoch : 86, train acc : 98.6111 %, train loss : 0.05278919, test acc : 86.8750 %, \n",
      "epoch : 87, train acc : 99.0000 %, train loss : 0.03605450, test acc : 87.5000 %, \n",
      "epoch : 88, train acc : 99.7222 %, train loss : 0.01345228, test acc : 91.2500 %, \n",
      "epoch : 89, train acc : 99.7778 %, train loss : 0.00921424, test acc : 90.0000 %, \n",
      "epoch : 90, train acc : 99.5556 %, train loss : 0.01848355, test acc : 90.6250 %, \n",
      "epoch : 91, train acc : 99.8333 %, train loss : 0.00895047, test acc : 89.3750 %, \n",
      "epoch : 92, train acc : 100.0000 %, train loss : 0.00451561, test acc : 91.8750 %, \n",
      "epoch : 93, train acc : 99.8889 %, train loss : 0.00671196, test acc : 91.8750 %, \n",
      "epoch : 94, train acc : 99.7222 %, train loss : 0.01530083, test acc : 86.8750 %, \n",
      "epoch : 95, train acc : 99.6667 %, train loss : 0.01546288, test acc : 90.6250 %, \n",
      "epoch : 96, train acc : 99.2222 %, train loss : 0.02334705, test acc : 88.1250 %, \n",
      "epoch : 97, train acc : 99.9444 %, train loss : 0.00721462, test acc : 89.3750 %, \n",
      "epoch : 98, train acc : 99.1667 %, train loss : 0.02507809, test acc : 85.0000 %, \n",
      "epoch : 99, train acc : 96.6111 %, train loss : 0.09683695, test acc : 84.3750 %, \n",
      "epoch : 100, train acc : 98.1667 %, train loss : 0.05569164, test acc : 88.1250 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 71.2778 %, train loss : 0.53759688, test acc : 45.6250 %, \n",
      "epoch : 2, train acc : 83.8333 %, train loss : 0.41471110, test acc : 56.2500 %, \n",
      "epoch : 3, train acc : 87.8333 %, train loss : 0.32244856, test acc : 57.5000 %, \n",
      "epoch : 4, train acc : 93.5000 %, train loss : 0.22653831, test acc : 73.1250 %, \n",
      "epoch : 5, train acc : 94.8889 %, train loss : 0.18056780, test acc : 73.1250 %, \n",
      "epoch : 6, train acc : 94.9444 %, train loss : 0.17695621, test acc : 75.6250 %, \n",
      "epoch : 7, train acc : 96.3889 %, train loss : 0.12843177, test acc : 77.5000 %, \n",
      "epoch : 8, train acc : 97.1667 %, train loss : 0.10149798, test acc : 80.6250 %, \n",
      "epoch : 9, train acc : 97.6667 %, train loss : 0.08923865, test acc : 81.2500 %, \n",
      "epoch : 10, train acc : 98.6111 %, train loss : 0.06223138, test acc : 84.3750 %, \n",
      "epoch : 11, train acc : 97.8889 %, train loss : 0.07181943, test acc : 85.0000 %, \n",
      "epoch : 12, train acc : 98.6111 %, train loss : 0.06291394, test acc : 81.8750 %, \n",
      "epoch : 13, train acc : 98.5000 %, train loss : 0.05583921, test acc : 77.5000 %, \n",
      "epoch : 14, train acc : 97.5556 %, train loss : 0.08634547, test acc : 82.5000 %, \n",
      "epoch : 15, train acc : 98.8333 %, train loss : 0.05613073, test acc : 85.6250 %, \n",
      "epoch : 16, train acc : 98.0556 %, train loss : 0.07831666, test acc : 84.3750 %, \n",
      "epoch : 17, train acc : 99.1111 %, train loss : 0.03712876, test acc : 83.7500 %, \n",
      "epoch : 18, train acc : 97.3333 %, train loss : 0.09696442, test acc : 83.1250 %, \n",
      "epoch : 19, train acc : 98.9444 %, train loss : 0.04479921, test acc : 85.0000 %, \n",
      "epoch : 20, train acc : 98.9444 %, train loss : 0.04420869, test acc : 81.8750 %, \n",
      "epoch : 21, train acc : 98.6111 %, train loss : 0.05141578, test acc : 83.7500 %, \n",
      "epoch : 22, train acc : 98.8889 %, train loss : 0.04823569, test acc : 83.7500 %, \n",
      "epoch : 23, train acc : 97.3333 %, train loss : 0.08860202, test acc : 86.2500 %, \n",
      "epoch : 24, train acc : 97.8889 %, train loss : 0.07869832, test acc : 88.7500 %, \n",
      "epoch : 25, train acc : 98.9444 %, train loss : 0.04134140, test acc : 78.7500 %, \n",
      "epoch : 26, train acc : 99.5556 %, train loss : 0.02124352, test acc : 87.5000 %, \n",
      "epoch : 27, train acc : 99.9444 %, train loss : 0.01036577, test acc : 86.8750 %, \n",
      "epoch : 28, train acc : 99.8333 %, train loss : 0.01082651, test acc : 86.8750 %, \n",
      "epoch : 29, train acc : 99.6667 %, train loss : 0.02148999, test acc : 87.5000 %, \n",
      "epoch : 30, train acc : 99.5000 %, train loss : 0.02374626, test acc : 85.6250 %, \n",
      "epoch : 31, train acc : 97.8889 %, train loss : 0.06149044, test acc : 86.2500 %, \n",
      "epoch : 32, train acc : 97.5000 %, train loss : 0.09047739, test acc : 84.3750 %, \n",
      "epoch : 33, train acc : 98.8889 %, train loss : 0.03743446, test acc : 88.1250 %, \n",
      "epoch : 34, train acc : 99.5000 %, train loss : 0.02001382, test acc : 83.7500 %, \n",
      "epoch : 35, train acc : 99.3333 %, train loss : 0.02789711, test acc : 86.8750 %, \n",
      "epoch : 36, train acc : 99.5556 %, train loss : 0.02059800, test acc : 87.5000 %, \n",
      "epoch : 37, train acc : 99.6667 %, train loss : 0.01704755, test acc : 88.7500 %, \n",
      "epoch : 38, train acc : 99.6667 %, train loss : 0.01891301, test acc : 86.2500 %, \n",
      "epoch : 39, train acc : 98.9444 %, train loss : 0.04040888, test acc : 80.0000 %, \n",
      "epoch : 40, train acc : 98.0556 %, train loss : 0.07093676, test acc : 87.5000 %, \n",
      "epoch : 41, train acc : 98.9444 %, train loss : 0.04002433, test acc : 80.0000 %, \n",
      "epoch : 42, train acc : 98.9444 %, train loss : 0.04131640, test acc : 84.3750 %, \n",
      "epoch : 43, train acc : 99.5000 %, train loss : 0.02267023, test acc : 85.6250 %, \n",
      "epoch : 44, train acc : 99.9444 %, train loss : 0.00809340, test acc : 88.1250 %, \n",
      "epoch : 45, train acc : 99.8889 %, train loss : 0.00704637, test acc : 86.8750 %, \n",
      "epoch : 46, train acc : 99.9444 %, train loss : 0.00534403, test acc : 86.2500 %, \n",
      "epoch : 47, train acc : 99.8889 %, train loss : 0.00644540, test acc : 86.8750 %, \n",
      "epoch : 48, train acc : 99.8889 %, train loss : 0.01141520, test acc : 86.2500 %, \n",
      "epoch : 49, train acc : 100.0000 %, train loss : 0.00334112, test acc : 85.6250 %, \n",
      "epoch : 50, train acc : 100.0000 %, train loss : 0.00284811, test acc : 85.6250 %, \n",
      "epoch : 51, train acc : 100.0000 %, train loss : 0.00294420, test acc : 86.2500 %, \n",
      "epoch : 52, train acc : 100.0000 %, train loss : 0.00282319, test acc : 87.5000 %, \n",
      "epoch : 53, train acc : 100.0000 %, train loss : 0.00241035, test acc : 87.5000 %, \n",
      "epoch : 54, train acc : 99.9444 %, train loss : 0.00359280, test acc : 88.1250 %, \n",
      "epoch : 55, train acc : 94.7222 %, train loss : 0.13657023, test acc : 80.0000 %, \n",
      "epoch : 56, train acc : 97.5000 %, train loss : 0.07320163, test acc : 86.8750 %, \n",
      "epoch : 57, train acc : 98.4444 %, train loss : 0.06259979, test acc : 85.6250 %, \n",
      "epoch : 58, train acc : 99.5556 %, train loss : 0.02196776, test acc : 87.5000 %, \n",
      "epoch : 59, train acc : 99.7222 %, train loss : 0.01400180, test acc : 86.2500 %, \n",
      "epoch : 60, train acc : 99.8333 %, train loss : 0.00965365, test acc : 87.5000 %, \n",
      "epoch : 61, train acc : 99.9444 %, train loss : 0.00608348, test acc : 86.8750 %, \n",
      "epoch : 62, train acc : 100.0000 %, train loss : 0.00469147, test acc : 87.5000 %, \n",
      "epoch : 63, train acc : 100.0000 %, train loss : 0.00362526, test acc : 88.7500 %, \n",
      "epoch : 64, train acc : 100.0000 %, train loss : 0.00303680, test acc : 88.7500 %, \n",
      "epoch : 65, train acc : 100.0000 %, train loss : 0.00167182, test acc : 88.7500 %, \n",
      "epoch : 66, train acc : 100.0000 %, train loss : 0.00302882, test acc : 88.1250 %, \n",
      "epoch : 67, train acc : 100.0000 %, train loss : 0.00281891, test acc : 88.7500 %, \n",
      "epoch : 68, train acc : 100.0000 %, train loss : 0.00151333, test acc : 88.1250 %, \n",
      "epoch : 69, train acc : 100.0000 %, train loss : 0.00203688, test acc : 88.1250 %, \n",
      "epoch : 70, train acc : 100.0000 %, train loss : 0.00176846, test acc : 88.1250 %, \n",
      "epoch : 71, train acc : 100.0000 %, train loss : 0.00093196, test acc : 88.1250 %, \n",
      "epoch : 72, train acc : 100.0000 %, train loss : 0.00219064, test acc : 88.1250 %, \n",
      "epoch : 73, train acc : 100.0000 %, train loss : 0.00176365, test acc : 88.1250 %, \n",
      "epoch : 74, train acc : 100.0000 %, train loss : 0.00159030, test acc : 88.1250 %, \n",
      "epoch : 75, train acc : 100.0000 %, train loss : 0.00170558, test acc : 88.1250 %, \n",
      "epoch : 76, train acc : 100.0000 %, train loss : 0.00204143, test acc : 88.1250 %, \n",
      "epoch : 77, train acc : 100.0000 %, train loss : 0.00083368, test acc : 88.1250 %, \n",
      "epoch : 78, train acc : 95.8333 %, train loss : 0.13882344, test acc : 77.5000 %, \n",
      "epoch : 79, train acc : 95.5556 %, train loss : 0.15183946, test acc : 77.5000 %, \n",
      "epoch : 80, train acc : 97.0000 %, train loss : 0.08740885, test acc : 84.3750 %, \n",
      "epoch : 81, train acc : 98.5556 %, train loss : 0.04989231, test acc : 86.8750 %, \n",
      "epoch : 82, train acc : 99.5000 %, train loss : 0.01927826, test acc : 82.5000 %, \n",
      "epoch : 83, train acc : 99.8333 %, train loss : 0.01021783, test acc : 83.7500 %, \n",
      "epoch : 84, train acc : 99.9444 %, train loss : 0.00860720, test acc : 86.8750 %, \n",
      "epoch : 85, train acc : 100.0000 %, train loss : 0.00488091, test acc : 86.8750 %, \n",
      "epoch : 86, train acc : 100.0000 %, train loss : 0.00328238, test acc : 86.2500 %, \n",
      "epoch : 87, train acc : 100.0000 %, train loss : 0.00288029, test acc : 86.8750 %, \n",
      "epoch : 88, train acc : 100.0000 %, train loss : 0.00222362, test acc : 86.8750 %, \n",
      "epoch : 89, train acc : 100.0000 %, train loss : 0.00212604, test acc : 86.8750 %, \n",
      "epoch : 90, train acc : 100.0000 %, train loss : 0.00156567, test acc : 87.5000 %, \n",
      "epoch : 91, train acc : 100.0000 %, train loss : 0.00205704, test acc : 85.6250 %, \n",
      "epoch : 92, train acc : 100.0000 %, train loss : 0.00157957, test acc : 85.6250 %, \n",
      "epoch : 93, train acc : 99.9444 %, train loss : 0.00353606, test acc : 85.6250 %, \n",
      "epoch : 94, train acc : 100.0000 %, train loss : 0.00244433, test acc : 86.2500 %, \n",
      "epoch : 95, train acc : 100.0000 %, train loss : 0.00185601, test acc : 85.0000 %, \n",
      "epoch : 96, train acc : 100.0000 %, train loss : 0.00178630, test acc : 86.8750 %, \n",
      "epoch : 97, train acc : 97.1667 %, train loss : 0.09146517, test acc : 82.5000 %, \n",
      "epoch : 98, train acc : 96.1111 %, train loss : 0.11791947, test acc : 79.3750 %, \n",
      "epoch : 99, train acc : 97.7222 %, train loss : 0.07930691, test acc : 81.8750 %, \n",
      "epoch : 100, train acc : 98.5556 %, train loss : 0.04499011, test acc : 85.0000 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 70.1111 %, train loss : 0.53209739, test acc : 50.6250 %, \n",
      "epoch : 2, train acc : 80.3889 %, train loss : 0.44246887, test acc : 54.3750 %, \n",
      "epoch : 3, train acc : 88.2222 %, train loss : 0.34874531, test acc : 66.2500 %, \n",
      "epoch : 4, train acc : 90.7222 %, train loss : 0.26223102, test acc : 71.8750 %, \n",
      "epoch : 5, train acc : 92.5000 %, train loss : 0.24014768, test acc : 73.1250 %, \n",
      "epoch : 6, train acc : 93.8333 %, train loss : 0.20873552, test acc : 76.2500 %, \n",
      "epoch : 7, train acc : 94.8889 %, train loss : 0.17530164, test acc : 88.7500 %, \n",
      "epoch : 8, train acc : 96.4444 %, train loss : 0.13893398, test acc : 86.8750 %, \n",
      "epoch : 9, train acc : 97.3333 %, train loss : 0.09561127, test acc : 88.7500 %, \n",
      "epoch : 10, train acc : 96.2222 %, train loss : 0.11906900, test acc : 90.6250 %, \n",
      "epoch : 11, train acc : 97.7778 %, train loss : 0.08326056, test acc : 87.5000 %, \n",
      "epoch : 12, train acc : 98.8333 %, train loss : 0.05849297, test acc : 86.2500 %, \n",
      "epoch : 13, train acc : 98.1111 %, train loss : 0.07802434, test acc : 88.7500 %, \n",
      "epoch : 14, train acc : 98.2222 %, train loss : 0.07532299, test acc : 87.5000 %, \n",
      "epoch : 15, train acc : 98.7222 %, train loss : 0.06320181, test acc : 86.2500 %, \n",
      "epoch : 16, train acc : 98.7778 %, train loss : 0.05607882, test acc : 87.5000 %, \n",
      "epoch : 17, train acc : 99.2222 %, train loss : 0.03392024, test acc : 90.0000 %, \n",
      "epoch : 18, train acc : 99.6111 %, train loss : 0.02191146, test acc : 88.1250 %, \n",
      "epoch : 19, train acc : 99.2222 %, train loss : 0.03069151, test acc : 90.0000 %, \n",
      "epoch : 20, train acc : 98.1667 %, train loss : 0.06844039, test acc : 88.7500 %, \n",
      "epoch : 21, train acc : 97.2222 %, train loss : 0.09378465, test acc : 83.7500 %, \n",
      "epoch : 22, train acc : 97.8889 %, train loss : 0.07222988, test acc : 88.1250 %, \n",
      "epoch : 23, train acc : 98.5000 %, train loss : 0.06484526, test acc : 87.5000 %, \n",
      "epoch : 24, train acc : 99.3889 %, train loss : 0.03107345, test acc : 91.8750 %, \n",
      "epoch : 25, train acc : 99.8889 %, train loss : 0.01448374, test acc : 88.7500 %, \n",
      "epoch : 26, train acc : 98.9444 %, train loss : 0.04223958, test acc : 89.3750 %, \n",
      "epoch : 27, train acc : 99.6111 %, train loss : 0.02270916, test acc : 90.0000 %, \n",
      "epoch : 28, train acc : 99.0000 %, train loss : 0.03717048, test acc : 91.2500 %, \n",
      "epoch : 29, train acc : 97.7778 %, train loss : 0.07420298, test acc : 85.6250 %, \n",
      "epoch : 30, train acc : 97.7778 %, train loss : 0.07962844, test acc : 85.0000 %, \n",
      "epoch : 31, train acc : 98.7222 %, train loss : 0.05374045, test acc : 89.3750 %, \n",
      "epoch : 32, train acc : 99.5000 %, train loss : 0.02412885, test acc : 89.3750 %, \n",
      "epoch : 33, train acc : 99.1111 %, train loss : 0.03857675, test acc : 88.1250 %, \n",
      "epoch : 34, train acc : 99.7778 %, train loss : 0.01838783, test acc : 90.0000 %, \n",
      "epoch : 35, train acc : 99.5000 %, train loss : 0.02724860, test acc : 90.0000 %, \n",
      "epoch : 36, train acc : 99.3333 %, train loss : 0.03065520, test acc : 88.7500 %, \n",
      "epoch : 37, train acc : 99.2222 %, train loss : 0.03650298, test acc : 90.6250 %, \n",
      "epoch : 38, train acc : 98.6111 %, train loss : 0.04795883, test acc : 87.5000 %, \n",
      "epoch : 39, train acc : 99.0000 %, train loss : 0.03624607, test acc : 90.0000 %, \n",
      "epoch : 40, train acc : 98.7778 %, train loss : 0.05389621, test acc : 88.1250 %, \n",
      "epoch : 41, train acc : 99.7778 %, train loss : 0.01394822, test acc : 89.3750 %, \n",
      "epoch : 42, train acc : 99.8889 %, train loss : 0.00895661, test acc : 90.0000 %, \n",
      "epoch : 43, train acc : 99.9444 %, train loss : 0.00627137, test acc : 90.0000 %, \n",
      "epoch : 44, train acc : 100.0000 %, train loss : 0.00616606, test acc : 90.6250 %, \n",
      "epoch : 45, train acc : 100.0000 %, train loss : 0.00447544, test acc : 90.0000 %, \n",
      "epoch : 46, train acc : 100.0000 %, train loss : 0.00290800, test acc : 90.0000 %, \n",
      "epoch : 47, train acc : 100.0000 %, train loss : 0.00346281, test acc : 90.0000 %, \n",
      "epoch : 48, train acc : 100.0000 %, train loss : 0.00343089, test acc : 90.0000 %, \n",
      "epoch : 49, train acc : 100.0000 %, train loss : 0.00251437, test acc : 90.6250 %, \n",
      "epoch : 50, train acc : 100.0000 %, train loss : 0.00225673, test acc : 90.6250 %, \n",
      "epoch : 51, train acc : 100.0000 %, train loss : 0.00189333, test acc : 90.6250 %, \n",
      "epoch : 52, train acc : 99.9444 %, train loss : 0.00583780, test acc : 90.0000 %, \n",
      "epoch : 53, train acc : 99.9444 %, train loss : 0.00527995, test acc : 89.3750 %, \n",
      "epoch : 54, train acc : 99.3333 %, train loss : 0.02358849, test acc : 83.1250 %, \n",
      "epoch : 55, train acc : 93.9444 %, train loss : 0.18801762, test acc : 90.6250 %, \n",
      "epoch : 56, train acc : 97.1111 %, train loss : 0.09424937, test acc : 86.2500 %, \n",
      "epoch : 57, train acc : 98.7778 %, train loss : 0.03866703, test acc : 90.6250 %, \n",
      "epoch : 58, train acc : 99.1667 %, train loss : 0.02777210, test acc : 88.7500 %, \n",
      "epoch : 59, train acc : 100.0000 %, train loss : 0.00812026, test acc : 91.2500 %, \n",
      "epoch : 60, train acc : 99.8333 %, train loss : 0.00902096, test acc : 90.6250 %, \n",
      "epoch : 61, train acc : 100.0000 %, train loss : 0.00631980, test acc : 90.6250 %, \n",
      "epoch : 62, train acc : 100.0000 %, train loss : 0.00341827, test acc : 90.6250 %, \n",
      "epoch : 63, train acc : 100.0000 %, train loss : 0.00396541, test acc : 91.2500 %, \n",
      "epoch : 64, train acc : 99.2778 %, train loss : 0.02970975, test acc : 86.2500 %, \n",
      "epoch : 65, train acc : 98.0000 %, train loss : 0.06961757, test acc : 85.6250 %, \n",
      "epoch : 66, train acc : 96.8889 %, train loss : 0.11859728, test acc : 87.5000 %, \n",
      "epoch : 67, train acc : 97.3889 %, train loss : 0.08236784, test acc : 88.1250 %, \n",
      "epoch : 68, train acc : 98.7778 %, train loss : 0.04236148, test acc : 90.0000 %, \n",
      "epoch : 69, train acc : 99.6111 %, train loss : 0.02030925, test acc : 88.7500 %, \n",
      "epoch : 70, train acc : 99.8333 %, train loss : 0.01315197, test acc : 89.3750 %, \n",
      "epoch : 71, train acc : 99.7778 %, train loss : 0.01470279, test acc : 90.0000 %, \n",
      "epoch : 72, train acc : 99.8889 %, train loss : 0.00641246, test acc : 90.0000 %, \n",
      "epoch : 73, train acc : 99.6667 %, train loss : 0.01268461, test acc : 90.0000 %, \n",
      "epoch : 74, train acc : 99.9444 %, train loss : 0.00621011, test acc : 89.3750 %, \n",
      "epoch : 75, train acc : 99.9444 %, train loss : 0.00628785, test acc : 88.7500 %, \n",
      "epoch : 76, train acc : 100.0000 %, train loss : 0.00375165, test acc : 87.5000 %, \n",
      "epoch : 77, train acc : 99.9444 %, train loss : 0.00436838, test acc : 88.7500 %, \n",
      "epoch : 78, train acc : 99.8889 %, train loss : 0.00404199, test acc : 90.0000 %, \n",
      "epoch : 79, train acc : 100.0000 %, train loss : 0.00319225, test acc : 90.6250 %, \n",
      "epoch : 80, train acc : 99.9444 %, train loss : 0.00335169, test acc : 89.3750 %, \n",
      "epoch : 81, train acc : 100.0000 %, train loss : 0.00294090, test acc : 90.0000 %, \n",
      "epoch : 82, train acc : 100.0000 %, train loss : 0.00151838, test acc : 91.8750 %, \n",
      "epoch : 83, train acc : 100.0000 %, train loss : 0.00246582, test acc : 89.3750 %, \n",
      "epoch : 84, train acc : 100.0000 %, train loss : 0.00280515, test acc : 89.3750 %, \n",
      "epoch : 85, train acc : 99.9444 %, train loss : 0.00599835, test acc : 83.1250 %, \n",
      "epoch : 86, train acc : 95.1111 %, train loss : 0.14915403, test acc : 78.1250 %, \n",
      "epoch : 87, train acc : 97.2222 %, train loss : 0.08697290, test acc : 90.6250 %, \n",
      "epoch : 88, train acc : 98.6111 %, train loss : 0.04560578, test acc : 88.7500 %, \n",
      "epoch : 89, train acc : 99.6111 %, train loss : 0.02440954, test acc : 93.1250 %, \n",
      "epoch : 90, train acc : 99.2778 %, train loss : 0.02365960, test acc : 92.5000 %, \n",
      "epoch : 91, train acc : 98.8333 %, train loss : 0.03638201, test acc : 91.2500 %, \n",
      "epoch : 92, train acc : 99.6667 %, train loss : 0.02050872, test acc : 88.7500 %, \n",
      "epoch : 93, train acc : 99.6667 %, train loss : 0.01282693, test acc : 90.6250 %, \n",
      "epoch : 94, train acc : 99.8889 %, train loss : 0.00495941, test acc : 91.8750 %, \n",
      "epoch : 95, train acc : 99.9444 %, train loss : 0.00596411, test acc : 90.6250 %, \n",
      "epoch : 96, train acc : 100.0000 %, train loss : 0.00375951, test acc : 88.7500 %, \n",
      "epoch : 97, train acc : 100.0000 %, train loss : 0.00285085, test acc : 90.0000 %, \n",
      "epoch : 98, train acc : 100.0000 %, train loss : 0.00283431, test acc : 90.6250 %, \n",
      "epoch : 99, train acc : 100.0000 %, train loss : 0.00315458, test acc : 90.6250 %, \n",
      "epoch : 100, train acc : 100.0000 %, train loss : 0.00188773, test acc : 90.6250 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 71.7222 %, train loss : 0.52668013, test acc : 40.0000 %, \n",
      "epoch : 2, train acc : 82.3889 %, train loss : 0.41876531, test acc : 63.1250 %, \n",
      "epoch : 3, train acc : 88.5556 %, train loss : 0.29384049, test acc : 67.5000 %, \n",
      "epoch : 4, train acc : 92.2778 %, train loss : 0.24168224, test acc : 70.6250 %, \n",
      "epoch : 5, train acc : 93.0556 %, train loss : 0.23116526, test acc : 74.3750 %, \n",
      "epoch : 6, train acc : 95.1111 %, train loss : 0.17740053, test acc : 77.5000 %, \n",
      "epoch : 7, train acc : 95.9444 %, train loss : 0.13619748, test acc : 75.0000 %, \n",
      "epoch : 8, train acc : 97.0556 %, train loss : 0.10840291, test acc : 78.7500 %, \n",
      "epoch : 9, train acc : 97.7778 %, train loss : 0.08355489, test acc : 85.0000 %, \n",
      "epoch : 10, train acc : 96.5556 %, train loss : 0.10817661, test acc : 86.2500 %, \n",
      "epoch : 11, train acc : 96.9444 %, train loss : 0.10215413, test acc : 76.8750 %, \n",
      "epoch : 12, train acc : 96.6667 %, train loss : 0.12479681, test acc : 82.5000 %, \n",
      "epoch : 13, train acc : 97.7222 %, train loss : 0.09135467, test acc : 86.2500 %, \n",
      "epoch : 14, train acc : 98.7778 %, train loss : 0.05494581, test acc : 83.7500 %, \n",
      "epoch : 15, train acc : 98.6111 %, train loss : 0.05466431, test acc : 82.5000 %, \n",
      "epoch : 16, train acc : 97.9444 %, train loss : 0.07265804, test acc : 85.0000 %, \n",
      "epoch : 17, train acc : 97.2778 %, train loss : 0.09133460, test acc : 84.3750 %, \n",
      "epoch : 18, train acc : 98.4444 %, train loss : 0.07243825, test acc : 87.5000 %, \n",
      "epoch : 19, train acc : 99.0000 %, train loss : 0.04724788, test acc : 88.1250 %, \n",
      "epoch : 20, train acc : 99.7222 %, train loss : 0.01993099, test acc : 86.8750 %, \n",
      "epoch : 21, train acc : 99.9444 %, train loss : 0.01244245, test acc : 88.7500 %, \n",
      "epoch : 22, train acc : 99.5000 %, train loss : 0.02463941, test acc : 88.7500 %, \n",
      "epoch : 23, train acc : 99.5000 %, train loss : 0.02792909, test acc : 88.1250 %, \n",
      "epoch : 24, train acc : 97.7222 %, train loss : 0.08952793, test acc : 78.7500 %, \n",
      "epoch : 25, train acc : 96.4444 %, train loss : 0.11539320, test acc : 84.3750 %, \n",
      "epoch : 26, train acc : 98.9444 %, train loss : 0.04643394, test acc : 86.8750 %, \n",
      "epoch : 27, train acc : 99.4444 %, train loss : 0.02604377, test acc : 83.1250 %, \n",
      "epoch : 28, train acc : 99.5556 %, train loss : 0.02588206, test acc : 87.5000 %, \n",
      "epoch : 29, train acc : 99.7778 %, train loss : 0.01952235, test acc : 89.3750 %, \n",
      "epoch : 30, train acc : 99.7778 %, train loss : 0.01133091, test acc : 89.3750 %, \n",
      "epoch : 31, train acc : 100.0000 %, train loss : 0.00822727, test acc : 89.3750 %, \n",
      "epoch : 32, train acc : 99.7778 %, train loss : 0.00782811, test acc : 87.5000 %, \n",
      "epoch : 33, train acc : 100.0000 %, train loss : 0.00539256, test acc : 86.8750 %, \n",
      "epoch : 34, train acc : 100.0000 %, train loss : 0.00370080, test acc : 86.8750 %, \n",
      "epoch : 35, train acc : 99.8889 %, train loss : 0.00620748, test acc : 86.8750 %, \n",
      "epoch : 36, train acc : 98.5556 %, train loss : 0.04899379, test acc : 85.6250 %, \n",
      "epoch : 37, train acc : 94.1111 %, train loss : 0.17443593, test acc : 70.6250 %, \n",
      "epoch : 38, train acc : 97.3333 %, train loss : 0.08472814, test acc : 84.3750 %, \n",
      "epoch : 39, train acc : 98.8333 %, train loss : 0.03878767, test acc : 88.1250 %, \n",
      "epoch : 40, train acc : 99.7222 %, train loss : 0.01968663, test acc : 86.8750 %, \n",
      "epoch : 41, train acc : 99.2222 %, train loss : 0.03984137, test acc : 86.2500 %, \n",
      "epoch : 42, train acc : 99.8889 %, train loss : 0.01621644, test acc : 89.3750 %, \n",
      "epoch : 43, train acc : 99.6667 %, train loss : 0.01654941, test acc : 87.5000 %, \n",
      "epoch : 44, train acc : 99.3333 %, train loss : 0.02204353, test acc : 88.1250 %, \n",
      "epoch : 45, train acc : 99.1667 %, train loss : 0.03646654, test acc : 86.2500 %, \n",
      "epoch : 46, train acc : 98.2778 %, train loss : 0.05492948, test acc : 85.6250 %, \n",
      "epoch : 47, train acc : 98.3333 %, train loss : 0.05632638, test acc : 85.0000 %, \n",
      "epoch : 48, train acc : 99.5556 %, train loss : 0.02067508, test acc : 87.5000 %, \n",
      "epoch : 49, train acc : 99.8333 %, train loss : 0.01268571, test acc : 86.8750 %, \n",
      "epoch : 50, train acc : 99.9444 %, train loss : 0.00690188, test acc : 87.5000 %, \n",
      "epoch : 51, train acc : 99.9444 %, train loss : 0.00698242, test acc : 87.5000 %, \n",
      "epoch : 52, train acc : 100.0000 %, train loss : 0.00470525, test acc : 88.1250 %, \n",
      "epoch : 53, train acc : 99.9444 %, train loss : 0.00322644, test acc : 87.5000 %, \n",
      "epoch : 54, train acc : 99.9444 %, train loss : 0.00387210, test acc : 87.5000 %, \n",
      "epoch : 55, train acc : 100.0000 %, train loss : 0.00297079, test acc : 87.5000 %, \n",
      "epoch : 56, train acc : 100.0000 %, train loss : 0.00218411, test acc : 87.5000 %, \n",
      "epoch : 57, train acc : 100.0000 %, train loss : 0.00220839, test acc : 87.5000 %, \n",
      "epoch : 58, train acc : 99.9444 %, train loss : 0.00629768, test acc : 86.8750 %, \n",
      "epoch : 59, train acc : 99.9444 %, train loss : 0.00541465, test acc : 86.8750 %, \n",
      "epoch : 60, train acc : 93.8889 %, train loss : 0.19843935, test acc : 77.5000 %, \n",
      "epoch : 61, train acc : 94.9444 %, train loss : 0.14879023, test acc : 87.5000 %, \n",
      "epoch : 62, train acc : 98.7222 %, train loss : 0.05779901, test acc : 88.1250 %, \n",
      "epoch : 63, train acc : 98.3333 %, train loss : 0.05435177, test acc : 87.5000 %, \n",
      "epoch : 64, train acc : 99.6667 %, train loss : 0.02162405, test acc : 88.1250 %, \n",
      "epoch : 65, train acc : 99.9444 %, train loss : 0.00763205, test acc : 88.1250 %, \n",
      "epoch : 66, train acc : 99.9444 %, train loss : 0.00862112, test acc : 89.3750 %, \n",
      "epoch : 67, train acc : 99.9444 %, train loss : 0.00625804, test acc : 88.7500 %, \n",
      "epoch : 68, train acc : 99.9444 %, train loss : 0.00526254, test acc : 88.1250 %, \n",
      "epoch : 69, train acc : 100.0000 %, train loss : 0.00357691, test acc : 88.7500 %, \n",
      "epoch : 70, train acc : 100.0000 %, train loss : 0.00317416, test acc : 88.1250 %, \n",
      "epoch : 71, train acc : 99.9444 %, train loss : 0.00641794, test acc : 88.7500 %, \n",
      "epoch : 72, train acc : 97.4444 %, train loss : 0.08241812, test acc : 84.3750 %, \n",
      "epoch : 73, train acc : 97.4444 %, train loss : 0.09217381, test acc : 85.6250 %, \n",
      "epoch : 74, train acc : 98.3889 %, train loss : 0.04324347, test acc : 88.1250 %, \n",
      "epoch : 75, train acc : 99.7222 %, train loss : 0.01569956, test acc : 86.2500 %, \n",
      "epoch : 76, train acc : 99.7222 %, train loss : 0.01180101, test acc : 86.2500 %, \n",
      "epoch : 77, train acc : 99.3889 %, train loss : 0.01928432, test acc : 85.0000 %, \n",
      "epoch : 78, train acc : 99.0556 %, train loss : 0.03809192, test acc : 88.7500 %, \n",
      "epoch : 79, train acc : 98.6667 %, train loss : 0.03989367, test acc : 87.5000 %, \n",
      "epoch : 80, train acc : 99.2778 %, train loss : 0.02385937, test acc : 88.1250 %, \n",
      "epoch : 81, train acc : 99.7222 %, train loss : 0.01560272, test acc : 86.2500 %, \n",
      "epoch : 82, train acc : 98.7222 %, train loss : 0.04237437, test acc : 86.8750 %, \n",
      "epoch : 83, train acc : 99.7222 %, train loss : 0.01692475, test acc : 87.5000 %, \n",
      "epoch : 84, train acc : 99.9444 %, train loss : 0.00896536, test acc : 88.1250 %, \n",
      "epoch : 85, train acc : 99.8889 %, train loss : 0.00710194, test acc : 83.7500 %, \n",
      "epoch : 86, train acc : 99.7222 %, train loss : 0.00777243, test acc : 86.8750 %, \n",
      "epoch : 87, train acc : 99.7222 %, train loss : 0.00983811, test acc : 89.3750 %, \n",
      "epoch : 88, train acc : 98.4444 %, train loss : 0.04436996, test acc : 85.0000 %, \n",
      "epoch : 89, train acc : 98.1111 %, train loss : 0.05742908, test acc : 85.6250 %, \n",
      "epoch : 90, train acc : 96.7222 %, train loss : 0.10502348, test acc : 88.7500 %, \n",
      "epoch : 91, train acc : 99.2222 %, train loss : 0.02656828, test acc : 88.7500 %, \n",
      "epoch : 92, train acc : 99.1667 %, train loss : 0.02989155, test acc : 86.8750 %, \n",
      "epoch : 93, train acc : 99.5000 %, train loss : 0.01612739, test acc : 86.2500 %, \n",
      "epoch : 94, train acc : 99.6667 %, train loss : 0.01251736, test acc : 86.8750 %, \n",
      "epoch : 95, train acc : 99.9444 %, train loss : 0.00489090, test acc : 88.1250 %, \n",
      "epoch : 96, train acc : 99.8889 %, train loss : 0.00386640, test acc : 85.6250 %, \n",
      "epoch : 97, train acc : 100.0000 %, train loss : 0.00239269, test acc : 85.0000 %, \n",
      "epoch : 98, train acc : 100.0000 %, train loss : 0.00181122, test acc : 85.6250 %, \n",
      "epoch : 99, train acc : 100.0000 %, train loss : 0.00280251, test acc : 85.0000 %, \n",
      "epoch : 100, train acc : 99.8889 %, train loss : 0.00362019, test acc : 87.5000 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 71.0000 %, train loss : 0.53188415, test acc : 47.5000 %, \n",
      "epoch : 2, train acc : 79.8333 %, train loss : 0.45003291, test acc : 51.2500 %, \n",
      "epoch : 3, train acc : 88.1111 %, train loss : 0.33660938, test acc : 60.0000 %, \n",
      "epoch : 4, train acc : 89.6111 %, train loss : 0.30265286, test acc : 58.7500 %, \n",
      "epoch : 5, train acc : 93.3889 %, train loss : 0.20782840, test acc : 84.3750 %, \n",
      "epoch : 6, train acc : 95.6667 %, train loss : 0.15613621, test acc : 84.3750 %, \n",
      "epoch : 7, train acc : 96.5000 %, train loss : 0.11450835, test acc : 85.6250 %, \n",
      "epoch : 8, train acc : 96.7222 %, train loss : 0.11238114, test acc : 87.5000 %, \n",
      "epoch : 9, train acc : 97.3889 %, train loss : 0.09747099, test acc : 84.3750 %, \n",
      "epoch : 10, train acc : 97.3333 %, train loss : 0.10677947, test acc : 91.2500 %, \n",
      "epoch : 11, train acc : 98.2778 %, train loss : 0.06172674, test acc : 90.0000 %, \n",
      "epoch : 12, train acc : 97.3333 %, train loss : 0.09462166, test acc : 86.8750 %, \n",
      "epoch : 13, train acc : 98.9444 %, train loss : 0.05240776, test acc : 86.8750 %, \n",
      "epoch : 14, train acc : 99.0556 %, train loss : 0.03937697, test acc : 86.8750 %, \n",
      "epoch : 15, train acc : 98.8889 %, train loss : 0.04725264, test acc : 89.3750 %, \n",
      "epoch : 16, train acc : 97.7778 %, train loss : 0.08103588, test acc : 77.5000 %, \n",
      "epoch : 17, train acc : 97.8889 %, train loss : 0.08696154, test acc : 85.6250 %, \n",
      "epoch : 18, train acc : 98.6667 %, train loss : 0.06194959, test acc : 88.1250 %, \n",
      "epoch : 19, train acc : 98.7778 %, train loss : 0.04872009, test acc : 86.8750 %, \n",
      "epoch : 20, train acc : 98.6111 %, train loss : 0.04825809, test acc : 88.1250 %, \n",
      "epoch : 21, train acc : 99.3333 %, train loss : 0.03615680, test acc : 88.1250 %, \n",
      "epoch : 22, train acc : 98.7222 %, train loss : 0.05362704, test acc : 86.2500 %, \n",
      "epoch : 23, train acc : 98.8889 %, train loss : 0.05024114, test acc : 86.2500 %, \n",
      "epoch : 24, train acc : 98.8333 %, train loss : 0.03738641, test acc : 85.0000 %, \n",
      "epoch : 25, train acc : 99.3889 %, train loss : 0.02826674, test acc : 86.8750 %, \n",
      "epoch : 26, train acc : 99.0556 %, train loss : 0.03729773, test acc : 86.2500 %, \n",
      "epoch : 27, train acc : 99.4444 %, train loss : 0.02930928, test acc : 89.3750 %, \n",
      "epoch : 28, train acc : 99.1667 %, train loss : 0.03247234, test acc : 87.5000 %, \n",
      "epoch : 29, train acc : 99.3889 %, train loss : 0.02673042, test acc : 86.8750 %, \n",
      "epoch : 30, train acc : 98.7778 %, train loss : 0.05060055, test acc : 87.5000 %, \n",
      "epoch : 31, train acc : 99.7222 %, train loss : 0.02144196, test acc : 88.7500 %, \n",
      "epoch : 32, train acc : 99.8333 %, train loss : 0.01102932, test acc : 88.7500 %, \n",
      "epoch : 33, train acc : 99.4444 %, train loss : 0.02232571, test acc : 87.5000 %, \n",
      "epoch : 34, train acc : 97.0556 %, train loss : 0.09835976, test acc : 84.3750 %, \n",
      "epoch : 35, train acc : 98.6111 %, train loss : 0.05408373, test acc : 85.6250 %, \n",
      "epoch : 36, train acc : 99.1111 %, train loss : 0.03859179, test acc : 88.1250 %, \n",
      "epoch : 37, train acc : 99.2222 %, train loss : 0.03150957, test acc : 88.7500 %, \n",
      "epoch : 38, train acc : 99.4444 %, train loss : 0.02530304, test acc : 91.2500 %, \n",
      "epoch : 39, train acc : 99.8333 %, train loss : 0.00927843, test acc : 89.3750 %, \n",
      "epoch : 40, train acc : 99.1667 %, train loss : 0.03589232, test acc : 86.2500 %, \n",
      "epoch : 41, train acc : 98.8889 %, train loss : 0.04086641, test acc : 87.5000 %, \n",
      "epoch : 42, train acc : 98.6667 %, train loss : 0.04406945, test acc : 90.0000 %, \n",
      "epoch : 43, train acc : 99.5556 %, train loss : 0.02313077, test acc : 90.0000 %, \n",
      "epoch : 44, train acc : 99.8889 %, train loss : 0.00855513, test acc : 90.0000 %, \n",
      "epoch : 45, train acc : 100.0000 %, train loss : 0.00429166, test acc : 90.0000 %, \n",
      "epoch : 46, train acc : 100.0000 %, train loss : 0.00497773, test acc : 90.0000 %, \n",
      "epoch : 47, train acc : 100.0000 %, train loss : 0.00414160, test acc : 90.0000 %, \n",
      "epoch : 48, train acc : 100.0000 %, train loss : 0.00390348, test acc : 90.6250 %, \n",
      "epoch : 49, train acc : 97.7222 %, train loss : 0.07599304, test acc : 75.6250 %, \n",
      "epoch : 50, train acc : 96.3333 %, train loss : 0.12703140, test acc : 89.3750 %, \n",
      "epoch : 51, train acc : 98.2778 %, train loss : 0.05157711, test acc : 90.6250 %, \n",
      "epoch : 52, train acc : 99.8333 %, train loss : 0.01479768, test acc : 91.8750 %, \n",
      "epoch : 53, train acc : 99.7222 %, train loss : 0.01276117, test acc : 91.8750 %, \n",
      "epoch : 54, train acc : 99.9444 %, train loss : 0.00613102, test acc : 91.2500 %, \n",
      "epoch : 55, train acc : 100.0000 %, train loss : 0.00422084, test acc : 91.2500 %, \n",
      "epoch : 56, train acc : 99.8889 %, train loss : 0.00667443, test acc : 89.3750 %, \n",
      "epoch : 57, train acc : 99.9444 %, train loss : 0.00792628, test acc : 91.2500 %, \n",
      "epoch : 58, train acc : 100.0000 %, train loss : 0.00451040, test acc : 91.8750 %, \n",
      "epoch : 59, train acc : 99.8889 %, train loss : 0.00502043, test acc : 91.2500 %, \n",
      "epoch : 60, train acc : 100.0000 %, train loss : 0.00239163, test acc : 91.2500 %, \n",
      "epoch : 61, train acc : 100.0000 %, train loss : 0.00299217, test acc : 91.8750 %, \n",
      "epoch : 62, train acc : 100.0000 %, train loss : 0.00180723, test acc : 92.5000 %, \n",
      "epoch : 63, train acc : 100.0000 %, train loss : 0.00228147, test acc : 92.5000 %, \n",
      "epoch : 64, train acc : 100.0000 %, train loss : 0.00205330, test acc : 91.8750 %, \n",
      "epoch : 65, train acc : 100.0000 %, train loss : 0.00256776, test acc : 91.8750 %, \n",
      "epoch : 66, train acc : 99.8889 %, train loss : 0.00523021, test acc : 91.2500 %, \n",
      "epoch : 67, train acc : 99.7222 %, train loss : 0.01134250, test acc : 77.5000 %, \n",
      "epoch : 68, train acc : 93.7222 %, train loss : 0.20229208, test acc : 88.7500 %, \n",
      "epoch : 69, train acc : 97.2778 %, train loss : 0.08501462, test acc : 85.0000 %, \n",
      "epoch : 70, train acc : 98.5000 %, train loss : 0.04914685, test acc : 85.6250 %, \n",
      "epoch : 71, train acc : 99.9444 %, train loss : 0.00932826, test acc : 89.3750 %, \n",
      "epoch : 72, train acc : 99.9444 %, train loss : 0.00574228, test acc : 90.0000 %, \n",
      "epoch : 73, train acc : 99.8889 %, train loss : 0.00909064, test acc : 89.3750 %, \n",
      "epoch : 74, train acc : 100.0000 %, train loss : 0.00601098, test acc : 90.0000 %, \n",
      "epoch : 75, train acc : 99.7222 %, train loss : 0.01326980, test acc : 87.5000 %, \n",
      "epoch : 76, train acc : 99.7222 %, train loss : 0.00952778, test acc : 88.1250 %, \n",
      "epoch : 77, train acc : 99.8333 %, train loss : 0.00843288, test acc : 87.5000 %, \n",
      "epoch : 78, train acc : 99.2222 %, train loss : 0.03185120, test acc : 87.5000 %, \n",
      "epoch : 79, train acc : 98.4444 %, train loss : 0.06029491, test acc : 83.7500 %, \n",
      "epoch : 80, train acc : 97.7778 %, train loss : 0.07071815, test acc : 86.8750 %, \n",
      "epoch : 81, train acc : 99.4444 %, train loss : 0.02720855, test acc : 88.1250 %, \n",
      "epoch : 82, train acc : 99.2222 %, train loss : 0.02460993, test acc : 88.1250 %, \n",
      "epoch : 83, train acc : 99.3333 %, train loss : 0.02496567, test acc : 89.3750 %, \n",
      "epoch : 84, train acc : 98.5000 %, train loss : 0.04229810, test acc : 86.8750 %, \n",
      "epoch : 85, train acc : 98.6667 %, train loss : 0.03782833, test acc : 90.0000 %, \n",
      "epoch : 86, train acc : 99.3889 %, train loss : 0.02409459, test acc : 90.6250 %, \n",
      "epoch : 87, train acc : 99.6111 %, train loss : 0.01615809, test acc : 89.3750 %, \n",
      "epoch : 88, train acc : 99.9444 %, train loss : 0.00828301, test acc : 91.2500 %, \n",
      "epoch : 89, train acc : 99.9444 %, train loss : 0.00882533, test acc : 89.3750 %, \n",
      "epoch : 90, train acc : 99.9444 %, train loss : 0.00360447, test acc : 90.6250 %, \n",
      "epoch : 91, train acc : 100.0000 %, train loss : 0.00235223, test acc : 90.0000 %, \n",
      "epoch : 92, train acc : 100.0000 %, train loss : 0.00193203, test acc : 90.0000 %, \n",
      "epoch : 93, train acc : 99.9444 %, train loss : 0.00240805, test acc : 90.0000 %, \n",
      "epoch : 94, train acc : 100.0000 %, train loss : 0.00184248, test acc : 89.3750 %, \n",
      "epoch : 95, train acc : 100.0000 %, train loss : 0.00173913, test acc : 89.3750 %, \n",
      "epoch : 96, train acc : 100.0000 %, train loss : 0.00116660, test acc : 90.0000 %, \n",
      "epoch : 97, train acc : 100.0000 %, train loss : 0.00147493, test acc : 91.2500 %, \n",
      "epoch : 98, train acc : 100.0000 %, train loss : 0.00102539, test acc : 91.2500 %, \n",
      "epoch : 99, train acc : 100.0000 %, train loss : 0.00127511, test acc : 91.2500 %, \n",
      "epoch : 100, train acc : 100.0000 %, train loss : 0.00100847, test acc : 91.2500 %, \n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    split_ratio = 0.3\n",
    "    ref_key = 'numbers'\n",
    "    batch_size = 10            # fix : Not must be equaled with number of test pairs \n",
    "    n_batch = 180\n",
    "    lr = 0.0005\n",
    "    model_type = 'ViTBaseModel'\n",
    "    ViT_params = {}\n",
    "    epochs = 100\n",
    "\n",
    "best_params = best_perform_df.iloc[0].to_dict()\n",
    "best_params['mlp_units'] = re.sub('[\\[\\]]','',best_params['mlp_units'])\n",
    "best_params['mlp_units'] = list(map(int,best_params['mlp_units'].split(',')))       # str to list\n",
    "\n",
    "cfg = Config\n",
    "cfg.ViT_params = best_params\n",
    "\n",
    "times = 10\n",
    "raw_numbers_dict = defaultdict(list)\n",
    "for t in range(times):\n",
    "    _, raw_train_acc, raw_train_loss, raw_test_acc = utils.experiment(cfg, eog_raw_numbers, reference_data)\n",
    "    raw_numbers_dict[t] = [raw_train_acc, raw_train_loss, raw_test_acc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/home/donghyun/eye_writing_classification/experiments/save/'\n",
    "\n",
    "with open(save_path + 'experiment2_raw_numbers_results.json', 'w') as f:\n",
    "    json.dump(dict(raw_numbers_dict),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/home/donghyun/eye_writing_classification/experiments/save/'\n",
    "\n",
    "with open(save_path+'experiment1_raw_numbers_results.json') as f:\n",
    "    hybrid_raw_numbers_results = json.load(f)\n",
    "\n",
    "with open(save_path+'experiment2_raw_numbers_results.json') as f:\n",
    "    vit_raw_numbers_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_test_acc = []\n",
    "vit_test_acc = []\n",
    "for t in range(10):\n",
    "    key = str(t)\n",
    "    hybrid_test_acc.append(hybrid_raw_numbers_results[key][2])\n",
    "    vit_test_acc.append(vit_raw_numbers_results[key][2])\n",
    "\n",
    "hybrid_avg_results = np.array(hybrid_test_acc).mean(axis=0)\n",
    "vit_avg_results  =np.array(vit_test_acc).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy base on raw numbers with 10 repetitions\n",
      "                             1,     2,    3,      4,      5,      6,     7,     8,     9,     10,       Avg.   Best.   Worst.  Std.\n",
      "hybrid model performance : [86.25, 90.0, 84.375, 88.125, 85.0, 89.375, 90.0, 86.875, 90.625, 89.375], (88.0, 90.625, 84.375, 2.125)\n",
      "ViT model performance    : [90.625, 85.0, 91.875, 86.875, 85.0, 88.125, 85.0, 90.625, 87.5, 91.25], (88.1875, 91.875, 85.0, 2.5988278607864737)\n"
     ]
    }
   ],
   "source": [
    "def analysis(data_list):\n",
    "    return np.mean(data_list), max(data_list), min(data_list), np.std(data_list)\n",
    "\n",
    "hybrid_numbers_test_performance = [t[-1] for t in hybrid_test_acc]\n",
    "vit_numbers_test_performance = [t[-1] for t in vit_test_acc]\n",
    "\n",
    "print('Accuracy base on raw numbers with 10 repetitions')\n",
    "print(' '*29 +'1,     2,    3,      4,      5,      6,     7,     8,     9,     10,       Avg.   Best.   Worst.  Std.')\n",
    "print('hybrid model performance : {}, {}'.format(hybrid_numbers_test_performance, analysis(hybrid_numbers_test_performance)))\n",
    "print('ViT model performance    : {}, {}'.format(vit_numbers_test_performance, analysis(vit_numbers_test_performance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize = (20,8))\n",
    "\n",
    "# test accuracy\n",
    "axes[0].plot(hybrid_avg_results, c = 'b', linestyle = 'solid', linewidth = 3)\n",
    "axes[0].plot(vit_avg_results, c = 'r', linestyle = 'solid', linewidth = 3)\n",
    "\n",
    "axes[0].set_ylim(20,100)\n",
    "\n",
    "axes[0].set_title(\"Evaluation\", fontsize=20)\n",
    "axes[0].set_xlabel('Epoch', fontsize = 20)\n",
    "axes[0].set_ylabel('Accuracy', fontsize = 20)\n",
    "\n",
    "axes[0].legend(['Hybrid base model', 'ViT base model'], fontsize = 15)\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('eog')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db961e0436efeabab578e79efd22f04ed4082e196e7e1c09c24525d7c028a5aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
