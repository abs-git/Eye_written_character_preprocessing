{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3\n",
    "<br>\n",
    "-- Combination of raw numbers and katakana characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 13:58:48.873875: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-05 13:58:49.866908: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-01-05 13:58:49.866968: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-01-05 13:58:49.866973: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import data_handler as dh\n",
    "import utils\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/donghyun/eye_writing_classification/v2_dataset/200_points_dataset/'\n",
    "\n",
    "with open(data_path + 'eog_raw_numbers_200.json') as f:\n",
    "  eog_raw_numbers = json.load(f)\n",
    "\n",
    "with open(data_path + 'eog_katakana_200.json') as f:\n",
    "  eog_katakana = json.load(f)\n",
    "\n",
    "with open(data_path + 'reference_data_200.json') as f:\n",
    "  reference_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'])\n",
      "dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n"
     ]
    }
   ],
   "source": [
    "print(eog_katakana.keys())\n",
    "print(eog_raw_numbers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21'])\n",
      "(54, 200, 2)\n",
      "(60, 200, 2)\n",
      "dict_keys(['all'])\n",
      "dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21'])\n",
      "(1, 200, 2)\n",
      "(1, 200, 2)\n"
     ]
    }
   ],
   "source": [
    "# katakana class를 10~21으로 치환 후 merge\n",
    "\n",
    "katakana_le = {'1':'10', '2':'11', '3':'12', '4':'13', '5':'14',\n",
    "               '6':'15', '7':'16', '8':'17', '9':'18', '10':'19',\n",
    "               '11':'20', '12':'21'}\n",
    "\n",
    "all_data = eog_raw_numbers.copy()\n",
    "for k in eog_katakana.keys():\n",
    "    cnvt_key = katakana_le[k]\n",
    "    all_data[cnvt_key] = eog_katakana[k]\n",
    "\n",
    "all_ref = defaultdict()\n",
    "all_ref['all'] = reference_data['numbers'].copy()\n",
    "for k in reference_data['katakana'].keys():\n",
    "    cnvt_key = katakana_le[k]\n",
    "    all_ref['all'][cnvt_key] = reference_data['katakana'][k]\n",
    "\n",
    "\n",
    "print(all_data.keys())\n",
    "print(np.array(all_data['0']).shape)\n",
    "print(np.array(all_data['11']).shape)\n",
    "print(all_ref.keys())\n",
    "print(all_ref['all'].keys())\n",
    "print(np.array(all_ref['all']['0']).shape)\n",
    "print(np.array(all_ref['all']['11']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/home/donghyun/eye_writing_classification/experiments/save/'\n",
    "\n",
    "best_perform_df = pd.read_csv(save_path + 'experiment2_vit_hyperparams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config class\n",
    "\n",
    "class Config:\n",
    "    split_ratio = 0.3\n",
    "    ref_key = 'all'\n",
    "    batch_size = 22            # fix : must be equaled with number of test pairs \n",
    "    n_batch = 100\n",
    "    lr = 0.0001                 # default learning rate\n",
    "    model_type = 'ViTBaseModel'\n",
    "    ViT_params = {}\n",
    "    epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning with hyperparameters of previous experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "\n",
    "cfg.ViT_params = best_perform_df.iloc[0].to_dict()\n",
    "cfg.ViT_params['mlp_units'] = re.sub('[\\[\\]]','',cfg.ViT_params['mlp_units'])\n",
    "cfg.ViT_params['mlp_units'] = list(map(int,cfg.ViT_params['mlp_units'].split(',')))       # str to list\n",
    "cfg.ViT_params['batch_size'] = cfg.batch_size\n",
    "\n",
    "vit_dict = defaultdict(list)\n",
    "bybrid_dict = defaultdict(list)\n",
    "\n",
    "times = 10\n",
    "for t in range(times):\n",
    "    _, vit_train_acc, vit_train_loss, vit_test_acc = utils.experiment(cfg, all_data, all_ref, zero_shot_cls=None)\n",
    "    vit_dict[t] = [vit_train_acc, vit_train_loss, vit_test_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results save\n",
    "save_path = '/home/donghyun/eye_writing_classification/experiments/save/'\n",
    "\n",
    "with open(save_path + 'experiment3_all_prev_params_results.json', 'w') as f:\n",
    "    json.dump(dict(vit_dict),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching the new hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new hyperparameters list\n",
    "\n",
    "vit_hidden_size = [128,256,512]\n",
    "vit_patch_size = [5,10]\n",
    "vit_heads = [8,16]\n",
    "vit_n_layers = [12,16]\n",
    "vit_mlp_units = [[256,128],\n",
    "                 [128,64]]\n",
    "vit_dropout = [0]\n",
    "vit_mlp_dropout = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for hyperparameters\n",
    "\n",
    "cols = ['hidden_size', 'batch_size', 'patch_size', 'heads', 'n_layers', 'mlp_units', 'dropout', 'mlp_dropout', 'score']\n",
    "best_perform_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "i = 0\n",
    "for hs, ps, heads, n_layers, mlp_units, dropout, mlp_dropout in product(vit_hidden_size,\n",
    "                                                                            vit_patch_size,\n",
    "                                                                            vit_heads,\n",
    "                                                                            vit_n_layers,\n",
    "                                                                            vit_mlp_units,\n",
    "                                                                            vit_dropout,\n",
    "                                                                            vit_mlp_dropout\n",
    "                                                                            ):\n",
    "    i+=1\n",
    "    print('index : ', i)\n",
    "\n",
    "    cfg = Config()\n",
    "\n",
    "    cfg.ViT_params['hidden_size'] = hs\n",
    "    cfg.ViT_params['batch_size'] = cfg.batch_size\n",
    "    cfg.ViT_params['patch_size'] = ps\n",
    "    cfg.ViT_params['heads'] = heads\n",
    "    cfg.ViT_params['n_layers'] = n_layers\n",
    "    cfg.ViT_params['mlp_units'] = mlp_units\n",
    "    cfg.ViT_params['dropout'] = dropout\n",
    "    cfg.ViT_params['mlp_dropout'] = mlp_dropout\n",
    "\n",
    "    _, _, _, test_acc_list = utils.experiment(cfg, all_data, all_ref)\n",
    "    score = np.mean(test_acc_list[-3:])\n",
    "\n",
    "    best_perform_df.loc[i] = [hs, cfg.batch_size, ps, heads, n_layers, str(mlp_units), dropout, mlp_dropout, score]\n",
    "\n",
    "best_perform_df = best_perform_df.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters save\n",
    "\n",
    "save_path = '/home/donghyun/eye_writing_classification/experiments/save/'\n",
    "best_perform_df.to_csv(save_path+'experiment3_new_hyperparams.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>heads</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>mlp_units</th>\n",
       "      <th>dropout</th>\n",
       "      <th>mlp_dropout</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82.890071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>256</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81.117021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81.117021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.762411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79.521277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78.457447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77.570922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77.482270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76.861702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>128</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>[128, 64]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.797872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  hidden_size  batch_size  patch_size  heads  n_layers  \\\n",
       "0           9          128          22          10      8        12   \n",
       "1          24          256          22           5     16        16   \n",
       "2           7          128          22           5     16        16   \n",
       "3           2          128          22           5      8        12   \n",
       "4          13          128          22          10     16        12   \n",
       "5           1          128          22           5      8        12   \n",
       "6           3          128          22           5      8        16   \n",
       "7          10          128          22          10      8        12   \n",
       "8           5          128          22           5     16        12   \n",
       "9          14          128          22          10     16        12   \n",
       "\n",
       "    mlp_units  dropout  mlp_dropout      score  \n",
       "0  [256, 128]        0            0  82.890071  \n",
       "1   [128, 64]        0            0  81.117021  \n",
       "2  [256, 128]        0            0  81.117021  \n",
       "3   [128, 64]        0            0  80.762411  \n",
       "4  [256, 128]        0            0  79.521277  \n",
       "5  [256, 128]        0            0  78.457447  \n",
       "6  [256, 128]        0            0  77.570922  \n",
       "7   [128, 64]        0            0  77.482270  \n",
       "8  [256, 128]        0            0  76.861702  \n",
       "9   [128, 64]        0            0  75.797872  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the hyperparameters\n",
    "\n",
    "save_path = '/home/donghyun/eye_writing_classification/experiments/save/'\n",
    "best_perform_df = pd.read_csv(save_path+'experiment3_new_hyperparams.csv')\n",
    "\n",
    "best_perform_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_733), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 63.2727 %, train loss : 0.61447159, test acc : 22.6064 %, \n",
      "epoch : 2, train acc : 70.0455 %, train loss : 0.54801458, test acc : 25.0000 %, \n",
      "epoch : 3, train acc : 74.7727 %, train loss : 0.52418069, test acc : 35.1064 %, \n",
      "epoch : 4, train acc : 76.9545 %, train loss : 0.50011623, test acc : 26.0638 %, \n",
      "epoch : 5, train acc : 77.4545 %, train loss : 0.48289696, test acc : 37.5000 %, \n",
      "epoch : 6, train acc : 79.6818 %, train loss : 0.47169946, test acc : 32.1809 %, \n",
      "epoch : 7, train acc : 81.7273 %, train loss : 0.43563806, test acc : 43.3511 %, \n",
      "epoch : 8, train acc : 84.0000 %, train loss : 0.40419878, test acc : 43.0851 %, \n",
      "epoch : 9, train acc : 84.3182 %, train loss : 0.40370115, test acc : 41.7553 %, \n",
      "epoch : 10, train acc : 86.1818 %, train loss : 0.36108506, test acc : 45.4787 %, \n",
      "epoch : 11, train acc : 85.3636 %, train loss : 0.38170449, test acc : 47.8723 %, \n",
      "epoch : 12, train acc : 87.3182 %, train loss : 0.33567743, test acc : 49.4681 %, \n",
      "epoch : 13, train acc : 88.0000 %, train loss : 0.31375905, test acc : 63.2979 %, \n",
      "epoch : 14, train acc : 90.5000 %, train loss : 0.25251918, test acc : 52.6596 %, \n",
      "epoch : 15, train acc : 91.4091 %, train loss : 0.25770006, test acc : 60.6383 %, \n",
      "epoch : 16, train acc : 92.7273 %, train loss : 0.21493593, test acc : 59.3085 %, \n",
      "epoch : 17, train acc : 93.3182 %, train loss : 0.19678196, test acc : 58.2447 %, \n",
      "epoch : 18, train acc : 93.2273 %, train loss : 0.19783044, test acc : 65.6915 %, \n",
      "epoch : 19, train acc : 92.2727 %, train loss : 0.22244896, test acc : 59.5745 %, \n",
      "epoch : 20, train acc : 93.3182 %, train loss : 0.19794561, test acc : 66.7553 %, \n",
      "epoch : 21, train acc : 93.8636 %, train loss : 0.19175962, test acc : 61.4362 %, \n",
      "epoch : 22, train acc : 93.7727 %, train loss : 0.18556865, test acc : 66.2234 %, \n",
      "epoch : 23, train acc : 95.0909 %, train loss : 0.14504678, test acc : 65.1596 %, \n",
      "epoch : 24, train acc : 93.5455 %, train loss : 0.18817914, test acc : 58.2447 %, \n",
      "epoch : 25, train acc : 94.0909 %, train loss : 0.16236499, test acc : 63.5638 %, \n",
      "epoch : 26, train acc : 94.0000 %, train loss : 0.17463127, test acc : 65.1596 %, \n",
      "epoch : 27, train acc : 94.9545 %, train loss : 0.15241682, test acc : 68.3511 %, \n",
      "epoch : 28, train acc : 95.6364 %, train loss : 0.13555555, test acc : 66.4894 %, \n",
      "epoch : 29, train acc : 93.8182 %, train loss : 0.16640876, test acc : 67.2872 %, \n",
      "epoch : 30, train acc : 95.9545 %, train loss : 0.13915343, test acc : 73.4043 %, \n",
      "epoch : 31, train acc : 96.0909 %, train loss : 0.12586802, test acc : 73.1383 %, \n",
      "epoch : 32, train acc : 96.5455 %, train loss : 0.12347823, test acc : 71.5426 %, \n",
      "epoch : 33, train acc : 96.7727 %, train loss : 0.10643741, test acc : 72.8723 %, \n",
      "epoch : 34, train acc : 96.9091 %, train loss : 0.10795555, test acc : 74.4681 %, \n",
      "epoch : 35, train acc : 97.0000 %, train loss : 0.09794208, test acc : 72.3404 %, \n",
      "epoch : 36, train acc : 96.5909 %, train loss : 0.10732669, test acc : 74.4681 %, \n",
      "epoch : 37, train acc : 97.1818 %, train loss : 0.09841561, test acc : 76.3298 %, \n",
      "epoch : 38, train acc : 96.5455 %, train loss : 0.11706114, test acc : 77.3936 %, \n",
      "epoch : 39, train acc : 97.0455 %, train loss : 0.09012904, test acc : 78.7234 %, \n",
      "epoch : 40, train acc : 96.7273 %, train loss : 0.10622187, test acc : 73.1383 %, \n",
      "epoch : 41, train acc : 97.0909 %, train loss : 0.09850160, test acc : 73.6702 %, \n",
      "epoch : 42, train acc : 96.4545 %, train loss : 0.11191453, test acc : 72.6064 %, \n",
      "epoch : 43, train acc : 96.2727 %, train loss : 0.10399715, test acc : 71.8085 %, \n",
      "epoch : 44, train acc : 97.2273 %, train loss : 0.08683876, test acc : 76.3298 %, \n",
      "epoch : 45, train acc : 96.9091 %, train loss : 0.10373679, test acc : 78.1915 %, \n",
      "epoch : 46, train acc : 96.7727 %, train loss : 0.10574528, test acc : 73.6702 %, \n",
      "epoch : 47, train acc : 97.3182 %, train loss : 0.10699843, test acc : 78.7234 %, \n",
      "epoch : 48, train acc : 97.1818 %, train loss : 0.08593440, test acc : 72.6064 %, \n",
      "epoch : 49, train acc : 97.1364 %, train loss : 0.08747864, test acc : 78.7234 %, \n",
      "epoch : 50, train acc : 97.5909 %, train loss : 0.08913938, test acc : 76.0638 %, \n",
      "epoch : 51, train acc : 96.9091 %, train loss : 0.09650942, test acc : 75.0000 %, \n",
      "epoch : 52, train acc : 97.8636 %, train loss : 0.06615755, test acc : 73.6702 %, \n",
      "epoch : 53, train acc : 98.0909 %, train loss : 0.06547899, test acc : 78.7234 %, \n",
      "epoch : 54, train acc : 98.5000 %, train loss : 0.05701424, test acc : 79.2553 %, \n",
      "epoch : 55, train acc : 98.6364 %, train loss : 0.05305805, test acc : 76.0638 %, \n",
      "epoch : 56, train acc : 97.6818 %, train loss : 0.07908687, test acc : 77.3936 %, \n",
      "epoch : 57, train acc : 98.0909 %, train loss : 0.07227634, test acc : 78.9894 %, \n",
      "epoch : 58, train acc : 96.7727 %, train loss : 0.09285489, test acc : 76.3298 %, \n",
      "epoch : 59, train acc : 96.8636 %, train loss : 0.09367460, test acc : 68.8830 %, \n",
      "epoch : 60, train acc : 96.4545 %, train loss : 0.09888487, test acc : 75.7979 %, \n",
      "epoch : 61, train acc : 96.1818 %, train loss : 0.11671679, test acc : 75.7979 %, \n",
      "epoch : 62, train acc : 96.4091 %, train loss : 0.10582515, test acc : 70.4787 %, \n",
      "epoch : 63, train acc : 97.6818 %, train loss : 0.07085917, test acc : 77.1277 %, \n",
      "epoch : 64, train acc : 97.5455 %, train loss : 0.07760598, test acc : 77.3936 %, \n",
      "epoch : 65, train acc : 98.4091 %, train loss : 0.05022279, test acc : 78.4574 %, \n",
      "epoch : 66, train acc : 97.7273 %, train loss : 0.06783428, test acc : 76.3298 %, \n",
      "epoch : 67, train acc : 98.8636 %, train loss : 0.04666166, test acc : 76.5957 %, \n",
      "epoch : 68, train acc : 98.5000 %, train loss : 0.05038281, test acc : 76.8617 %, \n",
      "epoch : 69, train acc : 97.8182 %, train loss : 0.06134791, test acc : 78.9894 %, \n",
      "epoch : 70, train acc : 98.5455 %, train loss : 0.05441973, test acc : 75.7979 %, \n",
      "epoch : 71, train acc : 97.9545 %, train loss : 0.06902309, test acc : 78.1915 %, \n",
      "epoch : 72, train acc : 98.6364 %, train loss : 0.04086982, test acc : 76.5957 %, \n",
      "epoch : 73, train acc : 98.8182 %, train loss : 0.04558326, test acc : 76.0638 %, \n",
      "epoch : 74, train acc : 97.5455 %, train loss : 0.06173623, test acc : 77.3936 %, \n",
      "epoch : 75, train acc : 98.5909 %, train loss : 0.05806890, test acc : 78.4574 %, \n",
      "epoch : 76, train acc : 98.4545 %, train loss : 0.05568598, test acc : 77.6596 %, \n",
      "epoch : 77, train acc : 98.4091 %, train loss : 0.05158677, test acc : 78.9894 %, \n",
      "epoch : 78, train acc : 98.2727 %, train loss : 0.05133635, test acc : 80.0532 %, \n",
      "epoch : 79, train acc : 98.6364 %, train loss : 0.04366723, test acc : 79.5213 %, \n",
      "epoch : 80, train acc : 98.8182 %, train loss : 0.03886472, test acc : 78.7234 %, \n",
      "epoch : 81, train acc : 98.5000 %, train loss : 0.05526202, test acc : 76.8617 %, \n",
      "epoch : 82, train acc : 99.0455 %, train loss : 0.03876055, test acc : 78.7234 %, \n",
      "epoch : 83, train acc : 98.7273 %, train loss : 0.04707222, test acc : 76.5957 %, \n",
      "epoch : 84, train acc : 98.0909 %, train loss : 0.05202404, test acc : 73.4043 %, \n",
      "epoch : 85, train acc : 97.0909 %, train loss : 0.08601056, test acc : 73.9362 %, \n",
      "epoch : 86, train acc : 96.1364 %, train loss : 0.10949831, test acc : 70.4787 %, \n",
      "epoch : 87, train acc : 95.1818 %, train loss : 0.14910849, test acc : 75.0000 %, \n",
      "epoch : 88, train acc : 96.1364 %, train loss : 0.11879062, test acc : 73.9362 %, \n",
      "epoch : 89, train acc : 95.9545 %, train loss : 0.12707876, test acc : 76.0638 %, \n",
      "epoch : 90, train acc : 96.8636 %, train loss : 0.09297776, test acc : 73.9362 %, \n",
      "epoch : 91, train acc : 98.0000 %, train loss : 0.05969379, test acc : 77.3936 %, \n",
      "epoch : 92, train acc : 98.5455 %, train loss : 0.05439682, test acc : 79.2553 %, \n",
      "epoch : 93, train acc : 98.6818 %, train loss : 0.05199265, test acc : 77.3936 %, \n",
      "epoch : 94, train acc : 98.0000 %, train loss : 0.07044736, test acc : 77.6596 %, \n",
      "epoch : 95, train acc : 98.4545 %, train loss : 0.06017646, test acc : 78.7234 %, \n",
      "epoch : 96, train acc : 98.1818 %, train loss : 0.05799202, test acc : 78.4574 %, \n",
      "epoch : 97, train acc : 98.8182 %, train loss : 0.04720159, test acc : 76.8617 %, \n",
      "epoch : 98, train acc : 98.9545 %, train loss : 0.03937919, test acc : 78.7234 %, \n",
      "epoch : 99, train acc : 98.9091 %, train loss : 0.04059032, test acc : 76.8617 %, \n",
      "epoch : 100, train acc : 99.0909 %, train loss : 0.03224621, test acc : 79.7872 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_746), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 61.6364 %, train loss : 0.62008116, test acc : 14.6277 %, \n",
      "epoch : 2, train acc : 66.5455 %, train loss : 0.58005871, test acc : 26.0638 %, \n",
      "epoch : 3, train acc : 71.7273 %, train loss : 0.53510313, test acc : 33.2447 %, \n",
      "epoch : 4, train acc : 76.4545 %, train loss : 0.49448180, test acc : 30.3191 %, \n",
      "epoch : 5, train acc : 78.7727 %, train loss : 0.47816527, test acc : 30.3191 %, \n",
      "epoch : 6, train acc : 81.0000 %, train loss : 0.45372998, test acc : 44.6809 %, \n",
      "epoch : 7, train acc : 84.6818 %, train loss : 0.40373317, test acc : 39.8936 %, \n",
      "epoch : 8, train acc : 83.0909 %, train loss : 0.42577830, test acc : 28.7234 %, \n",
      "epoch : 9, train acc : 83.0000 %, train loss : 0.42621714, test acc : 36.7021 %, \n",
      "epoch : 10, train acc : 86.7727 %, train loss : 0.36988508, test acc : 40.4255 %, \n",
      "epoch : 11, train acc : 88.0000 %, train loss : 0.32175072, test acc : 51.8617 %, \n",
      "epoch : 12, train acc : 88.7273 %, train loss : 0.31257422, test acc : 51.0638 %, \n",
      "epoch : 13, train acc : 89.3182 %, train loss : 0.30855118, test acc : 48.9362 %, \n",
      "epoch : 14, train acc : 91.5455 %, train loss : 0.25559961, test acc : 56.6489 %, \n",
      "epoch : 15, train acc : 89.6364 %, train loss : 0.27358399, test acc : 55.8511 %, \n",
      "epoch : 16, train acc : 91.0909 %, train loss : 0.24962689, test acc : 58.7766 %, \n",
      "epoch : 17, train acc : 91.8636 %, train loss : 0.23686907, test acc : 57.9787 %, \n",
      "epoch : 18, train acc : 93.0455 %, train loss : 0.20467591, test acc : 59.5745 %, \n",
      "epoch : 19, train acc : 92.9091 %, train loss : 0.20004818, test acc : 59.5745 %, \n",
      "epoch : 20, train acc : 93.1364 %, train loss : 0.18910027, test acc : 63.8298 %, \n",
      "epoch : 21, train acc : 93.8636 %, train loss : 0.18054175, test acc : 63.8298 %, \n",
      "epoch : 22, train acc : 94.0000 %, train loss : 0.18569524, test acc : 63.2979 %, \n",
      "epoch : 23, train acc : 94.1818 %, train loss : 0.17675678, test acc : 63.0319 %, \n",
      "epoch : 24, train acc : 95.2727 %, train loss : 0.15379800, test acc : 63.2979 %, \n",
      "epoch : 25, train acc : 94.5909 %, train loss : 0.16812315, test acc : 67.2872 %, \n",
      "epoch : 26, train acc : 94.9091 %, train loss : 0.15203349, test acc : 68.3511 %, \n",
      "epoch : 27, train acc : 95.8636 %, train loss : 0.12498880, test acc : 71.2766 %, \n",
      "epoch : 28, train acc : 94.6818 %, train loss : 0.16136288, test acc : 71.8085 %, \n",
      "epoch : 29, train acc : 94.9545 %, train loss : 0.14468963, test acc : 73.6702 %, \n",
      "epoch : 30, train acc : 95.7727 %, train loss : 0.13869597, test acc : 70.7447 %, \n",
      "epoch : 31, train acc : 94.9545 %, train loss : 0.14924288, test acc : 71.0106 %, \n",
      "epoch : 32, train acc : 95.3636 %, train loss : 0.13657136, test acc : 77.9255 %, \n",
      "epoch : 33, train acc : 96.3636 %, train loss : 0.10840138, test acc : 72.8723 %, \n",
      "epoch : 34, train acc : 96.5000 %, train loss : 0.10493429, test acc : 77.1277 %, \n",
      "epoch : 35, train acc : 96.7727 %, train loss : 0.10395189, test acc : 78.7234 %, \n",
      "epoch : 36, train acc : 96.6818 %, train loss : 0.10419921, test acc : 73.6702 %, \n",
      "epoch : 37, train acc : 95.7727 %, train loss : 0.13380305, test acc : 71.5426 %, \n",
      "epoch : 38, train acc : 97.0455 %, train loss : 0.09443417, test acc : 74.2021 %, \n",
      "epoch : 39, train acc : 97.0909 %, train loss : 0.09223607, test acc : 75.7979 %, \n",
      "epoch : 40, train acc : 97.0000 %, train loss : 0.09775104, test acc : 73.1383 %, \n",
      "epoch : 41, train acc : 97.5000 %, train loss : 0.08575154, test acc : 75.0000 %, \n",
      "epoch : 42, train acc : 96.4545 %, train loss : 0.11634812, test acc : 77.1277 %, \n",
      "epoch : 43, train acc : 97.2727 %, train loss : 0.08648783, test acc : 74.4681 %, \n",
      "epoch : 44, train acc : 96.6364 %, train loss : 0.09729066, test acc : 72.0745 %, \n",
      "epoch : 45, train acc : 96.1818 %, train loss : 0.11442807, test acc : 74.7340 %, \n",
      "epoch : 46, train acc : 95.1818 %, train loss : 0.13638144, test acc : 72.6064 %, \n",
      "epoch : 47, train acc : 96.7273 %, train loss : 0.09683405, test acc : 75.0000 %, \n",
      "epoch : 48, train acc : 96.0455 %, train loss : 0.11309519, test acc : 76.0638 %, \n",
      "epoch : 49, train acc : 96.5455 %, train loss : 0.10007490, test acc : 72.8723 %, \n",
      "epoch : 50, train acc : 97.7727 %, train loss : 0.08096187, test acc : 77.6596 %, \n",
      "epoch : 51, train acc : 97.6818 %, train loss : 0.07471147, test acc : 75.5319 %, \n",
      "epoch : 52, train acc : 97.1364 %, train loss : 0.08003601, test acc : 76.5957 %, \n",
      "epoch : 53, train acc : 98.2273 %, train loss : 0.06803542, test acc : 77.6596 %, \n",
      "epoch : 54, train acc : 97.1818 %, train loss : 0.08690040, test acc : 70.2128 %, \n",
      "epoch : 55, train acc : 96.4545 %, train loss : 0.10859513, test acc : 77.1277 %, \n",
      "epoch : 56, train acc : 96.1818 %, train loss : 0.11636059, test acc : 72.0745 %, \n",
      "epoch : 57, train acc : 96.6818 %, train loss : 0.10761159, test acc : 72.8723 %, \n",
      "epoch : 58, train acc : 97.5909 %, train loss : 0.08670249, test acc : 74.2021 %, \n",
      "epoch : 59, train acc : 97.3182 %, train loss : 0.07870459, test acc : 76.5957 %, \n",
      "epoch : 60, train acc : 97.8636 %, train loss : 0.06824636, test acc : 76.8617 %, \n",
      "epoch : 61, train acc : 97.2273 %, train loss : 0.08974416, test acc : 73.4043 %, \n",
      "epoch : 62, train acc : 97.5909 %, train loss : 0.08016704, test acc : 72.8723 %, \n",
      "epoch : 63, train acc : 97.5455 %, train loss : 0.07161153, test acc : 75.0000 %, \n",
      "epoch : 64, train acc : 97.5000 %, train loss : 0.06856422, test acc : 74.2021 %, \n",
      "epoch : 65, train acc : 97.9091 %, train loss : 0.06949795, test acc : 77.9255 %, \n",
      "epoch : 66, train acc : 98.5455 %, train loss : 0.05506581, test acc : 80.0532 %, \n",
      "epoch : 67, train acc : 98.0000 %, train loss : 0.06444806, test acc : 78.9894 %, \n",
      "epoch : 68, train acc : 98.1364 %, train loss : 0.05611191, test acc : 75.7979 %, \n",
      "epoch : 69, train acc : 98.1818 %, train loss : 0.06024411, test acc : 73.9362 %, \n",
      "epoch : 70, train acc : 97.2727 %, train loss : 0.08104942, test acc : 70.7447 %, \n",
      "epoch : 71, train acc : 96.2273 %, train loss : 0.10964460, test acc : 71.8085 %, \n",
      "epoch : 72, train acc : 96.1364 %, train loss : 0.10815190, test acc : 69.6809 %, \n",
      "epoch : 73, train acc : 95.7273 %, train loss : 0.12275139, test acc : 73.6702 %, \n",
      "epoch : 74, train acc : 97.0909 %, train loss : 0.09905941, test acc : 75.7979 %, \n",
      "epoch : 75, train acc : 97.1364 %, train loss : 0.08314649, test acc : 74.2021 %, \n",
      "epoch : 76, train acc : 98.0909 %, train loss : 0.06096199, test acc : 73.4043 %, \n",
      "epoch : 77, train acc : 98.4091 %, train loss : 0.04943824, test acc : 76.3298 %, \n",
      "epoch : 78, train acc : 98.5000 %, train loss : 0.05334670, test acc : 77.6596 %, \n",
      "epoch : 79, train acc : 98.3636 %, train loss : 0.05155431, test acc : 74.4681 %, \n",
      "epoch : 80, train acc : 98.1364 %, train loss : 0.06370558, test acc : 77.6596 %, \n",
      "epoch : 81, train acc : 98.0909 %, train loss : 0.05882853, test acc : 75.7979 %, \n",
      "epoch : 82, train acc : 97.9091 %, train loss : 0.06360140, test acc : 76.3298 %, \n",
      "epoch : 83, train acc : 97.8182 %, train loss : 0.07611831, test acc : 75.7979 %, \n",
      "epoch : 84, train acc : 97.5455 %, train loss : 0.07074917, test acc : 79.2553 %, \n",
      "epoch : 85, train acc : 98.2273 %, train loss : 0.06176462, test acc : 76.5957 %, \n",
      "epoch : 86, train acc : 98.0455 %, train loss : 0.06074766, test acc : 75.7979 %, \n",
      "epoch : 87, train acc : 98.0000 %, train loss : 0.06247816, test acc : 75.7979 %, \n",
      "epoch : 88, train acc : 98.0909 %, train loss : 0.06265674, test acc : 75.2660 %, \n",
      "epoch : 89, train acc : 97.2727 %, train loss : 0.09183406, test acc : 72.3404 %, \n",
      "epoch : 90, train acc : 97.2727 %, train loss : 0.08125443, test acc : 74.4681 %, \n",
      "epoch : 91, train acc : 98.1364 %, train loss : 0.06676667, test acc : 76.8617 %, \n",
      "epoch : 92, train acc : 97.6818 %, train loss : 0.07141282, test acc : 78.7234 %, \n",
      "epoch : 93, train acc : 98.2273 %, train loss : 0.05990514, test acc : 75.7979 %, \n",
      "epoch : 94, train acc : 97.8636 %, train loss : 0.06118947, test acc : 78.9894 %, \n",
      "epoch : 95, train acc : 98.0000 %, train loss : 0.06028573, test acc : 78.7234 %, \n",
      "epoch : 96, train acc : 98.0455 %, train loss : 0.06214883, test acc : 78.4574 %, \n",
      "epoch : 97, train acc : 98.1818 %, train loss : 0.05534842, test acc : 80.3191 %, \n",
      "epoch : 98, train acc : 98.3636 %, train loss : 0.04838647, test acc : 77.1277 %, \n",
      "epoch : 99, train acc : 98.5455 %, train loss : 0.04715807, test acc : 79.5213 %, \n",
      "epoch : 100, train acc : 98.4545 %, train loss : 0.04790396, test acc : 79.2553 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_759), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 63.8636 %, train loss : 0.59927129, test acc : 19.1489 %, \n",
      "epoch : 2, train acc : 68.9091 %, train loss : 0.58497185, test acc : 20.7447 %, \n",
      "epoch : 3, train acc : 72.3182 %, train loss : 0.53950743, test acc : 36.9681 %, \n",
      "epoch : 4, train acc : 75.1364 %, train loss : 0.50706795, test acc : 35.1064 %, \n",
      "epoch : 5, train acc : 77.5909 %, train loss : 0.50532204, test acc : 35.3723 %, \n",
      "epoch : 6, train acc : 80.1364 %, train loss : 0.46046516, test acc : 34.8404 %, \n",
      "epoch : 7, train acc : 83.5000 %, train loss : 0.41088293, test acc : 43.8830 %, \n",
      "epoch : 8, train acc : 84.4091 %, train loss : 0.41031452, test acc : 46.0106 %, \n",
      "epoch : 9, train acc : 86.1818 %, train loss : 0.36134372, test acc : 41.2234 %, \n",
      "epoch : 10, train acc : 85.6818 %, train loss : 0.37041580, test acc : 34.3085 %, \n",
      "epoch : 11, train acc : 86.0909 %, train loss : 0.35890675, test acc : 35.3723 %, \n",
      "epoch : 12, train acc : 88.3182 %, train loss : 0.32485430, test acc : 45.7447 %, \n",
      "epoch : 13, train acc : 89.4091 %, train loss : 0.29171620, test acc : 59.5745 %, \n",
      "epoch : 14, train acc : 90.5000 %, train loss : 0.26431852, test acc : 52.1277 %, \n",
      "epoch : 15, train acc : 90.4091 %, train loss : 0.26970297, test acc : 51.0638 %, \n",
      "epoch : 16, train acc : 90.2727 %, train loss : 0.27712183, test acc : 53.9894 %, \n",
      "epoch : 17, train acc : 91.1364 %, train loss : 0.24475778, test acc : 62.7660 %, \n",
      "epoch : 18, train acc : 93.5455 %, train loss : 0.20868624, test acc : 61.7021 %, \n",
      "epoch : 19, train acc : 92.9545 %, train loss : 0.20026679, test acc : 67.5532 %, \n",
      "epoch : 20, train acc : 93.5455 %, train loss : 0.19973131, test acc : 61.9681 %, \n",
      "epoch : 21, train acc : 92.9091 %, train loss : 0.20481255, test acc : 64.8936 %, \n",
      "epoch : 22, train acc : 94.1364 %, train loss : 0.17641745, test acc : 67.2872 %, \n",
      "epoch : 23, train acc : 93.5000 %, train loss : 0.19064412, test acc : 64.6277 %, \n",
      "epoch : 24, train acc : 94.1818 %, train loss : 0.17599343, test acc : 60.6383 %, \n",
      "epoch : 25, train acc : 94.7727 %, train loss : 0.16475398, test acc : 64.6277 %, \n",
      "epoch : 26, train acc : 94.7727 %, train loss : 0.15885851, test acc : 67.2872 %, \n",
      "epoch : 27, train acc : 94.7273 %, train loss : 0.14767874, test acc : 72.8723 %, \n",
      "epoch : 28, train acc : 95.3636 %, train loss : 0.13716848, test acc : 72.6064 %, \n",
      "epoch : 29, train acc : 96.0909 %, train loss : 0.12059141, test acc : 70.2128 %, \n",
      "epoch : 30, train acc : 95.9091 %, train loss : 0.13620700, test acc : 75.0000 %, \n",
      "epoch : 31, train acc : 97.0909 %, train loss : 0.10313535, test acc : 73.1383 %, \n",
      "epoch : 32, train acc : 97.0455 %, train loss : 0.09503456, test acc : 73.9362 %, \n",
      "epoch : 33, train acc : 96.5455 %, train loss : 0.11992526, test acc : 74.2021 %, \n",
      "epoch : 34, train acc : 96.8636 %, train loss : 0.10234971, test acc : 69.6809 %, \n",
      "epoch : 35, train acc : 97.2727 %, train loss : 0.09040340, test acc : 77.6596 %, \n",
      "epoch : 36, train acc : 96.7727 %, train loss : 0.10496256, test acc : 74.2021 %, \n",
      "epoch : 37, train acc : 95.8636 %, train loss : 0.13068394, test acc : 77.9255 %, \n",
      "epoch : 38, train acc : 96.9091 %, train loss : 0.09981102, test acc : 75.7979 %, \n",
      "epoch : 39, train acc : 96.7727 %, train loss : 0.09608287, test acc : 77.9255 %, \n",
      "epoch : 40, train acc : 97.3182 %, train loss : 0.09494689, test acc : 69.6809 %, \n",
      "epoch : 41, train acc : 95.9545 %, train loss : 0.11933147, test acc : 76.3298 %, \n",
      "epoch : 42, train acc : 97.1364 %, train loss : 0.09277457, test acc : 75.5319 %, \n",
      "epoch : 43, train acc : 96.5455 %, train loss : 0.11245132, test acc : 70.7447 %, \n",
      "epoch : 44, train acc : 96.7273 %, train loss : 0.10082410, test acc : 73.6702 %, \n",
      "epoch : 45, train acc : 96.4545 %, train loss : 0.10583672, test acc : 74.2021 %, \n",
      "epoch : 46, train acc : 95.9545 %, train loss : 0.12470277, test acc : 67.8191 %, \n",
      "epoch : 47, train acc : 96.5455 %, train loss : 0.10508165, test acc : 72.8723 %, \n",
      "epoch : 48, train acc : 96.7727 %, train loss : 0.10971159, test acc : 74.7340 %, \n",
      "epoch : 49, train acc : 97.1364 %, train loss : 0.11278734, test acc : 75.5319 %, \n",
      "epoch : 50, train acc : 97.1818 %, train loss : 0.08962338, test acc : 77.1277 %, \n",
      "epoch : 51, train acc : 97.9545 %, train loss : 0.07148036, test acc : 80.3191 %, \n",
      "epoch : 52, train acc : 98.0909 %, train loss : 0.06641759, test acc : 79.5213 %, \n",
      "epoch : 53, train acc : 97.6818 %, train loss : 0.07233931, test acc : 74.7340 %, \n",
      "epoch : 54, train acc : 98.0909 %, train loss : 0.06503014, test acc : 77.3936 %, \n",
      "epoch : 55, train acc : 97.7273 %, train loss : 0.08466940, test acc : 76.8617 %, \n",
      "epoch : 56, train acc : 98.7273 %, train loss : 0.04767940, test acc : 78.7234 %, \n",
      "epoch : 57, train acc : 98.5000 %, train loss : 0.04890924, test acc : 78.7234 %, \n",
      "epoch : 58, train acc : 98.1818 %, train loss : 0.06033712, test acc : 78.1915 %, \n",
      "epoch : 59, train acc : 97.8636 %, train loss : 0.06950926, test acc : 76.8617 %, \n",
      "epoch : 60, train acc : 97.0909 %, train loss : 0.09432227, test acc : 78.9894 %, \n",
      "epoch : 61, train acc : 97.6818 %, train loss : 0.08011165, test acc : 77.3936 %, \n",
      "epoch : 62, train acc : 96.8182 %, train loss : 0.09376209, test acc : 73.6702 %, \n",
      "epoch : 63, train acc : 97.0909 %, train loss : 0.09157388, test acc : 75.5319 %, \n",
      "epoch : 64, train acc : 97.6364 %, train loss : 0.07284522, test acc : 76.0638 %, \n",
      "epoch : 65, train acc : 98.4091 %, train loss : 0.05705890, test acc : 77.6596 %, \n",
      "epoch : 66, train acc : 98.0909 %, train loss : 0.06063886, test acc : 78.9894 %, \n",
      "epoch : 67, train acc : 98.0000 %, train loss : 0.06787306, test acc : 77.9255 %, \n",
      "epoch : 68, train acc : 97.8636 %, train loss : 0.07757874, test acc : 73.4043 %, \n",
      "epoch : 69, train acc : 98.2273 %, train loss : 0.05691099, test acc : 80.8511 %, \n",
      "epoch : 70, train acc : 98.1364 %, train loss : 0.06662252, test acc : 79.7872 %, \n",
      "epoch : 71, train acc : 98.8182 %, train loss : 0.04422334, test acc : 79.5213 %, \n",
      "epoch : 72, train acc : 98.7273 %, train loss : 0.04869591, test acc : 80.5851 %, \n",
      "epoch : 73, train acc : 98.3636 %, train loss : 0.05524021, test acc : 74.4681 %, \n",
      "epoch : 74, train acc : 97.9545 %, train loss : 0.07391932, test acc : 78.1915 %, \n",
      "epoch : 75, train acc : 96.9545 %, train loss : 0.08118535, test acc : 76.5957 %, \n",
      "epoch : 76, train acc : 95.9545 %, train loss : 0.11952368, test acc : 75.7979 %, \n",
      "epoch : 77, train acc : 96.0455 %, train loss : 0.12473579, test acc : 73.4043 %, \n",
      "epoch : 78, train acc : 96.1364 %, train loss : 0.11621024, test acc : 76.8617 %, \n",
      "epoch : 79, train acc : 97.7727 %, train loss : 0.06970884, test acc : 78.1915 %, \n",
      "epoch : 80, train acc : 97.1364 %, train loss : 0.08168873, test acc : 80.5851 %, \n",
      "epoch : 81, train acc : 98.0455 %, train loss : 0.06901157, test acc : 80.0532 %, \n",
      "epoch : 82, train acc : 98.5909 %, train loss : 0.04814486, test acc : 78.9894 %, \n",
      "epoch : 83, train acc : 98.6818 %, train loss : 0.04574864, test acc : 80.5851 %, \n",
      "epoch : 84, train acc : 99.3636 %, train loss : 0.02911907, test acc : 80.8511 %, \n",
      "epoch : 85, train acc : 99.1364 %, train loss : 0.02871327, test acc : 81.1170 %, \n",
      "epoch : 86, train acc : 99.6818 %, train loss : 0.01654805, test acc : 81.6489 %, \n",
      "epoch : 87, train acc : 99.4091 %, train loss : 0.02024153, test acc : 81.9149 %, \n",
      "epoch : 88, train acc : 99.6818 %, train loss : 0.01820743, test acc : 80.0532 %, \n",
      "epoch : 89, train acc : 98.7727 %, train loss : 0.04186213, test acc : 79.7872 %, \n",
      "epoch : 90, train acc : 98.7727 %, train loss : 0.04681749, test acc : 80.0532 %, \n",
      "epoch : 91, train acc : 98.1818 %, train loss : 0.06338368, test acc : 78.9894 %, \n",
      "epoch : 92, train acc : 98.0455 %, train loss : 0.05896814, test acc : 79.7872 %, \n",
      "epoch : 93, train acc : 98.1818 %, train loss : 0.05569322, test acc : 76.3298 %, \n",
      "epoch : 94, train acc : 97.9545 %, train loss : 0.06571420, test acc : 76.8617 %, \n",
      "epoch : 95, train acc : 98.0909 %, train loss : 0.06279642, test acc : 78.7234 %, \n",
      "epoch : 96, train acc : 98.5455 %, train loss : 0.05146290, test acc : 79.5213 %, \n",
      "epoch : 97, train acc : 98.0000 %, train loss : 0.06851176, test acc : 79.2553 %, \n",
      "epoch : 98, train acc : 97.3636 %, train loss : 0.08928035, test acc : 74.7340 %, \n",
      "epoch : 99, train acc : 96.1364 %, train loss : 0.10808531, test acc : 77.3936 %, \n",
      "epoch : 100, train acc : 96.9545 %, train loss : 0.09132719, test acc : 79.7872 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_772), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 62.1364 %, train loss : 0.62313534, test acc : 23.4043 %, \n",
      "epoch : 2, train acc : 70.5909 %, train loss : 0.55916384, test acc : 21.8085 %, \n",
      "epoch : 3, train acc : 72.5000 %, train loss : 0.54512055, test acc : 28.9894 %, \n",
      "epoch : 4, train acc : 72.7727 %, train loss : 0.54243924, test acc : 33.5106 %, \n",
      "epoch : 5, train acc : 78.0909 %, train loss : 0.47817851, test acc : 22.3404 %, \n",
      "epoch : 6, train acc : 76.5000 %, train loss : 0.51133483, test acc : 25.7979 %, \n",
      "epoch : 7, train acc : 80.2273 %, train loss : 0.46580292, test acc : 27.9255 %, \n",
      "epoch : 8, train acc : 78.9091 %, train loss : 0.48508699, test acc : 29.5213 %, \n",
      "epoch : 9, train acc : 82.5000 %, train loss : 0.42606226, test acc : 37.5000 %, \n",
      "epoch : 10, train acc : 84.1818 %, train loss : 0.39899867, test acc : 40.6915 %, \n",
      "epoch : 11, train acc : 85.0909 %, train loss : 0.38960840, test acc : 39.0957 %, \n",
      "epoch : 12, train acc : 86.3636 %, train loss : 0.35834042, test acc : 44.1489 %, \n",
      "epoch : 13, train acc : 85.7727 %, train loss : 0.36238504, test acc : 52.3936 %, \n",
      "epoch : 14, train acc : 88.9545 %, train loss : 0.30438193, test acc : 55.3191 %, \n",
      "epoch : 15, train acc : 87.0455 %, train loss : 0.33276645, test acc : 50.0000 %, \n",
      "epoch : 16, train acc : 89.3182 %, train loss : 0.28797151, test acc : 58.7766 %, \n",
      "epoch : 17, train acc : 90.0909 %, train loss : 0.26111509, test acc : 55.5851 %, \n",
      "epoch : 18, train acc : 91.2273 %, train loss : 0.25118666, test acc : 64.0957 %, \n",
      "epoch : 19, train acc : 91.4545 %, train loss : 0.23978436, test acc : 65.4255 %, \n",
      "epoch : 20, train acc : 94.5000 %, train loss : 0.17726953, test acc : 69.6809 %, \n",
      "epoch : 21, train acc : 94.2273 %, train loss : 0.17521064, test acc : 64.3617 %, \n",
      "epoch : 22, train acc : 93.6818 %, train loss : 0.19224210, test acc : 68.8830 %, \n",
      "epoch : 23, train acc : 93.5000 %, train loss : 0.18096674, test acc : 66.4894 %, \n",
      "epoch : 24, train acc : 94.1818 %, train loss : 0.17594329, test acc : 71.0106 %, \n",
      "epoch : 25, train acc : 95.0000 %, train loss : 0.15788405, test acc : 65.9574 %, \n",
      "epoch : 26, train acc : 94.9091 %, train loss : 0.14923378, test acc : 72.0745 %, \n",
      "epoch : 27, train acc : 95.0909 %, train loss : 0.15060029, test acc : 72.0745 %, \n",
      "epoch : 28, train acc : 94.7273 %, train loss : 0.14650981, test acc : 70.4787 %, \n",
      "epoch : 29, train acc : 94.6364 %, train loss : 0.15249640, test acc : 70.4787 %, \n",
      "epoch : 30, train acc : 94.0000 %, train loss : 0.16984172, test acc : 72.6064 %, \n",
      "epoch : 31, train acc : 95.3182 %, train loss : 0.14494863, test acc : 73.9362 %, \n",
      "epoch : 32, train acc : 95.3636 %, train loss : 0.13398270, test acc : 79.2553 %, \n",
      "epoch : 33, train acc : 96.9091 %, train loss : 0.10273502, test acc : 75.7979 %, \n",
      "epoch : 34, train acc : 96.9545 %, train loss : 0.10544086, test acc : 71.0106 %, \n",
      "epoch : 35, train acc : 95.3182 %, train loss : 0.13592313, test acc : 69.1489 %, \n",
      "epoch : 36, train acc : 95.2273 %, train loss : 0.13724998, test acc : 72.0745 %, \n",
      "epoch : 37, train acc : 97.2273 %, train loss : 0.10304603, test acc : 71.0106 %, \n",
      "epoch : 38, train acc : 95.4091 %, train loss : 0.13994191, test acc : 69.9468 %, \n",
      "epoch : 39, train acc : 95.8636 %, train loss : 0.13255699, test acc : 72.0745 %, \n",
      "epoch : 40, train acc : 97.0909 %, train loss : 0.09607507, test acc : 74.2021 %, \n",
      "epoch : 41, train acc : 96.5000 %, train loss : 0.11335786, test acc : 75.7979 %, \n",
      "epoch : 42, train acc : 96.9545 %, train loss : 0.09607571, test acc : 73.9362 %, \n",
      "epoch : 43, train acc : 96.9545 %, train loss : 0.10085947, test acc : 76.8617 %, \n",
      "epoch : 44, train acc : 97.2727 %, train loss : 0.08119228, test acc : 75.0000 %, \n",
      "epoch : 45, train acc : 97.2727 %, train loss : 0.08349382, test acc : 76.3298 %, \n",
      "epoch : 46, train acc : 97.6818 %, train loss : 0.07471796, test acc : 76.0638 %, \n",
      "epoch : 47, train acc : 97.9091 %, train loss : 0.06920714, test acc : 75.2660 %, \n",
      "epoch : 48, train acc : 97.7273 %, train loss : 0.07180386, test acc : 78.7234 %, \n",
      "epoch : 49, train acc : 97.6818 %, train loss : 0.07692625, test acc : 75.5319 %, \n",
      "epoch : 50, train acc : 96.9091 %, train loss : 0.09512743, test acc : 75.7979 %, \n",
      "epoch : 51, train acc : 96.6364 %, train loss : 0.11717973, test acc : 73.9362 %, \n",
      "epoch : 52, train acc : 97.7273 %, train loss : 0.08855978, test acc : 76.3298 %, \n",
      "epoch : 53, train acc : 96.6818 %, train loss : 0.09662326, test acc : 74.4681 %, \n",
      "epoch : 54, train acc : 96.4545 %, train loss : 0.10983761, test acc : 73.4043 %, \n",
      "epoch : 55, train acc : 96.3636 %, train loss : 0.10721032, test acc : 78.4574 %, \n",
      "epoch : 56, train acc : 97.0909 %, train loss : 0.08587460, test acc : 75.7979 %, \n",
      "epoch : 57, train acc : 96.5455 %, train loss : 0.10588277, test acc : 70.7447 %, \n",
      "epoch : 58, train acc : 96.6818 %, train loss : 0.09625770, test acc : 77.1277 %, \n",
      "epoch : 59, train acc : 97.0000 %, train loss : 0.09180732, test acc : 78.1915 %, \n",
      "epoch : 60, train acc : 97.0455 %, train loss : 0.09799279, test acc : 76.8617 %, \n",
      "epoch : 61, train acc : 97.8182 %, train loss : 0.08451042, test acc : 72.6064 %, \n",
      "epoch : 62, train acc : 97.7273 %, train loss : 0.07364525, test acc : 77.3936 %, \n",
      "epoch : 63, train acc : 98.0909 %, train loss : 0.06591787, test acc : 76.3298 %, \n",
      "epoch : 64, train acc : 97.9545 %, train loss : 0.07053657, test acc : 76.8617 %, \n",
      "epoch : 65, train acc : 97.8182 %, train loss : 0.08115551, test acc : 74.7340 %, \n",
      "epoch : 66, train acc : 97.4091 %, train loss : 0.08134758, test acc : 76.5957 %, \n",
      "epoch : 67, train acc : 97.5000 %, train loss : 0.07633349, test acc : 79.5213 %, \n",
      "epoch : 68, train acc : 98.3636 %, train loss : 0.06074158, test acc : 76.8617 %, \n",
      "epoch : 69, train acc : 97.8636 %, train loss : 0.06468986, test acc : 76.8617 %, \n",
      "epoch : 70, train acc : 98.5000 %, train loss : 0.05153046, test acc : 73.6702 %, \n",
      "epoch : 71, train acc : 97.8182 %, train loss : 0.07046618, test acc : 77.9255 %, \n",
      "epoch : 72, train acc : 97.9545 %, train loss : 0.06528448, test acc : 80.0532 %, \n",
      "epoch : 73, train acc : 97.8636 %, train loss : 0.06917025, test acc : 77.6596 %, \n",
      "epoch : 74, train acc : 98.2727 %, train loss : 0.05592926, test acc : 78.1915 %, \n",
      "epoch : 75, train acc : 98.6364 %, train loss : 0.04621143, test acc : 77.6596 %, \n",
      "epoch : 76, train acc : 97.8182 %, train loss : 0.06508975, test acc : 71.8085 %, \n",
      "epoch : 77, train acc : 97.1818 %, train loss : 0.07919470, test acc : 67.2872 %, \n",
      "epoch : 78, train acc : 97.5000 %, train loss : 0.09128420, test acc : 76.0638 %, \n",
      "epoch : 79, train acc : 97.9091 %, train loss : 0.06933463, test acc : 78.1915 %, \n",
      "epoch : 80, train acc : 97.4091 %, train loss : 0.07359605, test acc : 77.1277 %, \n",
      "epoch : 81, train acc : 98.0455 %, train loss : 0.06577452, test acc : 80.8511 %, \n",
      "epoch : 82, train acc : 98.3182 %, train loss : 0.05575668, test acc : 80.0532 %, \n",
      "epoch : 83, train acc : 98.5000 %, train loss : 0.05054964, test acc : 77.6596 %, \n",
      "epoch : 84, train acc : 99.0909 %, train loss : 0.03652593, test acc : 78.4574 %, \n",
      "epoch : 85, train acc : 98.6364 %, train loss : 0.04733005, test acc : 77.6596 %, \n",
      "epoch : 86, train acc : 98.1818 %, train loss : 0.05856821, test acc : 79.5213 %, \n",
      "epoch : 87, train acc : 97.5909 %, train loss : 0.07091250, test acc : 80.5851 %, \n",
      "epoch : 88, train acc : 98.0000 %, train loss : 0.05501730, test acc : 77.1277 %, \n",
      "epoch : 89, train acc : 97.7727 %, train loss : 0.07526533, test acc : 75.2660 %, \n",
      "epoch : 90, train acc : 97.0000 %, train loss : 0.09049549, test acc : 76.8617 %, \n",
      "epoch : 91, train acc : 97.4091 %, train loss : 0.07572661, test acc : 77.6596 %, \n",
      "epoch : 92, train acc : 98.4545 %, train loss : 0.04604232, test acc : 74.7340 %, \n",
      "epoch : 93, train acc : 98.3182 %, train loss : 0.05514157, test acc : 78.9894 %, \n",
      "epoch : 94, train acc : 97.3636 %, train loss : 0.08442170, test acc : 74.2021 %, \n",
      "epoch : 95, train acc : 97.5000 %, train loss : 0.07288797, test acc : 77.6596 %, \n",
      "epoch : 96, train acc : 97.1818 %, train loss : 0.08002930, test acc : 77.6596 %, \n",
      "epoch : 97, train acc : 97.7273 %, train loss : 0.07448329, test acc : 77.6596 %, \n",
      "epoch : 98, train acc : 98.4091 %, train loss : 0.05767571, test acc : 80.0532 %, \n",
      "epoch : 99, train acc : 98.8636 %, train loss : 0.03968395, test acc : 80.8511 %, \n",
      "epoch : 100, train acc : 98.8636 %, train loss : 0.03713172, test acc : 81.3830 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_785), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 61.6364 %, train loss : 0.61681365, test acc : 13.2979 %, \n",
      "epoch : 2, train acc : 68.1364 %, train loss : 0.56837149, test acc : 36.4362 %, \n",
      "epoch : 3, train acc : 70.4091 %, train loss : 0.55055745, test acc : 19.9468 %, \n",
      "epoch : 4, train acc : 76.4091 %, train loss : 0.50141797, test acc : 29.2553 %, \n",
      "epoch : 5, train acc : 77.8636 %, train loss : 0.50000540, test acc : 22.3404 %, \n",
      "epoch : 6, train acc : 79.5909 %, train loss : 0.48169145, test acc : 35.6383 %, \n",
      "epoch : 7, train acc : 80.7273 %, train loss : 0.46599004, test acc : 35.6383 %, \n",
      "epoch : 8, train acc : 81.6818 %, train loss : 0.45099975, test acc : 33.2447 %, \n",
      "epoch : 9, train acc : 83.0455 %, train loss : 0.41801470, test acc : 43.6170 %, \n",
      "epoch : 10, train acc : 85.8182 %, train loss : 0.37098785, test acc : 42.0213 %, \n",
      "epoch : 11, train acc : 87.5455 %, train loss : 0.34363663, test acc : 47.6064 %, \n",
      "epoch : 12, train acc : 88.9545 %, train loss : 0.31205064, test acc : 42.8191 %, \n",
      "epoch : 13, train acc : 87.0909 %, train loss : 0.33610349, test acc : 48.4043 %, \n",
      "epoch : 14, train acc : 89.7273 %, train loss : 0.27834430, test acc : 50.2660 %, \n",
      "epoch : 15, train acc : 89.1818 %, train loss : 0.30093257, test acc : 59.3085 %, \n",
      "epoch : 16, train acc : 91.9545 %, train loss : 0.24002679, test acc : 50.5319 %, \n",
      "epoch : 17, train acc : 93.0000 %, train loss : 0.21451818, test acc : 65.1596 %, \n",
      "epoch : 18, train acc : 91.2727 %, train loss : 0.24106317, test acc : 61.7021 %, \n",
      "epoch : 19, train acc : 92.9545 %, train loss : 0.21837873, test acc : 61.4362 %, \n",
      "epoch : 20, train acc : 92.3636 %, train loss : 0.22666686, test acc : 66.7553 %, \n",
      "epoch : 21, train acc : 94.1818 %, train loss : 0.18091811, test acc : 65.1596 %, \n",
      "epoch : 22, train acc : 93.5909 %, train loss : 0.18328624, test acc : 68.6170 %, \n",
      "epoch : 23, train acc : 94.7727 %, train loss : 0.17584204, test acc : 73.4043 %, \n",
      "epoch : 24, train acc : 95.8182 %, train loss : 0.13689862, test acc : 73.4043 %, \n",
      "epoch : 25, train acc : 95.0000 %, train loss : 0.14797646, test acc : 75.2660 %, \n",
      "epoch : 26, train acc : 95.7273 %, train loss : 0.13243111, test acc : 72.0745 %, \n",
      "epoch : 27, train acc : 96.2273 %, train loss : 0.12486852, test acc : 73.9362 %, \n",
      "epoch : 28, train acc : 95.0455 %, train loss : 0.16359656, test acc : 69.1489 %, \n",
      "epoch : 29, train acc : 95.5455 %, train loss : 0.13983841, test acc : 75.2660 %, \n",
      "epoch : 30, train acc : 95.3636 %, train loss : 0.14118663, test acc : 73.4043 %, \n",
      "epoch : 31, train acc : 95.4091 %, train loss : 0.13318643, test acc : 73.1383 %, \n",
      "epoch : 32, train acc : 95.5909 %, train loss : 0.13987657, test acc : 75.0000 %, \n",
      "epoch : 33, train acc : 95.2273 %, train loss : 0.15897686, test acc : 73.6702 %, \n",
      "epoch : 34, train acc : 95.2273 %, train loss : 0.13767271, test acc : 77.6596 %, \n",
      "epoch : 35, train acc : 96.3182 %, train loss : 0.11442287, test acc : 77.9255 %, \n",
      "epoch : 36, train acc : 95.9091 %, train loss : 0.12633343, test acc : 72.8723 %, \n",
      "epoch : 37, train acc : 95.4091 %, train loss : 0.13688500, test acc : 73.6702 %, \n",
      "epoch : 38, train acc : 95.7273 %, train loss : 0.12587271, test acc : 73.9362 %, \n",
      "epoch : 39, train acc : 95.2727 %, train loss : 0.15880424, test acc : 74.2021 %, \n",
      "epoch : 40, train acc : 96.1364 %, train loss : 0.12892842, test acc : 74.4681 %, \n",
      "epoch : 41, train acc : 96.8636 %, train loss : 0.10629825, test acc : 75.0000 %, \n",
      "epoch : 42, train acc : 97.6818 %, train loss : 0.07789581, test acc : 78.9894 %, \n",
      "epoch : 43, train acc : 97.5909 %, train loss : 0.08088097, test acc : 78.9894 %, \n",
      "epoch : 44, train acc : 96.2727 %, train loss : 0.10225373, test acc : 79.2553 %, \n",
      "epoch : 45, train acc : 96.5455 %, train loss : 0.10734196, test acc : 74.4681 %, \n",
      "epoch : 46, train acc : 97.1364 %, train loss : 0.09336639, test acc : 76.8617 %, \n",
      "epoch : 47, train acc : 96.5000 %, train loss : 0.10900634, test acc : 78.1915 %, \n",
      "epoch : 48, train acc : 96.2273 %, train loss : 0.10993143, test acc : 69.1489 %, \n",
      "epoch : 49, train acc : 96.7727 %, train loss : 0.09856524, test acc : 74.7340 %, \n",
      "epoch : 50, train acc : 95.8182 %, train loss : 0.11950242, test acc : 74.2021 %, \n",
      "epoch : 51, train acc : 96.6818 %, train loss : 0.10167186, test acc : 76.8617 %, \n",
      "epoch : 52, train acc : 97.7273 %, train loss : 0.08208526, test acc : 80.5851 %, \n",
      "epoch : 53, train acc : 97.7727 %, train loss : 0.07652157, test acc : 77.3936 %, \n",
      "epoch : 54, train acc : 97.5909 %, train loss : 0.08465951, test acc : 80.8511 %, \n",
      "epoch : 55, train acc : 97.8636 %, train loss : 0.07620755, test acc : 80.3191 %, \n",
      "epoch : 56, train acc : 98.3636 %, train loss : 0.05680639, test acc : 80.5851 %, \n",
      "epoch : 57, train acc : 97.9545 %, train loss : 0.06286070, test acc : 75.5319 %, \n",
      "epoch : 58, train acc : 98.8182 %, train loss : 0.04681693, test acc : 80.5851 %, \n",
      "epoch : 59, train acc : 97.9545 %, train loss : 0.06482091, test acc : 77.3936 %, \n",
      "epoch : 60, train acc : 98.2273 %, train loss : 0.05539868, test acc : 76.0638 %, \n",
      "epoch : 61, train acc : 97.1364 %, train loss : 0.09516105, test acc : 78.7234 %, \n",
      "epoch : 62, train acc : 96.4091 %, train loss : 0.11135342, test acc : 72.0745 %, \n",
      "epoch : 63, train acc : 96.4545 %, train loss : 0.12242590, test acc : 75.5319 %, \n",
      "epoch : 64, train acc : 97.1364 %, train loss : 0.10067966, test acc : 76.5957 %, \n",
      "epoch : 65, train acc : 96.9545 %, train loss : 0.09433707, test acc : 77.1277 %, \n",
      "epoch : 66, train acc : 97.7273 %, train loss : 0.07916995, test acc : 81.6489 %, \n",
      "epoch : 67, train acc : 98.1818 %, train loss : 0.05931713, test acc : 81.3830 %, \n",
      "epoch : 68, train acc : 98.6364 %, train loss : 0.04901804, test acc : 75.5319 %, \n",
      "epoch : 69, train acc : 98.0455 %, train loss : 0.06550034, test acc : 80.5851 %, \n",
      "epoch : 70, train acc : 98.3182 %, train loss : 0.05772798, test acc : 76.8617 %, \n",
      "epoch : 71, train acc : 97.9545 %, train loss : 0.06447769, test acc : 78.1915 %, \n",
      "epoch : 72, train acc : 97.9091 %, train loss : 0.06771998, test acc : 78.4574 %, \n",
      "epoch : 73, train acc : 97.9091 %, train loss : 0.06725703, test acc : 83.5106 %, \n",
      "epoch : 74, train acc : 98.1818 %, train loss : 0.05763347, test acc : 80.0532 %, \n",
      "epoch : 75, train acc : 98.1818 %, train loss : 0.05775928, test acc : 82.7128 %, \n",
      "epoch : 76, train acc : 97.0909 %, train loss : 0.08660144, test acc : 76.8617 %, \n",
      "epoch : 77, train acc : 97.8182 %, train loss : 0.06941465, test acc : 81.6489 %, \n",
      "epoch : 78, train acc : 97.2273 %, train loss : 0.08193932, test acc : 81.1170 %, \n",
      "epoch : 79, train acc : 96.0909 %, train loss : 0.11372908, test acc : 78.7234 %, \n",
      "epoch : 80, train acc : 94.6364 %, train loss : 0.15916029, test acc : 71.5426 %, \n",
      "epoch : 81, train acc : 95.2273 %, train loss : 0.14179707, test acc : 80.5851 %, \n",
      "epoch : 82, train acc : 95.8182 %, train loss : 0.12517346, test acc : 77.3936 %, \n",
      "epoch : 83, train acc : 97.2273 %, train loss : 0.08997986, test acc : 83.5106 %, \n",
      "epoch : 84, train acc : 97.9545 %, train loss : 0.06099790, test acc : 81.3830 %, \n",
      "epoch : 85, train acc : 98.4545 %, train loss : 0.05136816, test acc : 80.5851 %, \n",
      "epoch : 86, train acc : 98.6364 %, train loss : 0.04818580, test acc : 82.4468 %, \n",
      "epoch : 87, train acc : 98.9545 %, train loss : 0.03968786, test acc : 82.4468 %, \n",
      "epoch : 88, train acc : 98.8182 %, train loss : 0.04316544, test acc : 78.4574 %, \n",
      "epoch : 89, train acc : 98.0909 %, train loss : 0.06360645, test acc : 77.9255 %, \n",
      "epoch : 90, train acc : 98.6818 %, train loss : 0.04881500, test acc : 79.7872 %, \n",
      "epoch : 91, train acc : 97.7273 %, train loss : 0.06864809, test acc : 78.4574 %, \n",
      "epoch : 92, train acc : 98.1818 %, train loss : 0.05874078, test acc : 81.1170 %, \n",
      "epoch : 93, train acc : 97.8182 %, train loss : 0.07196230, test acc : 80.0532 %, \n",
      "epoch : 94, train acc : 97.9545 %, train loss : 0.06875998, test acc : 79.2553 %, \n",
      "epoch : 95, train acc : 97.8182 %, train loss : 0.07165235, test acc : 81.6489 %, \n",
      "epoch : 96, train acc : 97.0000 %, train loss : 0.08287205, test acc : 77.1277 %, \n",
      "epoch : 97, train acc : 97.3636 %, train loss : 0.08315375, test acc : 80.3191 %, \n",
      "epoch : 98, train acc : 97.9545 %, train loss : 0.06630556, test acc : 80.0532 %, \n",
      "epoch : 99, train acc : 97.5455 %, train loss : 0.06569505, test acc : 79.2553 %, \n",
      "epoch : 100, train acc : 97.0455 %, train loss : 0.09379442, test acc : 78.1915 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_798), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 63.5455 %, train loss : 0.61320045, test acc : 26.3298 %, \n",
      "epoch : 2, train acc : 67.8636 %, train loss : 0.57158137, test acc : 29.5213 %, \n",
      "epoch : 3, train acc : 73.9091 %, train loss : 0.54339515, test acc : 33.7766 %, \n",
      "epoch : 4, train acc : 75.6364 %, train loss : 0.51581701, test acc : 18.3511 %, \n",
      "epoch : 5, train acc : 79.1364 %, train loss : 0.49035396, test acc : 43.8830 %, \n",
      "epoch : 6, train acc : 81.7727 %, train loss : 0.43804489, test acc : 39.8936 %, \n",
      "epoch : 7, train acc : 80.9545 %, train loss : 0.45179844, test acc : 32.7128 %, \n",
      "epoch : 8, train acc : 82.1818 %, train loss : 0.44364550, test acc : 18.3511 %, \n",
      "epoch : 9, train acc : 84.2727 %, train loss : 0.40789014, test acc : 39.6277 %, \n",
      "epoch : 10, train acc : 83.4091 %, train loss : 0.40301702, test acc : 38.0319 %, \n",
      "epoch : 11, train acc : 84.9545 %, train loss : 0.36705359, test acc : 53.1915 %, \n",
      "epoch : 12, train acc : 87.8636 %, train loss : 0.32710327, test acc : 45.4787 %, \n",
      "epoch : 13, train acc : 87.4091 %, train loss : 0.33280139, test acc : 50.0000 %, \n",
      "epoch : 14, train acc : 87.2727 %, train loss : 0.32704248, test acc : 43.8830 %, \n",
      "epoch : 15, train acc : 88.4545 %, train loss : 0.31562764, test acc : 62.2340 %, \n",
      "epoch : 16, train acc : 91.6818 %, train loss : 0.24496079, test acc : 59.8404 %, \n",
      "epoch : 17, train acc : 91.8636 %, train loss : 0.23425321, test acc : 57.1809 %, \n",
      "epoch : 18, train acc : 93.4545 %, train loss : 0.20282829, test acc : 64.6277 %, \n",
      "epoch : 19, train acc : 92.5000 %, train loss : 0.21740093, test acc : 64.8936 %, \n",
      "epoch : 20, train acc : 93.0000 %, train loss : 0.21072946, test acc : 60.3723 %, \n",
      "epoch : 21, train acc : 93.2273 %, train loss : 0.19512460, test acc : 65.6915 %, \n",
      "epoch : 22, train acc : 94.4091 %, train loss : 0.17430338, test acc : 70.4787 %, \n",
      "epoch : 23, train acc : 93.3182 %, train loss : 0.20879044, test acc : 59.3085 %, \n",
      "epoch : 24, train acc : 93.7727 %, train loss : 0.18269442, test acc : 71.8085 %, \n",
      "epoch : 25, train acc : 93.9545 %, train loss : 0.17026996, test acc : 71.0106 %, \n",
      "epoch : 26, train acc : 94.5909 %, train loss : 0.15775441, test acc : 72.8723 %, \n",
      "epoch : 27, train acc : 94.2273 %, train loss : 0.17472376, test acc : 75.0000 %, \n",
      "epoch : 28, train acc : 94.7727 %, train loss : 0.15007448, test acc : 69.4149 %, \n",
      "epoch : 29, train acc : 95.7273 %, train loss : 0.13942098, test acc : 74.4681 %, \n",
      "epoch : 30, train acc : 95.9545 %, train loss : 0.12906358, test acc : 77.9255 %, \n",
      "epoch : 31, train acc : 95.0909 %, train loss : 0.14089894, test acc : 75.0000 %, \n",
      "epoch : 32, train acc : 95.7273 %, train loss : 0.13426701, test acc : 75.5319 %, \n",
      "epoch : 33, train acc : 95.2273 %, train loss : 0.13635268, test acc : 75.5319 %, \n",
      "epoch : 34, train acc : 98.0455 %, train loss : 0.07531983, test acc : 80.0532 %, \n",
      "epoch : 35, train acc : 97.1818 %, train loss : 0.09261683, test acc : 78.1915 %, \n",
      "epoch : 36, train acc : 95.8182 %, train loss : 0.11986792, test acc : 77.9255 %, \n",
      "epoch : 37, train acc : 97.1364 %, train loss : 0.09495369, test acc : 80.0532 %, \n",
      "epoch : 38, train acc : 96.6818 %, train loss : 0.10414901, test acc : 70.4787 %, \n",
      "epoch : 39, train acc : 96.2273 %, train loss : 0.11335653, test acc : 79.2553 %, \n",
      "epoch : 40, train acc : 95.5909 %, train loss : 0.13037130, test acc : 76.0638 %, \n",
      "epoch : 41, train acc : 96.5000 %, train loss : 0.11031324, test acc : 76.3298 %, \n",
      "epoch : 42, train acc : 96.6818 %, train loss : 0.10184782, test acc : 77.3936 %, \n",
      "epoch : 43, train acc : 96.9091 %, train loss : 0.10784817, test acc : 69.6809 %, \n",
      "epoch : 44, train acc : 97.2727 %, train loss : 0.08781776, test acc : 79.7872 %, \n",
      "epoch : 45, train acc : 96.5455 %, train loss : 0.10698373, test acc : 78.7234 %, \n",
      "epoch : 46, train acc : 96.8636 %, train loss : 0.10529958, test acc : 77.1277 %, \n",
      "epoch : 47, train acc : 96.9545 %, train loss : 0.09168297, test acc : 77.3936 %, \n",
      "epoch : 48, train acc : 97.1818 %, train loss : 0.08552897, test acc : 76.5957 %, \n",
      "epoch : 49, train acc : 96.8636 %, train loss : 0.09639633, test acc : 76.3298 %, \n",
      "epoch : 50, train acc : 96.0000 %, train loss : 0.11106729, test acc : 78.1915 %, \n",
      "epoch : 51, train acc : 95.5000 %, train loss : 0.12209636, test acc : 73.1383 %, \n",
      "epoch : 52, train acc : 97.0000 %, train loss : 0.09516514, test acc : 74.7340 %, \n",
      "epoch : 53, train acc : 97.5455 %, train loss : 0.08097530, test acc : 78.1915 %, \n",
      "epoch : 54, train acc : 96.9091 %, train loss : 0.09659238, test acc : 81.6489 %, \n",
      "epoch : 55, train acc : 97.0909 %, train loss : 0.09074372, test acc : 77.3936 %, \n",
      "epoch : 56, train acc : 97.2727 %, train loss : 0.07927330, test acc : 79.5213 %, \n",
      "epoch : 57, train acc : 96.5455 %, train loss : 0.09650402, test acc : 78.4574 %, \n",
      "epoch : 58, train acc : 96.5455 %, train loss : 0.10697937, test acc : 78.9894 %, \n",
      "epoch : 59, train acc : 97.1364 %, train loss : 0.08231836, test acc : 82.1809 %, \n",
      "epoch : 60, train acc : 98.1364 %, train loss : 0.06217366, test acc : 80.3191 %, \n",
      "epoch : 61, train acc : 98.1818 %, train loss : 0.06513069, test acc : 79.2553 %, \n",
      "epoch : 62, train acc : 98.3636 %, train loss : 0.06089887, test acc : 81.9149 %, \n",
      "epoch : 63, train acc : 99.0000 %, train loss : 0.04038213, test acc : 83.2447 %, \n",
      "epoch : 64, train acc : 99.0455 %, train loss : 0.04289208, test acc : 82.1809 %, \n",
      "epoch : 65, train acc : 98.9545 %, train loss : 0.04760695, test acc : 82.7128 %, \n",
      "epoch : 66, train acc : 98.4545 %, train loss : 0.04914956, test acc : 78.7234 %, \n",
      "epoch : 67, train acc : 98.7273 %, train loss : 0.04480890, test acc : 82.1809 %, \n",
      "epoch : 68, train acc : 97.5000 %, train loss : 0.07113624, test acc : 76.5957 %, \n",
      "epoch : 69, train acc : 98.0909 %, train loss : 0.06485055, test acc : 78.9894 %, \n",
      "epoch : 70, train acc : 97.5455 %, train loss : 0.07720200, test acc : 80.5851 %, \n",
      "epoch : 71, train acc : 96.4545 %, train loss : 0.09695897, test acc : 77.1277 %, \n",
      "epoch : 72, train acc : 96.1818 %, train loss : 0.10275176, test acc : 78.7234 %, \n",
      "epoch : 73, train acc : 96.3636 %, train loss : 0.09789827, test acc : 79.7872 %, \n",
      "epoch : 74, train acc : 97.4091 %, train loss : 0.09390696, test acc : 74.7340 %, \n",
      "epoch : 75, train acc : 96.8182 %, train loss : 0.09593482, test acc : 83.2447 %, \n",
      "epoch : 76, train acc : 98.1818 %, train loss : 0.06255740, test acc : 80.0532 %, \n",
      "epoch : 77, train acc : 98.0000 %, train loss : 0.06113331, test acc : 80.5851 %, \n",
      "epoch : 78, train acc : 98.7273 %, train loss : 0.04272564, test acc : 81.1170 %, \n",
      "epoch : 79, train acc : 98.3182 %, train loss : 0.05689673, test acc : 81.6489 %, \n",
      "epoch : 80, train acc : 97.9091 %, train loss : 0.06902107, test acc : 79.2553 %, \n",
      "epoch : 81, train acc : 97.9545 %, train loss : 0.07248346, test acc : 80.0532 %, \n",
      "epoch : 82, train acc : 97.5455 %, train loss : 0.07674596, test acc : 76.3298 %, \n",
      "epoch : 83, train acc : 97.6364 %, train loss : 0.07263358, test acc : 79.5213 %, \n",
      "epoch : 84, train acc : 98.5455 %, train loss : 0.04925024, test acc : 78.4574 %, \n",
      "epoch : 85, train acc : 98.2273 %, train loss : 0.05703747, test acc : 83.2447 %, \n",
      "epoch : 86, train acc : 98.0455 %, train loss : 0.07762525, test acc : 78.1915 %, \n",
      "epoch : 87, train acc : 97.4091 %, train loss : 0.07640118, test acc : 75.2660 %, \n",
      "epoch : 88, train acc : 97.0000 %, train loss : 0.09763510, test acc : 75.7979 %, \n",
      "epoch : 89, train acc : 97.0455 %, train loss : 0.08381544, test acc : 80.0532 %, \n",
      "epoch : 90, train acc : 97.2727 %, train loss : 0.08007284, test acc : 75.7979 %, \n",
      "epoch : 91, train acc : 97.7273 %, train loss : 0.07213974, test acc : 76.8617 %, \n",
      "epoch : 92, train acc : 97.8636 %, train loss : 0.06081180, test acc : 77.9255 %, \n",
      "epoch : 93, train acc : 97.8636 %, train loss : 0.06328145, test acc : 76.8617 %, \n",
      "epoch : 94, train acc : 98.1364 %, train loss : 0.05850947, test acc : 75.7979 %, \n",
      "epoch : 95, train acc : 98.5455 %, train loss : 0.05474514, test acc : 77.3936 %, \n",
      "epoch : 96, train acc : 98.2727 %, train loss : 0.05461431, test acc : 80.0532 %, \n",
      "epoch : 97, train acc : 98.2727 %, train loss : 0.05329913, test acc : 78.4574 %, \n",
      "epoch : 98, train acc : 97.7727 %, train loss : 0.07765304, test acc : 81.3830 %, \n",
      "epoch : 99, train acc : 97.7273 %, train loss : 0.05897822, test acc : 81.1170 %, \n",
      "epoch : 100, train acc : 97.7727 %, train loss : 0.07019445, test acc : 78.4574 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_811), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 60.6364 %, train loss : 0.61280436, test acc : 25.7979 %, \n",
      "epoch : 2, train acc : 70.1364 %, train loss : 0.55653835, test acc : 36.1702 %, \n",
      "epoch : 3, train acc : 71.7273 %, train loss : 0.54018899, test acc : 35.1064 %, \n",
      "epoch : 4, train acc : 77.0909 %, train loss : 0.49923457, test acc : 29.7872 %, \n",
      "epoch : 5, train acc : 77.1364 %, train loss : 0.48532707, test acc : 34.0426 %, \n",
      "epoch : 6, train acc : 80.0455 %, train loss : 0.45910596, test acc : 37.2340 %, \n",
      "epoch : 7, train acc : 81.8182 %, train loss : 0.44565422, test acc : 26.0638 %, \n",
      "epoch : 8, train acc : 80.2727 %, train loss : 0.47176913, test acc : 34.3085 %, \n",
      "epoch : 9, train acc : 84.3636 %, train loss : 0.40518069, test acc : 29.5213 %, \n",
      "epoch : 10, train acc : 85.3182 %, train loss : 0.37304765, test acc : 39.0957 %, \n",
      "epoch : 11, train acc : 85.2273 %, train loss : 0.37094267, test acc : 41.2234 %, \n",
      "epoch : 12, train acc : 87.2727 %, train loss : 0.35140791, test acc : 56.1170 %, \n",
      "epoch : 13, train acc : 88.0455 %, train loss : 0.31764211, test acc : 44.1489 %, \n",
      "epoch : 14, train acc : 89.6364 %, train loss : 0.28514673, test acc : 50.0000 %, \n",
      "epoch : 15, train acc : 90.4545 %, train loss : 0.25089869, test acc : 53.7234 %, \n",
      "epoch : 16, train acc : 91.4091 %, train loss : 0.23879215, test acc : 66.2234 %, \n",
      "epoch : 17, train acc : 91.9091 %, train loss : 0.23115129, test acc : 64.0957 %, \n",
      "epoch : 18, train acc : 92.6818 %, train loss : 0.20294409, test acc : 63.0319 %, \n",
      "epoch : 19, train acc : 92.8636 %, train loss : 0.20892567, test acc : 63.5638 %, \n",
      "epoch : 20, train acc : 94.2273 %, train loss : 0.17866934, test acc : 55.8511 %, \n",
      "epoch : 21, train acc : 93.3182 %, train loss : 0.19596268, test acc : 62.7660 %, \n",
      "epoch : 22, train acc : 94.1818 %, train loss : 0.17996382, test acc : 66.2234 %, \n",
      "epoch : 23, train acc : 94.1818 %, train loss : 0.17898860, test acc : 69.1489 %, \n",
      "epoch : 24, train acc : 95.6364 %, train loss : 0.14244401, test acc : 67.2872 %, \n",
      "epoch : 25, train acc : 95.2727 %, train loss : 0.15071139, test acc : 70.7447 %, \n",
      "epoch : 26, train acc : 95.7727 %, train loss : 0.13538702, test acc : 68.8830 %, \n",
      "epoch : 27, train acc : 94.3636 %, train loss : 0.15922298, test acc : 64.6277 %, \n",
      "epoch : 28, train acc : 93.6818 %, train loss : 0.17350884, test acc : 72.6064 %, \n",
      "epoch : 29, train acc : 95.3182 %, train loss : 0.14573185, test acc : 71.8085 %, \n",
      "epoch : 30, train acc : 95.4091 %, train loss : 0.13624893, test acc : 75.7979 %, \n",
      "epoch : 31, train acc : 96.9091 %, train loss : 0.10205808, test acc : 69.1489 %, \n",
      "epoch : 32, train acc : 95.5000 %, train loss : 0.13421761, test acc : 73.6702 %, \n",
      "epoch : 33, train acc : 95.6818 %, train loss : 0.13004002, test acc : 71.8085 %, \n",
      "epoch : 34, train acc : 96.0909 %, train loss : 0.13470742, test acc : 75.7979 %, \n",
      "epoch : 35, train acc : 94.8636 %, train loss : 0.13580981, test acc : 76.0638 %, \n",
      "epoch : 36, train acc : 97.2273 %, train loss : 0.09906844, test acc : 77.9255 %, \n",
      "epoch : 37, train acc : 96.0000 %, train loss : 0.12416137, test acc : 80.0532 %, \n",
      "epoch : 38, train acc : 97.0000 %, train loss : 0.09927716, test acc : 75.0000 %, \n",
      "epoch : 39, train acc : 96.9091 %, train loss : 0.11276750, test acc : 72.3404 %, \n",
      "epoch : 40, train acc : 96.6364 %, train loss : 0.10297161, test acc : 72.3404 %, \n",
      "epoch : 41, train acc : 95.7273 %, train loss : 0.12308149, test acc : 75.2660 %, \n",
      "epoch : 42, train acc : 96.5000 %, train loss : 0.10656561, test acc : 74.7340 %, \n",
      "epoch : 43, train acc : 96.2273 %, train loss : 0.11799636, test acc : 74.7340 %, \n",
      "epoch : 44, train acc : 95.5000 %, train loss : 0.12383215, test acc : 71.8085 %, \n",
      "epoch : 45, train acc : 96.0455 %, train loss : 0.10972760, test acc : 74.4681 %, \n",
      "epoch : 46, train acc : 97.8182 %, train loss : 0.07013369, test acc : 73.1383 %, \n",
      "epoch : 47, train acc : 97.0909 %, train loss : 0.09446546, test acc : 75.5319 %, \n",
      "epoch : 48, train acc : 97.5909 %, train loss : 0.08737112, test acc : 80.0532 %, \n",
      "epoch : 49, train acc : 96.9545 %, train loss : 0.09274202, test acc : 78.4574 %, \n",
      "epoch : 50, train acc : 97.0909 %, train loss : 0.08339047, test acc : 75.5319 %, \n",
      "epoch : 51, train acc : 95.7727 %, train loss : 0.12625455, test acc : 77.1277 %, \n",
      "epoch : 52, train acc : 97.3636 %, train loss : 0.09676443, test acc : 77.3936 %, \n",
      "epoch : 53, train acc : 97.1364 %, train loss : 0.08730535, test acc : 75.2660 %, \n",
      "epoch : 54, train acc : 97.7273 %, train loss : 0.08324892, test acc : 78.1915 %, \n",
      "epoch : 55, train acc : 96.5909 %, train loss : 0.10246991, test acc : 77.6596 %, \n",
      "epoch : 56, train acc : 97.5000 %, train loss : 0.09096655, test acc : 75.0000 %, \n",
      "epoch : 57, train acc : 98.2727 %, train loss : 0.06479506, test acc : 79.2553 %, \n",
      "epoch : 58, train acc : 98.5909 %, train loss : 0.05456047, test acc : 78.7234 %, \n",
      "epoch : 59, train acc : 98.0909 %, train loss : 0.06486801, test acc : 79.7872 %, \n",
      "epoch : 60, train acc : 98.3636 %, train loss : 0.06093918, test acc : 75.7979 %, \n",
      "epoch : 61, train acc : 97.4545 %, train loss : 0.07640376, test acc : 76.8617 %, \n",
      "epoch : 62, train acc : 97.0455 %, train loss : 0.08878459, test acc : 77.3936 %, \n",
      "epoch : 63, train acc : 97.0000 %, train loss : 0.09838391, test acc : 79.5213 %, \n",
      "epoch : 64, train acc : 96.6364 %, train loss : 0.09908145, test acc : 72.6064 %, \n",
      "epoch : 65, train acc : 96.0000 %, train loss : 0.11894798, test acc : 76.3298 %, \n",
      "epoch : 66, train acc : 97.0000 %, train loss : 0.08884818, test acc : 77.1277 %, \n",
      "epoch : 67, train acc : 98.1364 %, train loss : 0.07382920, test acc : 77.9255 %, \n",
      "epoch : 68, train acc : 97.8182 %, train loss : 0.07609164, test acc : 79.2553 %, \n",
      "epoch : 69, train acc : 98.3182 %, train loss : 0.05363549, test acc : 79.7872 %, \n",
      "epoch : 70, train acc : 98.7273 %, train loss : 0.05342212, test acc : 77.9255 %, \n",
      "epoch : 71, train acc : 98.4545 %, train loss : 0.05783000, test acc : 80.0532 %, \n",
      "epoch : 72, train acc : 98.4091 %, train loss : 0.05064923, test acc : 77.6596 %, \n",
      "epoch : 73, train acc : 98.4091 %, train loss : 0.05862358, test acc : 78.1915 %, \n",
      "epoch : 74, train acc : 98.2273 %, train loss : 0.06676356, test acc : 78.9894 %, \n",
      "epoch : 75, train acc : 97.2727 %, train loss : 0.08922830, test acc : 79.2553 %, \n",
      "epoch : 76, train acc : 98.2273 %, train loss : 0.05346751, test acc : 79.2553 %, \n",
      "epoch : 77, train acc : 98.0000 %, train loss : 0.06109667, test acc : 79.2553 %, \n",
      "epoch : 78, train acc : 98.1818 %, train loss : 0.05849517, test acc : 76.3298 %, \n",
      "epoch : 79, train acc : 98.4091 %, train loss : 0.05270819, test acc : 78.4574 %, \n",
      "epoch : 80, train acc : 98.2727 %, train loss : 0.05738613, test acc : 79.7872 %, \n",
      "epoch : 81, train acc : 98.1364 %, train loss : 0.06295394, test acc : 76.8617 %, \n",
      "epoch : 82, train acc : 96.9545 %, train loss : 0.09954268, test acc : 73.4043 %, \n",
      "epoch : 83, train acc : 96.0000 %, train loss : 0.11555791, test acc : 73.1383 %, \n",
      "epoch : 84, train acc : 95.5455 %, train loss : 0.13056135, test acc : 74.2021 %, \n",
      "epoch : 85, train acc : 96.7727 %, train loss : 0.10249753, test acc : 78.1915 %, \n",
      "epoch : 86, train acc : 97.0909 %, train loss : 0.08367652, test acc : 77.3936 %, \n",
      "epoch : 87, train acc : 97.4091 %, train loss : 0.07897412, test acc : 79.5213 %, \n",
      "epoch : 88, train acc : 98.7727 %, train loss : 0.04872544, test acc : 76.0638 %, \n",
      "epoch : 89, train acc : 98.3636 %, train loss : 0.05423236, test acc : 80.3191 %, \n",
      "epoch : 90, train acc : 98.7273 %, train loss : 0.04891861, test acc : 80.3191 %, \n",
      "epoch : 91, train acc : 98.9545 %, train loss : 0.03839888, test acc : 79.5213 %, \n",
      "epoch : 92, train acc : 98.8636 %, train loss : 0.04505815, test acc : 78.7234 %, \n",
      "epoch : 93, train acc : 98.7273 %, train loss : 0.03796330, test acc : 78.9894 %, \n",
      "epoch : 94, train acc : 99.0909 %, train loss : 0.03297856, test acc : 76.8617 %, \n",
      "epoch : 95, train acc : 98.3182 %, train loss : 0.05404893, test acc : 75.7979 %, \n",
      "epoch : 96, train acc : 97.5909 %, train loss : 0.07750171, test acc : 77.1277 %, \n",
      "epoch : 97, train acc : 97.6818 %, train loss : 0.07030695, test acc : 76.5957 %, \n",
      "epoch : 98, train acc : 97.3636 %, train loss : 0.08766059, test acc : 77.9255 %, \n",
      "epoch : 99, train acc : 97.5000 %, train loss : 0.08631212, test acc : 77.6596 %, \n",
      "epoch : 100, train acc : 98.1818 %, train loss : 0.06152010, test acc : 79.5213 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_824), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 62.7273 %, train loss : 0.60992672, test acc : 25.0000 %, \n",
      "epoch : 2, train acc : 67.4091 %, train loss : 0.56579168, test acc : 19.9468 %, \n",
      "epoch : 3, train acc : 72.1364 %, train loss : 0.53806005, test acc : 30.3191 %, \n",
      "epoch : 4, train acc : 76.7727 %, train loss : 0.49879905, test acc : 22.0745 %, \n",
      "epoch : 5, train acc : 76.8636 %, train loss : 0.50678174, test acc : 15.6915 %, \n",
      "epoch : 6, train acc : 81.4545 %, train loss : 0.45545432, test acc : 16.4894 %, \n",
      "epoch : 7, train acc : 80.0000 %, train loss : 0.47936792, test acc : 23.6702 %, \n",
      "epoch : 8, train acc : 82.5455 %, train loss : 0.44893097, test acc : 25.2660 %, \n",
      "epoch : 9, train acc : 83.9091 %, train loss : 0.41967146, test acc : 35.9043 %, \n",
      "epoch : 10, train acc : 83.6818 %, train loss : 0.40403537, test acc : 36.9681 %, \n",
      "epoch : 11, train acc : 88.0455 %, train loss : 0.33409449, test acc : 48.6702 %, \n",
      "epoch : 12, train acc : 86.9091 %, train loss : 0.33836560, test acc : 47.0745 %, \n",
      "epoch : 13, train acc : 88.5000 %, train loss : 0.30815154, test acc : 51.3298 %, \n",
      "epoch : 14, train acc : 91.0455 %, train loss : 0.26171446, test acc : 52.1277 %, \n",
      "epoch : 15, train acc : 89.9091 %, train loss : 0.28151189, test acc : 49.7340 %, \n",
      "epoch : 16, train acc : 92.0909 %, train loss : 0.24065883, test acc : 55.5851 %, \n",
      "epoch : 17, train acc : 91.5000 %, train loss : 0.23900576, test acc : 51.5957 %, \n",
      "epoch : 18, train acc : 91.8636 %, train loss : 0.23992904, test acc : 53.7234 %, \n",
      "epoch : 19, train acc : 92.9545 %, train loss : 0.21700450, test acc : 53.1915 %, \n",
      "epoch : 20, train acc : 91.8636 %, train loss : 0.21936995, test acc : 53.7234 %, \n",
      "epoch : 21, train acc : 92.3636 %, train loss : 0.22341691, test acc : 60.9043 %, \n",
      "epoch : 22, train acc : 92.8636 %, train loss : 0.20059820, test acc : 56.9149 %, \n",
      "epoch : 23, train acc : 92.4545 %, train loss : 0.21263898, test acc : 57.7128 %, \n",
      "epoch : 24, train acc : 92.2273 %, train loss : 0.22952845, test acc : 61.1702 %, \n",
      "epoch : 25, train acc : 92.9091 %, train loss : 0.20530092, test acc : 61.4362 %, \n",
      "epoch : 26, train acc : 92.8182 %, train loss : 0.20499959, test acc : 64.8936 %, \n",
      "epoch : 27, train acc : 93.3182 %, train loss : 0.19456628, test acc : 67.0213 %, \n",
      "epoch : 28, train acc : 95.1818 %, train loss : 0.15885167, test acc : 66.7553 %, \n",
      "epoch : 29, train acc : 96.2727 %, train loss : 0.13205861, test acc : 72.3404 %, \n",
      "epoch : 30, train acc : 95.2727 %, train loss : 0.15538232, test acc : 66.7553 %, \n",
      "epoch : 31, train acc : 93.8182 %, train loss : 0.17425317, test acc : 66.2234 %, \n",
      "epoch : 32, train acc : 95.2273 %, train loss : 0.14642455, test acc : 74.7340 %, \n",
      "epoch : 33, train acc : 94.5455 %, train loss : 0.14025879, test acc : 69.4149 %, \n",
      "epoch : 34, train acc : 96.7273 %, train loss : 0.11460920, test acc : 73.6702 %, \n",
      "epoch : 35, train acc : 96.7273 %, train loss : 0.11861474, test acc : 73.1383 %, \n",
      "epoch : 36, train acc : 96.9545 %, train loss : 0.10318402, test acc : 73.6702 %, \n",
      "epoch : 37, train acc : 95.6364 %, train loss : 0.13000664, test acc : 73.6702 %, \n",
      "epoch : 38, train acc : 97.0909 %, train loss : 0.09895323, test acc : 71.8085 %, \n",
      "epoch : 39, train acc : 96.1818 %, train loss : 0.12001491, test acc : 70.4787 %, \n",
      "epoch : 40, train acc : 95.8636 %, train loss : 0.12498829, test acc : 71.8085 %, \n",
      "epoch : 41, train acc : 96.7727 %, train loss : 0.10307342, test acc : 76.5957 %, \n",
      "epoch : 42, train acc : 96.9545 %, train loss : 0.09399833, test acc : 75.5319 %, \n",
      "epoch : 43, train acc : 96.6364 %, train loss : 0.10577004, test acc : 73.1383 %, \n",
      "epoch : 44, train acc : 96.2273 %, train loss : 0.12547165, test acc : 71.5426 %, \n",
      "epoch : 45, train acc : 96.6818 %, train loss : 0.11145780, test acc : 69.4149 %, \n",
      "epoch : 46, train acc : 96.6364 %, train loss : 0.10911997, test acc : 72.0745 %, \n",
      "epoch : 47, train acc : 96.0455 %, train loss : 0.10756725, test acc : 69.6809 %, \n",
      "epoch : 48, train acc : 96.4091 %, train loss : 0.12060735, test acc : 72.0745 %, \n",
      "epoch : 49, train acc : 97.2727 %, train loss : 0.09178022, test acc : 74.7340 %, \n",
      "epoch : 50, train acc : 97.0455 %, train loss : 0.09507652, test acc : 68.8830 %, \n",
      "epoch : 51, train acc : 97.2273 %, train loss : 0.08394031, test acc : 73.9362 %, \n",
      "epoch : 52, train acc : 97.3182 %, train loss : 0.08929918, test acc : 71.8085 %, \n",
      "epoch : 53, train acc : 97.1364 %, train loss : 0.08521539, test acc : 75.7979 %, \n",
      "epoch : 54, train acc : 97.2727 %, train loss : 0.08980713, test acc : 73.9362 %, \n",
      "epoch : 55, train acc : 96.8636 %, train loss : 0.09464149, test acc : 77.1277 %, \n",
      "epoch : 56, train acc : 96.2273 %, train loss : 0.11045106, test acc : 70.7447 %, \n",
      "epoch : 57, train acc : 97.5000 %, train loss : 0.08386484, test acc : 73.4043 %, \n",
      "epoch : 58, train acc : 97.1364 %, train loss : 0.09470432, test acc : 73.6702 %, \n",
      "epoch : 59, train acc : 97.2273 %, train loss : 0.08851108, test acc : 75.5319 %, \n",
      "epoch : 60, train acc : 97.0455 %, train loss : 0.09031157, test acc : 73.4043 %, \n",
      "epoch : 61, train acc : 96.9091 %, train loss : 0.09283256, test acc : 75.2660 %, \n",
      "epoch : 62, train acc : 97.5455 %, train loss : 0.06889734, test acc : 78.7234 %, \n",
      "epoch : 63, train acc : 98.0909 %, train loss : 0.07017303, test acc : 76.5957 %, \n",
      "epoch : 64, train acc : 97.8182 %, train loss : 0.06556412, test acc : 76.8617 %, \n",
      "epoch : 65, train acc : 97.9545 %, train loss : 0.06034829, test acc : 72.8723 %, \n",
      "epoch : 66, train acc : 98.1818 %, train loss : 0.06344536, test acc : 72.3404 %, \n",
      "epoch : 67, train acc : 97.6364 %, train loss : 0.07701804, test acc : 79.5213 %, \n",
      "epoch : 68, train acc : 98.3182 %, train loss : 0.05333558, test acc : 73.9362 %, \n",
      "epoch : 69, train acc : 98.5000 %, train loss : 0.04968549, test acc : 77.9255 %, \n",
      "epoch : 70, train acc : 98.0455 %, train loss : 0.05658022, test acc : 77.6596 %, \n",
      "epoch : 71, train acc : 98.1364 %, train loss : 0.05542119, test acc : 77.9255 %, \n",
      "epoch : 72, train acc : 98.1818 %, train loss : 0.05617029, test acc : 74.7340 %, \n",
      "epoch : 73, train acc : 98.0909 %, train loss : 0.05785651, test acc : 74.2021 %, \n",
      "epoch : 74, train acc : 96.9545 %, train loss : 0.08433349, test acc : 73.1383 %, \n",
      "epoch : 75, train acc : 97.3182 %, train loss : 0.08853403, test acc : 72.3404 %, \n",
      "epoch : 76, train acc : 97.5909 %, train loss : 0.07175541, test acc : 74.4681 %, \n",
      "epoch : 77, train acc : 96.8636 %, train loss : 0.09219413, test acc : 79.2553 %, \n",
      "epoch : 78, train acc : 97.4545 %, train loss : 0.08305862, test acc : 76.3298 %, \n",
      "epoch : 79, train acc : 97.5909 %, train loss : 0.07648890, test acc : 76.5957 %, \n",
      "epoch : 80, train acc : 98.0455 %, train loss : 0.06525370, test acc : 73.6702 %, \n",
      "epoch : 81, train acc : 97.6818 %, train loss : 0.07585866, test acc : 76.0638 %, \n",
      "epoch : 82, train acc : 98.3182 %, train loss : 0.05422024, test acc : 76.0638 %, \n",
      "epoch : 83, train acc : 97.0455 %, train loss : 0.08206179, test acc : 75.2660 %, \n",
      "epoch : 84, train acc : 98.1364 %, train loss : 0.05821712, test acc : 74.7340 %, \n",
      "epoch : 85, train acc : 97.3182 %, train loss : 0.08340318, test acc : 76.0638 %, \n",
      "epoch : 86, train acc : 97.4091 %, train loss : 0.07833955, test acc : 76.5957 %, \n",
      "epoch : 87, train acc : 97.6364 %, train loss : 0.06937510, test acc : 78.4574 %, \n",
      "epoch : 88, train acc : 97.5455 %, train loss : 0.07753241, test acc : 79.2553 %, \n",
      "epoch : 89, train acc : 97.9545 %, train loss : 0.05892651, test acc : 76.8617 %, \n",
      "epoch : 90, train acc : 98.1364 %, train loss : 0.05774063, test acc : 76.5957 %, \n",
      "epoch : 91, train acc : 97.8636 %, train loss : 0.05637136, test acc : 78.4574 %, \n",
      "epoch : 92, train acc : 98.5000 %, train loss : 0.05194142, test acc : 78.7234 %, \n",
      "epoch : 93, train acc : 98.5455 %, train loss : 0.04861768, test acc : 79.2553 %, \n",
      "epoch : 94, train acc : 98.5000 %, train loss : 0.05635842, test acc : 79.5213 %, \n",
      "epoch : 95, train acc : 98.4091 %, train loss : 0.05719133, test acc : 77.9255 %, \n",
      "epoch : 96, train acc : 98.0000 %, train loss : 0.05511655, test acc : 78.9894 %, \n",
      "epoch : 97, train acc : 98.3182 %, train loss : 0.05546698, test acc : 77.9255 %, \n",
      "epoch : 98, train acc : 98.1364 %, train loss : 0.05526255, test acc : 76.5957 %, \n",
      "epoch : 99, train acc : 97.3182 %, train loss : 0.08273924, test acc : 73.4043 %, \n",
      "epoch : 100, train acc : 97.2727 %, train loss : 0.07707129, test acc : 80.8511 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_837), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 64.7273 %, train loss : 0.59656253, test acc : 24.4681 %, \n",
      "epoch : 2, train acc : 68.0000 %, train loss : 0.56691695, test acc : 11.1702 %, \n",
      "epoch : 3, train acc : 72.2727 %, train loss : 0.53006736, test acc : 37.7660 %, \n",
      "epoch : 4, train acc : 74.8182 %, train loss : 0.51725402, test acc : 25.7979 %, \n",
      "epoch : 5, train acc : 78.6818 %, train loss : 0.47635160, test acc : 26.8617 %, \n",
      "epoch : 6, train acc : 80.0909 %, train loss : 0.47159573, test acc : 28.7234 %, \n",
      "epoch : 7, train acc : 81.3636 %, train loss : 0.46029644, test acc : 38.2979 %, \n",
      "epoch : 8, train acc : 84.0000 %, train loss : 0.40754693, test acc : 26.3298 %, \n",
      "epoch : 9, train acc : 84.0455 %, train loss : 0.41226858, test acc : 34.5745 %, \n",
      "epoch : 10, train acc : 85.4545 %, train loss : 0.37182841, test acc : 34.3085 %, \n",
      "epoch : 11, train acc : 86.0000 %, train loss : 0.37275859, test acc : 51.8617 %, \n",
      "epoch : 12, train acc : 85.9091 %, train loss : 0.36375078, test acc : 44.9468 %, \n",
      "epoch : 13, train acc : 86.8182 %, train loss : 0.33554635, test acc : 49.4681 %, \n",
      "epoch : 14, train acc : 87.9545 %, train loss : 0.31355050, test acc : 52.6596 %, \n",
      "epoch : 15, train acc : 90.4091 %, train loss : 0.26387156, test acc : 56.9149 %, \n",
      "epoch : 16, train acc : 90.0909 %, train loss : 0.27095259, test acc : 51.5957 %, \n",
      "epoch : 17, train acc : 90.8636 %, train loss : 0.25106562, test acc : 57.4468 %, \n",
      "epoch : 18, train acc : 90.8636 %, train loss : 0.25426071, test acc : 62.7660 %, \n",
      "epoch : 19, train acc : 92.3636 %, train loss : 0.22242377, test acc : 61.9681 %, \n",
      "epoch : 20, train acc : 93.1364 %, train loss : 0.20203474, test acc : 69.1489 %, \n",
      "epoch : 21, train acc : 93.4091 %, train loss : 0.19441917, test acc : 67.2872 %, \n",
      "epoch : 22, train acc : 94.5909 %, train loss : 0.17752678, test acc : 62.7660 %, \n",
      "epoch : 23, train acc : 93.8636 %, train loss : 0.19286921, test acc : 64.0957 %, \n",
      "epoch : 24, train acc : 94.9091 %, train loss : 0.15401548, test acc : 67.2872 %, \n",
      "epoch : 25, train acc : 93.0909 %, train loss : 0.19878920, test acc : 61.9681 %, \n",
      "epoch : 26, train acc : 94.3636 %, train loss : 0.16348738, test acc : 68.3511 %, \n",
      "epoch : 27, train acc : 94.4545 %, train loss : 0.16931823, test acc : 63.0319 %, \n",
      "epoch : 28, train acc : 93.2727 %, train loss : 0.19018300, test acc : 63.2979 %, \n",
      "epoch : 29, train acc : 94.3182 %, train loss : 0.16756064, test acc : 69.9468 %, \n",
      "epoch : 30, train acc : 94.2727 %, train loss : 0.16278684, test acc : 66.4894 %, \n",
      "epoch : 31, train acc : 95.4091 %, train loss : 0.14353916, test acc : 68.6170 %, \n",
      "epoch : 32, train acc : 94.4545 %, train loss : 0.16247985, test acc : 60.9043 %, \n",
      "epoch : 33, train acc : 94.2727 %, train loss : 0.16306415, test acc : 66.7553 %, \n",
      "epoch : 34, train acc : 94.9091 %, train loss : 0.14994981, test acc : 69.6809 %, \n",
      "epoch : 35, train acc : 95.0455 %, train loss : 0.15165671, test acc : 70.2128 %, \n",
      "epoch : 36, train acc : 95.0000 %, train loss : 0.15870990, test acc : 68.0851 %, \n",
      "epoch : 37, train acc : 95.1364 %, train loss : 0.14099461, test acc : 73.4043 %, \n",
      "epoch : 38, train acc : 95.5455 %, train loss : 0.13846102, test acc : 71.5426 %, \n",
      "epoch : 39, train acc : 96.8182 %, train loss : 0.10984331, test acc : 69.6809 %, \n",
      "epoch : 40, train acc : 96.1818 %, train loss : 0.11969286, test acc : 71.5426 %, \n",
      "epoch : 41, train acc : 96.4545 %, train loss : 0.10378113, test acc : 75.2660 %, \n",
      "epoch : 42, train acc : 97.0000 %, train loss : 0.08951372, test acc : 73.1383 %, \n",
      "epoch : 43, train acc : 95.9545 %, train loss : 0.12060568, test acc : 73.4043 %, \n",
      "epoch : 44, train acc : 97.1818 %, train loss : 0.08599801, test acc : 72.8723 %, \n",
      "epoch : 45, train acc : 96.9091 %, train loss : 0.09669951, test acc : 76.3298 %, \n",
      "epoch : 46, train acc : 97.1818 %, train loss : 0.10119976, test acc : 77.1277 %, \n",
      "epoch : 47, train acc : 97.3636 %, train loss : 0.08396747, test acc : 73.6702 %, \n",
      "epoch : 48, train acc : 97.2273 %, train loss : 0.09212192, test acc : 73.6702 %, \n",
      "epoch : 49, train acc : 96.7273 %, train loss : 0.10257882, test acc : 74.7340 %, \n",
      "epoch : 50, train acc : 97.2727 %, train loss : 0.08442328, test acc : 72.3404 %, \n",
      "epoch : 51, train acc : 97.2273 %, train loss : 0.08887608, test acc : 72.8723 %, \n",
      "epoch : 52, train acc : 97.7727 %, train loss : 0.08024172, test acc : 72.6064 %, \n",
      "epoch : 53, train acc : 97.6818 %, train loss : 0.07033313, test acc : 76.5957 %, \n",
      "epoch : 54, train acc : 98.2727 %, train loss : 0.05670622, test acc : 78.1915 %, \n",
      "epoch : 55, train acc : 98.3636 %, train loss : 0.05422063, test acc : 75.2660 %, \n",
      "epoch : 56, train acc : 97.8636 %, train loss : 0.07562444, test acc : 76.0638 %, \n",
      "epoch : 57, train acc : 98.1818 %, train loss : 0.07027810, test acc : 72.6064 %, \n",
      "epoch : 58, train acc : 96.4091 %, train loss : 0.11162006, test acc : 75.7979 %, \n",
      "epoch : 59, train acc : 97.3182 %, train loss : 0.08794910, test acc : 68.8830 %, \n",
      "epoch : 60, train acc : 95.4091 %, train loss : 0.13954836, test acc : 73.6702 %, \n",
      "epoch : 61, train acc : 96.2727 %, train loss : 0.11250926, test acc : 66.7553 %, \n",
      "epoch : 62, train acc : 96.4091 %, train loss : 0.10787510, test acc : 71.0106 %, \n",
      "epoch : 63, train acc : 97.2273 %, train loss : 0.08356720, test acc : 77.9255 %, \n",
      "epoch : 64, train acc : 98.1818 %, train loss : 0.06100824, test acc : 75.7979 %, \n",
      "epoch : 65, train acc : 98.5000 %, train loss : 0.05114488, test acc : 76.3298 %, \n",
      "epoch : 66, train acc : 98.1818 %, train loss : 0.06252946, test acc : 77.3936 %, \n",
      "epoch : 67, train acc : 98.5455 %, train loss : 0.05732232, test acc : 76.3298 %, \n",
      "epoch : 68, train acc : 98.3182 %, train loss : 0.05745645, test acc : 76.8617 %, \n",
      "epoch : 69, train acc : 98.4091 %, train loss : 0.05385877, test acc : 76.3298 %, \n",
      "epoch : 70, train acc : 97.0909 %, train loss : 0.08601155, test acc : 75.0000 %, \n",
      "epoch : 71, train acc : 97.5455 %, train loss : 0.09025183, test acc : 75.2660 %, \n",
      "epoch : 72, train acc : 97.5000 %, train loss : 0.07956817, test acc : 72.8723 %, \n",
      "epoch : 73, train acc : 97.0909 %, train loss : 0.09705005, test acc : 70.2128 %, \n",
      "epoch : 74, train acc : 97.8636 %, train loss : 0.06819159, test acc : 78.1915 %, \n",
      "epoch : 75, train acc : 97.7727 %, train loss : 0.07082700, test acc : 77.6596 %, \n",
      "epoch : 76, train acc : 98.0000 %, train loss : 0.06155014, test acc : 76.3298 %, \n",
      "epoch : 77, train acc : 98.5455 %, train loss : 0.04150294, test acc : 79.5213 %, \n",
      "epoch : 78, train acc : 98.4545 %, train loss : 0.05566126, test acc : 78.1915 %, \n",
      "epoch : 79, train acc : 98.9091 %, train loss : 0.03894124, test acc : 77.1277 %, \n",
      "epoch : 80, train acc : 99.1364 %, train loss : 0.03217360, test acc : 75.7979 %, \n",
      "epoch : 81, train acc : 98.4091 %, train loss : 0.04932873, test acc : 69.9468 %, \n",
      "epoch : 82, train acc : 97.8636 %, train loss : 0.07867970, test acc : 73.9362 %, \n",
      "epoch : 83, train acc : 97.2727 %, train loss : 0.08086568, test acc : 75.7979 %, \n",
      "epoch : 84, train acc : 97.9545 %, train loss : 0.06929146, test acc : 75.2660 %, \n",
      "epoch : 85, train acc : 98.3182 %, train loss : 0.05268350, test acc : 75.5319 %, \n",
      "epoch : 86, train acc : 96.5000 %, train loss : 0.10158378, test acc : 78.1915 %, \n",
      "epoch : 87, train acc : 97.5455 %, train loss : 0.07668896, test acc : 74.4681 %, \n",
      "epoch : 88, train acc : 97.8636 %, train loss : 0.06461882, test acc : 76.0638 %, \n",
      "epoch : 89, train acc : 98.0909 %, train loss : 0.06885259, test acc : 77.6596 %, \n",
      "epoch : 90, train acc : 98.4091 %, train loss : 0.05214347, test acc : 75.7979 %, \n",
      "epoch : 91, train acc : 98.1364 %, train loss : 0.05485497, test acc : 76.3298 %, \n",
      "epoch : 92, train acc : 98.1818 %, train loss : 0.05930673, test acc : 76.8617 %, \n",
      "epoch : 93, train acc : 97.3636 %, train loss : 0.08387958, test acc : 77.6596 %, \n",
      "epoch : 94, train acc : 97.0909 %, train loss : 0.08360740, test acc : 72.8723 %, \n",
      "epoch : 95, train acc : 97.9545 %, train loss : 0.06436865, test acc : 78.7234 %, \n",
      "epoch : 96, train acc : 98.1364 %, train loss : 0.05774879, test acc : 76.0638 %, \n",
      "epoch : 97, train acc : 98.3636 %, train loss : 0.05473766, test acc : 76.8617 %, \n",
      "epoch : 98, train acc : 98.9091 %, train loss : 0.04455674, test acc : 81.3830 %, \n",
      "epoch : 99, train acc : 99.5909 %, train loss : 0.01901418, test acc : 79.2553 %, \n",
      "epoch : 100, train acc : 99.3636 %, train loss : 0.02379000, test acc : 78.1915 %, \n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_850), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(1, 21, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "epoch : 1, train acc : 60.0000 %, train loss : 0.61211469, test acc : 12.7660 %, \n",
      "epoch : 2, train acc : 70.3636 %, train loss : 0.55123380, test acc : 28.9894 %, \n",
      "epoch : 3, train acc : 73.4091 %, train loss : 0.53817025, test acc : 37.2340 %, \n",
      "epoch : 4, train acc : 76.7273 %, train loss : 0.49532409, test acc : 26.3298 %, \n",
      "epoch : 5, train acc : 76.6818 %, train loss : 0.51262079, test acc : 26.5957 %, \n",
      "epoch : 6, train acc : 78.5909 %, train loss : 0.48270172, test acc : 24.2021 %, \n",
      "epoch : 7, train acc : 81.7273 %, train loss : 0.45819361, test acc : 21.8085 %, \n",
      "epoch : 8, train acc : 83.5455 %, train loss : 0.42684023, test acc : 32.9787 %, \n",
      "epoch : 9, train acc : 84.6364 %, train loss : 0.39357814, test acc : 43.6170 %, \n",
      "epoch : 10, train acc : 85.0455 %, train loss : 0.38266352, test acc : 42.5532 %, \n",
      "epoch : 11, train acc : 88.1818 %, train loss : 0.32606465, test acc : 43.6170 %, \n",
      "epoch : 12, train acc : 86.3182 %, train loss : 0.34182452, test acc : 40.6915 %, \n",
      "epoch : 13, train acc : 89.5455 %, train loss : 0.30055056, test acc : 48.9362 %, \n",
      "epoch : 14, train acc : 89.3182 %, train loss : 0.29758353, test acc : 38.2979 %, \n",
      "epoch : 15, train acc : 89.0000 %, train loss : 0.28760617, test acc : 51.3298 %, \n",
      "epoch : 16, train acc : 89.3182 %, train loss : 0.29276628, test acc : 52.6596 %, \n",
      "epoch : 17, train acc : 90.8182 %, train loss : 0.25728604, test acc : 57.1809 %, \n",
      "epoch : 18, train acc : 91.3182 %, train loss : 0.24216475, test acc : 50.0000 %, \n",
      "epoch : 19, train acc : 92.5909 %, train loss : 0.21400534, test acc : 59.3085 %, \n",
      "epoch : 20, train acc : 92.6364 %, train loss : 0.21480933, test acc : 69.4149 %, \n",
      "epoch : 21, train acc : 94.0455 %, train loss : 0.19184456, test acc : 67.8191 %, \n",
      "epoch : 22, train acc : 93.5000 %, train loss : 0.19876345, test acc : 67.2872 %, \n",
      "epoch : 23, train acc : 93.8182 %, train loss : 0.17727370, test acc : 64.3617 %, \n",
      "epoch : 24, train acc : 93.9545 %, train loss : 0.16288833, test acc : 63.0319 %, \n",
      "epoch : 25, train acc : 93.2727 %, train loss : 0.19385091, test acc : 68.8830 %, \n",
      "epoch : 26, train acc : 93.0455 %, train loss : 0.20192375, test acc : 74.4681 %, \n",
      "epoch : 27, train acc : 94.8636 %, train loss : 0.16095742, test acc : 65.4255 %, \n",
      "epoch : 28, train acc : 94.4091 %, train loss : 0.16601389, test acc : 68.0851 %, \n",
      "epoch : 29, train acc : 95.5000 %, train loss : 0.13417966, test acc : 69.9468 %, \n",
      "epoch : 30, train acc : 95.2273 %, train loss : 0.14336435, test acc : 67.5532 %, \n",
      "epoch : 31, train acc : 95.8182 %, train loss : 0.12549293, test acc : 76.5957 %, \n",
      "epoch : 32, train acc : 95.4545 %, train loss : 0.13755745, test acc : 72.8723 %, \n",
      "epoch : 33, train acc : 95.7727 %, train loss : 0.12754966, test acc : 73.9362 %, \n",
      "epoch : 34, train acc : 95.6818 %, train loss : 0.13845397, test acc : 77.6596 %, \n",
      "epoch : 35, train acc : 96.6364 %, train loss : 0.10613841, test acc : 80.3191 %, \n",
      "epoch : 36, train acc : 95.5909 %, train loss : 0.12623501, test acc : 80.8511 %, \n",
      "epoch : 37, train acc : 96.1818 %, train loss : 0.11553274, test acc : 74.7340 %, \n",
      "epoch : 38, train acc : 96.7727 %, train loss : 0.10587982, test acc : 76.5957 %, \n",
      "epoch : 39, train acc : 96.9091 %, train loss : 0.09516842, test acc : 74.4681 %, \n",
      "epoch : 40, train acc : 97.3182 %, train loss : 0.09234858, test acc : 75.5319 %, \n",
      "epoch : 41, train acc : 96.7727 %, train loss : 0.10303698, test acc : 77.1277 %, \n",
      "epoch : 42, train acc : 96.5909 %, train loss : 0.09929217, test acc : 76.5957 %, \n",
      "epoch : 43, train acc : 96.4545 %, train loss : 0.11289799, test acc : 75.5319 %, \n",
      "epoch : 44, train acc : 96.7273 %, train loss : 0.11200255, test acc : 75.0000 %, \n",
      "epoch : 45, train acc : 96.8182 %, train loss : 0.10150697, test acc : 75.7979 %, \n",
      "epoch : 46, train acc : 95.9091 %, train loss : 0.12225020, test acc : 76.5957 %, \n",
      "epoch : 47, train acc : 97.0455 %, train loss : 0.09667395, test acc : 78.4574 %, \n",
      "epoch : 48, train acc : 97.4545 %, train loss : 0.08202012, test acc : 78.4574 %, \n",
      "epoch : 49, train acc : 98.1818 %, train loss : 0.06926617, test acc : 78.9894 %, \n",
      "epoch : 50, train acc : 97.5455 %, train loss : 0.08384161, test acc : 79.2553 %, \n",
      "epoch : 51, train acc : 97.3182 %, train loss : 0.08205114, test acc : 80.3191 %, \n",
      "epoch : 52, train acc : 97.2273 %, train loss : 0.09318736, test acc : 76.5957 %, \n",
      "epoch : 53, train acc : 97.9545 %, train loss : 0.06947051, test acc : 76.5957 %, \n",
      "epoch : 54, train acc : 97.0455 %, train loss : 0.09727276, test acc : 77.1277 %, \n",
      "epoch : 55, train acc : 96.6364 %, train loss : 0.10599670, test acc : 72.6064 %, \n",
      "epoch : 56, train acc : 97.3636 %, train loss : 0.08485500, test acc : 75.2660 %, \n",
      "epoch : 57, train acc : 97.9091 %, train loss : 0.07148296, test acc : 78.4574 %, \n",
      "epoch : 58, train acc : 97.7727 %, train loss : 0.08062122, test acc : 79.5213 %, \n",
      "epoch : 59, train acc : 97.0000 %, train loss : 0.09842061, test acc : 75.7979 %, \n",
      "epoch : 60, train acc : 97.2727 %, train loss : 0.07934423, test acc : 77.3936 %, \n",
      "epoch : 61, train acc : 97.0909 %, train loss : 0.09554653, test acc : 75.7979 %, \n",
      "epoch : 62, train acc : 96.8182 %, train loss : 0.09820186, test acc : 74.2021 %, \n",
      "epoch : 63, train acc : 96.6364 %, train loss : 0.10401922, test acc : 81.6489 %, \n",
      "epoch : 64, train acc : 96.9091 %, train loss : 0.08895659, test acc : 79.2553 %, \n",
      "epoch : 65, train acc : 96.5000 %, train loss : 0.10629107, test acc : 75.5319 %, \n",
      "epoch : 66, train acc : 97.1364 %, train loss : 0.08841650, test acc : 77.9255 %, \n",
      "epoch : 67, train acc : 97.3182 %, train loss : 0.08350396, test acc : 76.8617 %, \n",
      "epoch : 68, train acc : 97.3182 %, train loss : 0.08668999, test acc : 78.7234 %, \n",
      "epoch : 69, train acc : 97.2273 %, train loss : 0.07513870, test acc : 79.2553 %, \n",
      "epoch : 70, train acc : 98.1364 %, train loss : 0.05654742, test acc : 77.9255 %, \n",
      "epoch : 71, train acc : 98.8182 %, train loss : 0.04674190, test acc : 77.9255 %, \n",
      "epoch : 72, train acc : 98.1818 %, train loss : 0.06026319, test acc : 80.5851 %, \n",
      "epoch : 73, train acc : 98.6364 %, train loss : 0.05293878, test acc : 79.2553 %, \n",
      "epoch : 74, train acc : 98.1818 %, train loss : 0.04534375, test acc : 78.1915 %, \n",
      "epoch : 75, train acc : 98.2273 %, train loss : 0.05020149, test acc : 81.9149 %, \n",
      "epoch : 76, train acc : 98.3636 %, train loss : 0.05799286, test acc : 79.7872 %, \n",
      "epoch : 77, train acc : 98.4545 %, train loss : 0.04902837, test acc : 82.4468 %, \n",
      "epoch : 78, train acc : 99.1364 %, train loss : 0.03819764, test acc : 80.8511 %, \n",
      "epoch : 79, train acc : 99.0000 %, train loss : 0.04208950, test acc : 78.1915 %, \n",
      "epoch : 80, train acc : 98.6364 %, train loss : 0.04725219, test acc : 78.1915 %, \n",
      "epoch : 81, train acc : 99.0000 %, train loss : 0.04218019, test acc : 78.1915 %, \n",
      "epoch : 82, train acc : 97.9545 %, train loss : 0.05966165, test acc : 77.1277 %, \n",
      "epoch : 83, train acc : 96.7273 %, train loss : 0.09770603, test acc : 77.9255 %, \n",
      "epoch : 84, train acc : 96.0000 %, train loss : 0.11928432, test acc : 75.7979 %, \n",
      "epoch : 85, train acc : 96.4545 %, train loss : 0.11513586, test acc : 79.2553 %, \n",
      "epoch : 86, train acc : 97.3182 %, train loss : 0.08522715, test acc : 76.0638 %, \n",
      "epoch : 87, train acc : 97.4091 %, train loss : 0.09504152, test acc : 80.3191 %, \n",
      "epoch : 88, train acc : 97.5000 %, train loss : 0.06753929, test acc : 79.7872 %, \n",
      "epoch : 89, train acc : 98.2273 %, train loss : 0.05813475, test acc : 77.9255 %, \n",
      "epoch : 90, train acc : 98.9545 %, train loss : 0.04152445, test acc : 80.0532 %, \n",
      "epoch : 91, train acc : 98.9545 %, train loss : 0.03502026, test acc : 80.8511 %, \n",
      "epoch : 92, train acc : 98.4091 %, train loss : 0.04699043, test acc : 81.6489 %, \n",
      "epoch : 93, train acc : 98.4091 %, train loss : 0.04359656, test acc : 80.3191 %, \n",
      "epoch : 94, train acc : 98.7727 %, train loss : 0.03599992, test acc : 79.2553 %, \n",
      "epoch : 95, train acc : 98.8182 %, train loss : 0.03989262, test acc : 79.7872 %, \n",
      "epoch : 96, train acc : 98.4545 %, train loss : 0.04209171, test acc : 80.8511 %, \n",
      "epoch : 97, train acc : 98.2727 %, train loss : 0.05337371, test acc : 79.7872 %, \n",
      "epoch : 98, train acc : 98.6818 %, train loss : 0.04864767, test acc : 76.8617 %, \n",
      "epoch : 99, train acc : 98.3636 %, train loss : 0.05028562, test acc : 79.7872 %, \n",
      "epoch : 100, train acc : 98.1364 %, train loss : 0.05442157, test acc : 81.9149 %, \n"
     ]
    }
   ],
   "source": [
    "best_params = best_perform_df.iloc[0].to_dict()\n",
    "best_params['mlp_units'] = re.sub('[\\[\\]]','',best_params['mlp_units'])\n",
    "best_params['mlp_units'] = list(map(int,best_params['mlp_units'].split(',')))       # str to list\n",
    "\n",
    "cfg = Config\n",
    "cfg.ViT_params = best_params\n",
    "\n",
    "times = 10\n",
    "vit_all_result = defaultdict(list)\n",
    "for t in range(times):\n",
    "    _, all_train_acc, all_train_loss, all_test_acc = utils.experiment(cfg, all_data, all_ref)\n",
    "    vit_all_result[t] = [all_train_acc, all_train_loss, all_test_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result\n",
    "\n",
    "with open(save_path + 'experiment3_all_new_params_results.json', 'w') as f:\n",
    "    json.dump(dict(vit_all_result),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/home/donghyun/eye_writing_classification/experiments/save/'\n",
    "\n",
    "with open(save_path+'experiment3_all_prev_params_results.json') as f:\n",
    "    all_prev_results = json.load(f)\n",
    "\n",
    "with open(save_path+'experiment3_all_new_params_results.json') as f:\n",
    "    all_new_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_test_acc = []\n",
    "new_test_acc = []\n",
    "for t in range(10):\n",
    "    key = str(t)\n",
    "    prev = list(map(int, all_prev_results[key][2]))\n",
    "    new = list(map(int, all_new_results[key][2]))\n",
    "    prev_test_acc.append(prev)\n",
    "    new_test_acc.append(new)\n",
    "\n",
    "prev_avg_results = np.array(prev_test_acc).mean(axis=0)\n",
    "new_avg_results  =np.array(new_test_acc).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy base on raw numbers with 10 repetitions\n",
      "                             1,     2,    3,      4,      5,      6,     7,     8,     9,     10,       Avg.   Best.   Worst.  Std.\n",
      "prev vit model performance : [77, 81, 81, 77, 77, 77, 79, 78, 79, 82], (78.8, 82, 77, 1.8330302779823362)\n",
      "new vit model performance  : [79, 79, 79, 81, 78, 78, 79, 80, 78, 81], (79.2, 81, 78, 1.0770329614269007)\n"
     ]
    }
   ],
   "source": [
    "def analysis(data_list):\n",
    "    return np.mean(data_list), max(data_list), min(data_list), np.std(data_list)\n",
    "\n",
    "prev_all_test_performance = [t[-1] for t in prev_test_acc]\n",
    "new_all_test_performance = [t[-1] for t in new_test_acc]\n",
    "\n",
    "print('Accuracy base on raw numbers with 10 repetitions')\n",
    "print(' '*29 +'1,     2,    3,      4,      5,      6,     7,     8,     9,     10,       Avg.   Best.   Worst.  Std.')\n",
    "print('prev vit model performance : {}, {}'.format(prev_all_test_performance, analysis(prev_all_test_performance)))\n",
    "print('new vit model performance  : {}, {}'.format(new_all_test_performance, analysis(new_all_test_performance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize = (20,8))\n",
    "\n",
    "# test accuracy\n",
    "axes[0].plot(prev_avg_results, c = 'b', linestyle = 'solid', linewidth = 3)\n",
    "axes[0].plot(new_avg_results, c = 'r', linestyle = 'solid', linewidth = 3)\n",
    "\n",
    "axes[0].set_ylim(20,100)\n",
    "\n",
    "axes[0].set_title(\"Evaluation\", fontsize=20)\n",
    "axes[0].set_xlabel('Epoch', fontsize = 20)\n",
    "axes[0].set_ylabel('Accuracy', fontsize = 20)\n",
    "\n",
    "axes[0].legend(['prev hyperparams ViT', 'new hyperparams ViT'], fontsize = 15)\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('eog')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db961e0436efeabab578e79efd22f04ed4082e196e7e1c09c24525d7c028a5aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
